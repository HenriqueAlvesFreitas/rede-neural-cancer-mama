{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "le-uJe_LQI_n"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "df = pd.read_csv('cancer_mama.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classes  ['recorrencia'],['idade'],['menopausa'],['tumor'],['nodulos'],['capsula'],['maligno'],['seio'],['quadrante_seio'],['radioterapia']\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ecKPzzbiZB9M",
        "outputId": "f011ca8d-8c7d-4477-f91f-d9c0384ef145"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              recorrencia  idade menopausa  tumor nodulos capsula  maligno  \\\n",
              "0    no-recurrence-events  30-39   premeno  30-34     0-2      no        3   \n",
              "1    no-recurrence-events  40-49   premeno  20-24     0-2      no        2   \n",
              "2    no-recurrence-events  40-49   premeno  20-24     0-2      no        2   \n",
              "3    no-recurrence-events  60-69      ge40  15-19     0-2      no        2   \n",
              "4    no-recurrence-events  40-49   premeno    0-4     0-2      no        2   \n",
              "..                    ...    ...       ...    ...     ...     ...      ...   \n",
              "281     recurrence-events  30-39   premeno  30-34     0-2      no        2   \n",
              "282     recurrence-events  30-39   premeno  20-24     0-2      no        3   \n",
              "283     recurrence-events  60-69      ge40  20-24     0-2      no        1   \n",
              "284     recurrence-events  40-49      ge40  30-34     3-5      no        3   \n",
              "285     recurrence-events  50-59      ge40  30-34     3-5      no        3   \n",
              "\n",
              "      seio quadrante_seio radioterapia  \n",
              "0     left       left_low           no  \n",
              "1    right       right_up           no  \n",
              "2     left       left_low           no  \n",
              "3    right        left_up           no  \n",
              "4    right      right_low           no  \n",
              "..     ...            ...          ...  \n",
              "281   left        left_up           no  \n",
              "282   left        left_up          yes  \n",
              "283  right        left_up           no  \n",
              "284   left       left_low           no  \n",
              "285   left       left_low           no  \n",
              "\n",
              "[286 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-858d1eda-1a2d-459e-9a5a-7554a8005f6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recorrencia</th>\n",
              "      <th>idade</th>\n",
              "      <th>menopausa</th>\n",
              "      <th>tumor</th>\n",
              "      <th>nodulos</th>\n",
              "      <th>capsula</th>\n",
              "      <th>maligno</th>\n",
              "      <th>seio</th>\n",
              "      <th>quadrante_seio</th>\n",
              "      <th>radioterapia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no-recurrence-events</td>\n",
              "      <td>30-39</td>\n",
              "      <td>premeno</td>\n",
              "      <td>30-34</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>no-recurrence-events</td>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>20-24</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>right_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>no-recurrence-events</td>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>20-24</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no-recurrence-events</td>\n",
              "      <td>60-69</td>\n",
              "      <td>ge40</td>\n",
              "      <td>15-19</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>left_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no-recurrence-events</td>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>0-4</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>right_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>recurrence-events</td>\n",
              "      <td>30-39</td>\n",
              "      <td>premeno</td>\n",
              "      <td>30-34</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>left_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>recurrence-events</td>\n",
              "      <td>30-39</td>\n",
              "      <td>premeno</td>\n",
              "      <td>20-24</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>left_up</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>recurrence-events</td>\n",
              "      <td>60-69</td>\n",
              "      <td>ge40</td>\n",
              "      <td>20-24</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>right</td>\n",
              "      <td>left_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>recurrence-events</td>\n",
              "      <td>40-49</td>\n",
              "      <td>ge40</td>\n",
              "      <td>30-34</td>\n",
              "      <td>3-5</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>recurrence-events</td>\n",
              "      <td>50-59</td>\n",
              "      <td>ge40</td>\n",
              "      <td>30-34</td>\n",
              "      <td>3-5</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>286 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-858d1eda-1a2d-459e-9a5a-7554a8005f6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-858d1eda-1a2d-459e-9a5a-7554a8005f6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-858d1eda-1a2d-459e-9a5a-7554a8005f6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retirando uma coluna que não tem importancia para o resultado\n",
        "# Como mostrado nesse artigo https://www.cancer.org/cancer/breast-cancer/about/what-is-breast-cancer.html\n",
        "df = df.drop('seio', axis=1)\n",
        "# ja o quadrante é mais importante, podendo definir o tipo de cirurgia realizada como diz nesse artigo https://www.breastcancer.org/symptoms/types/quadrant."
      ],
      "metadata": {
        "id": "ubRw65SLX8bN"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para encontrar a porcentagem dos dados faltantes,\n",
        "def percentual(dados, coluna):\n",
        "  percentuais = dados.loc[pd.isna(dados[coluna]),coluna].shape[0]\n",
        "  total = dados[coluna].shape[0]\n",
        "\n",
        "  return (percentuais/total)*100"
      ],
      "metadata": {
        "id": "Fxlhu2lAY3PR"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "códigos individuais para testar a porcentagem de cada um"
      ],
      "metadata": {
        "id": "75IL7lkS72L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'idade')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEbITRrgZ5kU",
        "outputId": "83cc80ef-ce39-4b4f-fc83-dd1370379795"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'menopausa')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUnF4oWQZ5rQ",
        "outputId": "80c5ea4e-3753-4346-9716-306c5519916f"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'maligno')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzrAZfI0Z5_k",
        "outputId": "6f3593b5-222f-4e54-856b-c31cbe1cb178"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'tumor')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anqlQ8lrZ5xa",
        "outputId": "77f0aaf0-b1c1-4841-b51b-54a66dd952ab"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'recorrencia')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqrHOKdfZzpB",
        "outputId": "83921fe8-0359-4572-ec86-926ca13d5732"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'radioterapia')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4cL54IwZ6Ea",
        "outputId": "db170897-ec36-4fe9-9d60-4415bbcd72ae"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'capsula')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrt4IEVcZ57z",
        "outputId": "ede44b9c-d868-4c5d-a57a-7db52bcbb70e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'quadrante_seio')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kffyjKyiZ6AP",
        "outputId": "78a04d84-40e0-42c9-a9a5-bbdaf26ce8a0"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentual(df,'nodulos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4WkNp_ZZ52g",
        "outputId": "f324fda0-187b-47a9-e3cf-bf375f9fd8f1"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processo para remover valores encontrados que estão faltando\n",
        "faltantes = []\n",
        "for coluna in df:\n",
        "    todos_faltantes = len(df.loc[df[coluna]=='?'])\n",
        "    faltantes.append(todos_faltantes)"
      ],
      "metadata": {
        "id": "OEqOcoEwazi6"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['capsula']=='?', 'capsula'] = np.nan\n",
        "df.loc[df['quadrante_seio']=='?', 'quadrante_seio'] = np.nan"
      ],
      "metadata": {
        "id": "d0xFp3EcbIq7"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~pd.isna(df.quadrante_seio)].reset_index()\n",
        "df = df.drop(columns='index')"
      ],
      "metadata": {
        "id": "Peuv3v8pbKmj"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostrando os uniques de todas as colunas"
      ],
      "metadata": {
        "id": "WXFQuOLH82-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['idade'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tx56yaixDEh",
        "outputId": "2c7fa51a-72a3-4aed-b967-c2482ee30918"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['30-39', '40-49', '60-69', '50-59', '70-79', '20-29'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['menopausa'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvyGHEUyKtCY",
        "outputId": "e1ade967-163a-4ac6-ba26-39e9bb42aa06"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['premeno', 'ge40', 'lt40'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['tumor'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEI5Q188x5ZF",
        "outputId": "8930d513-1f11-446b-ad90-b7869694fd34"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['30-34', '20-24', '15-19', '0-4', '25-29', '50-54', '10-14',\n",
              "       '40-44', '35-39', '5-9', '45-49'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['nodulos'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAVeEvn6x5Z8",
        "outputId": "66814424-12b5-41c3-e855-483a89a67d77"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0-2', '6-8', '9-11', '3-5', '15-17', '12-14', '24-26'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['capsula'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cmmc84MKvK3",
        "outputId": "0a1ec1db-d50f-4500-f1d5-79c393179b6a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['no', 'yes', nan], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['maligno'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0jEmcpHkg9q",
        "outputId": "80b4716d-2019-4ae0-ec79-c4fb2a7161ef"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['quadrante_seio'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOJMDsY_K2dB",
        "outputId": "0ae1101b-4ca6-4541-b942-b11c363347f1"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['left_low', 'right_up', 'left_up', 'right_low', 'central'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['radioterapia'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaZEDEb1K4cN",
        "outputId": "fbb6b81d-082c-4aa6-d7c9-845c88d5cd0d"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['no', 'yes'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convertendo os campos categóricos em on hot\n",
        "\n",
        "def one_hot(categorias, valor):\n",
        "    categorias = list(set(categorias))\n",
        "    index = categorias.index(valor)\n",
        "    vetor = np.zeros(len(categorias))\n",
        "    vetor[index] = 1\n",
        "    return vetor"
      ],
      "metadata": {
        "id": "LPyUouGrw8tA"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.recorrencia = [one_hot(df.recorrencia, _) for _ in df.recorrencia]\n",
        "df.idade = [one_hot(df.idade, _) for _ in df.idade]\n",
        "df.menopausa = [one_hot(df.menopausa, _) for _ in df.menopausa]\n",
        "df.maligno = [one_hot(df.maligno, _) for _ in df.maligno]\n",
        "df.capsula = [one_hot(df.capsula, _) for _ in df.capsula]\n",
        "df.quadrante_seio = [one_hot(df.quadrante_seio, _) for _ in df.quadrante_seio]\n",
        "df.tumor = [one_hot(df.tumor, _) for _ in df.tumor]\n",
        "df.nodulos = [one_hot(df.nodulos, _) for _ in df.nodulos]\n",
        "df.radioterapia = [one_hot(df.radioterapia, _) for _ in df.radioterapia]"
      ],
      "metadata": {
        "id": "6QhcSg11LEHG"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apagando a coluna das respostas\n",
        "\n",
        "resultado = df.recorrencia\n",
        "df = df.drop(columns = 'recorrencia')"
      ],
      "metadata": {
        "id": "WB0KI58idiQ6"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenando as colunas para conseguir utilizar os dados\n",
        "def vetorizacao(idade,menopausa,tumor,nodulos,capsula,maligno,quadrante_seio,radioterapia):\n",
        "    return  idade.tolist() + menopausa.tolist() + tumor.tolist() + nodulos.tolist() \\\n",
        "+ capsula.tolist() + maligno.tolist() + quadrante_seio.tolist() + radioterapia.tolist()"
      ],
      "metadata": {
        "id": "LuH_-_JVcq-A"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = [vetorizacao(*_) for _ in df.values]"
      ],
      "metadata": {
        "id": "ADu6vS0hdQWP"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "separando os valores e os resultados em dois arquvios diferentes para comparalos no futuro, determinando a precisão"
      ],
      "metadata": {
        "id": "6mej0F9KLoNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"dados_processados.csv\", entrada, delimiter=',')"
      ],
      "metadata": {
        "id": "tJ7aYGhFkxmK"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(\"resultado.csv\", resultado.tolist(), delimiter=',')"
      ],
      "metadata": {
        "id": "iTmnn9D9k6y4"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.loadtxt(\"dados_processados.csv\", delimiter=',')\n",
        "y = np.loadtxt(\"resultado.csv\", delimiter=',')"
      ],
      "metadata": {
        "id": "GTI3VgR0k8i5"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tamanho_treino = int(x.shape[0]*0.75)"
      ],
      "metadata": {
        "id": "iXFiuwZO4ovH"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_treino = x[ :tamanho_treino]\n",
        "y_treino = y[ :tamanho_treino]\n",
        "\n",
        "x_s = x[tamanho_treino: ]\n",
        "y_s = y[tamanho_treino: ]"
      ],
      "metadata": {
        "id": "JctqeDeF48Yu"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           tol=0.000001,\n",
        "                           activation='relu',\n",
        "                           #hiden_layer_sizes=(100,100),\n",
        "                           learning_rate_init=0.0001)"
      ],
      "metadata": {
        "id": "TJEVHfniuiNl"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural.fit(x,y)"
      ],
      "metadata": {
        "id": "4qkfD4Qxuj1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a424523-a94a-441a-de8b-a1bbce14170a"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 1975, loss = 0.46951913\n",
            "Iteration 1976, loss = 0.46930929\n",
            "Iteration 1977, loss = 0.46910146\n",
            "Iteration 1978, loss = 0.46888109\n",
            "Iteration 1979, loss = 0.46867533\n",
            "Iteration 1980, loss = 0.46846531\n",
            "Iteration 1981, loss = 0.46826435\n",
            "Iteration 1982, loss = 0.46810135\n",
            "Iteration 1983, loss = 0.46789563\n",
            "Iteration 1984, loss = 0.46770611\n",
            "Iteration 1985, loss = 0.46747649\n",
            "Iteration 1986, loss = 0.46726399\n",
            "Iteration 1987, loss = 0.46703629\n",
            "Iteration 1988, loss = 0.46680865\n",
            "Iteration 1989, loss = 0.46666973\n",
            "Iteration 1990, loss = 0.46639456\n",
            "Iteration 1991, loss = 0.46618939\n",
            "Iteration 1992, loss = 0.46594405\n",
            "Iteration 1993, loss = 0.46574265\n",
            "Iteration 1994, loss = 0.46553112\n",
            "Iteration 1995, loss = 0.46533833\n",
            "Iteration 1996, loss = 0.46512141\n",
            "Iteration 1997, loss = 0.46490479\n",
            "Iteration 1998, loss = 0.46469794\n",
            "Iteration 1999, loss = 0.46449078\n",
            "Iteration 2000, loss = 0.46429603\n",
            "Iteration 2001, loss = 0.46407172\n",
            "Iteration 2002, loss = 0.46387820\n",
            "Iteration 2003, loss = 0.46368824\n",
            "Iteration 2004, loss = 0.46346933\n",
            "Iteration 2005, loss = 0.46328928\n",
            "Iteration 2006, loss = 0.46315211\n",
            "Iteration 2007, loss = 0.46291995\n",
            "Iteration 2008, loss = 0.46268197\n",
            "Iteration 2009, loss = 0.46248000\n",
            "Iteration 2010, loss = 0.46225359\n",
            "Iteration 2011, loss = 0.46206944\n",
            "Iteration 2012, loss = 0.46183977\n",
            "Iteration 2013, loss = 0.46161305\n",
            "Iteration 2014, loss = 0.46139455\n",
            "Iteration 2015, loss = 0.46119040\n",
            "Iteration 2016, loss = 0.46098006\n",
            "Iteration 2017, loss = 0.46081612\n",
            "Iteration 2018, loss = 0.46054080\n",
            "Iteration 2019, loss = 0.46035252\n",
            "Iteration 2020, loss = 0.46014591\n",
            "Iteration 2021, loss = 0.45994663\n",
            "Iteration 2022, loss = 0.45973485\n",
            "Iteration 2023, loss = 0.45952906\n",
            "Iteration 2024, loss = 0.45930164\n",
            "Iteration 2025, loss = 0.45909620\n",
            "Iteration 2026, loss = 0.45888271\n",
            "Iteration 2027, loss = 0.45866504\n",
            "Iteration 2028, loss = 0.45847053\n",
            "Iteration 2029, loss = 0.45826177\n",
            "Iteration 2030, loss = 0.45808944\n",
            "Iteration 2031, loss = 0.45785657\n",
            "Iteration 2032, loss = 0.45764984\n",
            "Iteration 2033, loss = 0.45744439\n",
            "Iteration 2034, loss = 0.45728332\n",
            "Iteration 2035, loss = 0.45704994\n",
            "Iteration 2036, loss = 0.45681749\n",
            "Iteration 2037, loss = 0.45661567\n",
            "Iteration 2038, loss = 0.45640987\n",
            "Iteration 2039, loss = 0.45622488\n",
            "Iteration 2040, loss = 0.45602077\n",
            "Iteration 2041, loss = 0.45581248\n",
            "Iteration 2042, loss = 0.45561787\n",
            "Iteration 2043, loss = 0.45541081\n",
            "Iteration 2044, loss = 0.45519951\n",
            "Iteration 2045, loss = 0.45503047\n",
            "Iteration 2046, loss = 0.45480024\n",
            "Iteration 2047, loss = 0.45459134\n",
            "Iteration 2048, loss = 0.45437973\n",
            "Iteration 2049, loss = 0.45415096\n",
            "Iteration 2050, loss = 0.45396763\n",
            "Iteration 2051, loss = 0.45376923\n",
            "Iteration 2052, loss = 0.45356610\n",
            "Iteration 2053, loss = 0.45334906\n",
            "Iteration 2054, loss = 0.45318871\n",
            "Iteration 2055, loss = 0.45296868\n",
            "Iteration 2056, loss = 0.45277792\n",
            "Iteration 2057, loss = 0.45258090\n",
            "Iteration 2058, loss = 0.45237440\n",
            "Iteration 2059, loss = 0.45214267\n",
            "Iteration 2060, loss = 0.45194944\n",
            "Iteration 2061, loss = 0.45173901\n",
            "Iteration 2062, loss = 0.45154094\n",
            "Iteration 2063, loss = 0.45131308\n",
            "Iteration 2064, loss = 0.45111245\n",
            "Iteration 2065, loss = 0.45090317\n",
            "Iteration 2066, loss = 0.45071013\n",
            "Iteration 2067, loss = 0.45054321\n",
            "Iteration 2068, loss = 0.45030926\n",
            "Iteration 2069, loss = 0.45012826\n",
            "Iteration 2070, loss = 0.44992327\n",
            "Iteration 2071, loss = 0.44974571\n",
            "Iteration 2072, loss = 0.44949238\n",
            "Iteration 2073, loss = 0.44931376\n",
            "Iteration 2074, loss = 0.44911726\n",
            "Iteration 2075, loss = 0.44892138\n",
            "Iteration 2076, loss = 0.44869913\n",
            "Iteration 2077, loss = 0.44851122\n",
            "Iteration 2078, loss = 0.44828181\n",
            "Iteration 2079, loss = 0.44807664\n",
            "Iteration 2080, loss = 0.44787377\n",
            "Iteration 2081, loss = 0.44772418\n",
            "Iteration 2082, loss = 0.44747114\n",
            "Iteration 2083, loss = 0.44730602\n",
            "Iteration 2084, loss = 0.44709128\n",
            "Iteration 2085, loss = 0.44688135\n",
            "Iteration 2086, loss = 0.44670209\n",
            "Iteration 2087, loss = 0.44649151\n",
            "Iteration 2088, loss = 0.44631072\n",
            "Iteration 2089, loss = 0.44607079\n",
            "Iteration 2090, loss = 0.44590388\n",
            "Iteration 2091, loss = 0.44567666\n",
            "Iteration 2092, loss = 0.44548203\n",
            "Iteration 2093, loss = 0.44530866\n",
            "Iteration 2094, loss = 0.44510571\n",
            "Iteration 2095, loss = 0.44490146\n",
            "Iteration 2096, loss = 0.44470887\n",
            "Iteration 2097, loss = 0.44448730\n",
            "Iteration 2098, loss = 0.44427707\n",
            "Iteration 2099, loss = 0.44410042\n",
            "Iteration 2100, loss = 0.44386725\n",
            "Iteration 2101, loss = 0.44366035\n",
            "Iteration 2102, loss = 0.44349427\n",
            "Iteration 2103, loss = 0.44325993\n",
            "Iteration 2104, loss = 0.44308356\n",
            "Iteration 2105, loss = 0.44290755\n",
            "Iteration 2106, loss = 0.44271995\n",
            "Iteration 2107, loss = 0.44257961\n",
            "Iteration 2108, loss = 0.44238976\n",
            "Iteration 2109, loss = 0.44225564\n",
            "Iteration 2110, loss = 0.44204302\n",
            "Iteration 2111, loss = 0.44188007\n",
            "Iteration 2112, loss = 0.44161216\n",
            "Iteration 2113, loss = 0.44141444\n",
            "Iteration 2114, loss = 0.44126547\n",
            "Iteration 2115, loss = 0.44108291\n",
            "Iteration 2116, loss = 0.44086223\n",
            "Iteration 2117, loss = 0.44061181\n",
            "Iteration 2118, loss = 0.44042569\n",
            "Iteration 2119, loss = 0.44022278\n",
            "Iteration 2120, loss = 0.44003805\n",
            "Iteration 2121, loss = 0.43984932\n",
            "Iteration 2122, loss = 0.43961946\n",
            "Iteration 2123, loss = 0.43944383\n",
            "Iteration 2124, loss = 0.43923226\n",
            "Iteration 2125, loss = 0.43903650\n",
            "Iteration 2126, loss = 0.43882431\n",
            "Iteration 2127, loss = 0.43866953\n",
            "Iteration 2128, loss = 0.43848848\n",
            "Iteration 2129, loss = 0.43828586\n",
            "Iteration 2130, loss = 0.43808447\n",
            "Iteration 2131, loss = 0.43789479\n",
            "Iteration 2132, loss = 0.43768081\n",
            "Iteration 2133, loss = 0.43749449\n",
            "Iteration 2134, loss = 0.43726227\n",
            "Iteration 2135, loss = 0.43709918\n",
            "Iteration 2136, loss = 0.43690720\n",
            "Iteration 2137, loss = 0.43668668\n",
            "Iteration 2138, loss = 0.43649067\n",
            "Iteration 2139, loss = 0.43629463\n",
            "Iteration 2140, loss = 0.43611458\n",
            "Iteration 2141, loss = 0.43588423\n",
            "Iteration 2142, loss = 0.43567922\n",
            "Iteration 2143, loss = 0.43548025\n",
            "Iteration 2144, loss = 0.43529639\n",
            "Iteration 2145, loss = 0.43508255\n",
            "Iteration 2146, loss = 0.43490063\n",
            "Iteration 2147, loss = 0.43468949\n",
            "Iteration 2148, loss = 0.43449859\n",
            "Iteration 2149, loss = 0.43431438\n",
            "Iteration 2150, loss = 0.43411472\n",
            "Iteration 2151, loss = 0.43395012\n",
            "Iteration 2152, loss = 0.43375059\n",
            "Iteration 2153, loss = 0.43353992\n",
            "Iteration 2154, loss = 0.43336964\n",
            "Iteration 2155, loss = 0.43315812\n",
            "Iteration 2156, loss = 0.43297860\n",
            "Iteration 2157, loss = 0.43278081\n",
            "Iteration 2158, loss = 0.43261227\n",
            "Iteration 2159, loss = 0.43238710\n",
            "Iteration 2160, loss = 0.43217779\n",
            "Iteration 2161, loss = 0.43198189\n",
            "Iteration 2162, loss = 0.43183126\n",
            "Iteration 2163, loss = 0.43159699\n",
            "Iteration 2164, loss = 0.43141093\n",
            "Iteration 2165, loss = 0.43122578\n",
            "Iteration 2166, loss = 0.43104100\n",
            "Iteration 2167, loss = 0.43083395\n",
            "Iteration 2168, loss = 0.43062803\n",
            "Iteration 2169, loss = 0.43043480\n",
            "Iteration 2170, loss = 0.43027792\n",
            "Iteration 2171, loss = 0.43009196\n",
            "Iteration 2172, loss = 0.42990106\n",
            "Iteration 2173, loss = 0.42971238\n",
            "Iteration 2174, loss = 0.42953636\n",
            "Iteration 2175, loss = 0.42932661\n",
            "Iteration 2176, loss = 0.42914934\n",
            "Iteration 2177, loss = 0.42894906\n",
            "Iteration 2178, loss = 0.42875742\n",
            "Iteration 2179, loss = 0.42855954\n",
            "Iteration 2180, loss = 0.42836911\n",
            "Iteration 2181, loss = 0.42819596\n",
            "Iteration 2182, loss = 0.42798964\n",
            "Iteration 2183, loss = 0.42780499\n",
            "Iteration 2184, loss = 0.42762587\n",
            "Iteration 2185, loss = 0.42739871\n",
            "Iteration 2186, loss = 0.42724881\n",
            "Iteration 2187, loss = 0.42710053\n",
            "Iteration 2188, loss = 0.42687081\n",
            "Iteration 2189, loss = 0.42666493\n",
            "Iteration 2190, loss = 0.42650795\n",
            "Iteration 2191, loss = 0.42629358\n",
            "Iteration 2192, loss = 0.42608689\n",
            "Iteration 2193, loss = 0.42591625\n",
            "Iteration 2194, loss = 0.42573876\n",
            "Iteration 2195, loss = 0.42552186\n",
            "Iteration 2196, loss = 0.42534892\n",
            "Iteration 2197, loss = 0.42516625\n",
            "Iteration 2198, loss = 0.42501470\n",
            "Iteration 2199, loss = 0.42478667\n",
            "Iteration 2200, loss = 0.42460581\n",
            "Iteration 2201, loss = 0.42441425\n",
            "Iteration 2202, loss = 0.42426675\n",
            "Iteration 2203, loss = 0.42405487\n",
            "Iteration 2204, loss = 0.42389783\n",
            "Iteration 2205, loss = 0.42370444\n",
            "Iteration 2206, loss = 0.42354580\n",
            "Iteration 2207, loss = 0.42333262\n",
            "Iteration 2208, loss = 0.42311966\n",
            "Iteration 2209, loss = 0.42293896\n",
            "Iteration 2210, loss = 0.42276395\n",
            "Iteration 2211, loss = 0.42258899\n",
            "Iteration 2212, loss = 0.42237435\n",
            "Iteration 2213, loss = 0.42224809\n",
            "Iteration 2214, loss = 0.42205402\n",
            "Iteration 2215, loss = 0.42183846\n",
            "Iteration 2216, loss = 0.42165410\n",
            "Iteration 2217, loss = 0.42149640\n",
            "Iteration 2218, loss = 0.42136406\n",
            "Iteration 2219, loss = 0.42117202\n",
            "Iteration 2220, loss = 0.42098798\n",
            "Iteration 2221, loss = 0.42079655\n",
            "Iteration 2222, loss = 0.42058350\n",
            "Iteration 2223, loss = 0.42038370\n",
            "Iteration 2224, loss = 0.42021377\n",
            "Iteration 2225, loss = 0.42007932\n",
            "Iteration 2226, loss = 0.41990353\n",
            "Iteration 2227, loss = 0.41970558\n",
            "Iteration 2228, loss = 0.41950533\n",
            "Iteration 2229, loss = 0.41933598\n",
            "Iteration 2230, loss = 0.41915437\n",
            "Iteration 2231, loss = 0.41898095\n",
            "Iteration 2232, loss = 0.41880332\n",
            "Iteration 2233, loss = 0.41863530\n",
            "Iteration 2234, loss = 0.41843882\n",
            "Iteration 2235, loss = 0.41826175\n",
            "Iteration 2236, loss = 0.41803916\n",
            "Iteration 2237, loss = 0.41784404\n",
            "Iteration 2238, loss = 0.41768513\n",
            "Iteration 2239, loss = 0.41749792\n",
            "Iteration 2240, loss = 0.41732939\n",
            "Iteration 2241, loss = 0.41711493\n",
            "Iteration 2242, loss = 0.41697601\n",
            "Iteration 2243, loss = 0.41676761\n",
            "Iteration 2244, loss = 0.41656583\n",
            "Iteration 2245, loss = 0.41638728\n",
            "Iteration 2246, loss = 0.41620410\n",
            "Iteration 2247, loss = 0.41604129\n",
            "Iteration 2248, loss = 0.41585206\n",
            "Iteration 2249, loss = 0.41564602\n",
            "Iteration 2250, loss = 0.41547664\n",
            "Iteration 2251, loss = 0.41528275\n",
            "Iteration 2252, loss = 0.41514955\n",
            "Iteration 2253, loss = 0.41492561\n",
            "Iteration 2254, loss = 0.41473509\n",
            "Iteration 2255, loss = 0.41455262\n",
            "Iteration 2256, loss = 0.41437691\n",
            "Iteration 2257, loss = 0.41418889\n",
            "Iteration 2258, loss = 0.41402285\n",
            "Iteration 2259, loss = 0.41388389\n",
            "Iteration 2260, loss = 0.41367467\n",
            "Iteration 2261, loss = 0.41349159\n",
            "Iteration 2262, loss = 0.41331726\n",
            "Iteration 2263, loss = 0.41311563\n",
            "Iteration 2264, loss = 0.41292637\n",
            "Iteration 2265, loss = 0.41273362\n",
            "Iteration 2266, loss = 0.41254898\n",
            "Iteration 2267, loss = 0.41240051\n",
            "Iteration 2268, loss = 0.41227055\n",
            "Iteration 2269, loss = 0.41207707\n",
            "Iteration 2270, loss = 0.41186453\n",
            "Iteration 2271, loss = 0.41170103\n",
            "Iteration 2272, loss = 0.41154603\n",
            "Iteration 2273, loss = 0.41138458\n",
            "Iteration 2274, loss = 0.41121452\n",
            "Iteration 2275, loss = 0.41101995\n",
            "Iteration 2276, loss = 0.41082229\n",
            "Iteration 2277, loss = 0.41063327\n",
            "Iteration 2278, loss = 0.41044117\n",
            "Iteration 2279, loss = 0.41024836\n",
            "Iteration 2280, loss = 0.41006109\n",
            "Iteration 2281, loss = 0.40987308\n",
            "Iteration 2282, loss = 0.40968929\n",
            "Iteration 2283, loss = 0.40951686\n",
            "Iteration 2284, loss = 0.40933700\n",
            "Iteration 2285, loss = 0.40916505\n",
            "Iteration 2286, loss = 0.40904691\n",
            "Iteration 2287, loss = 0.40879969\n",
            "Iteration 2288, loss = 0.40861085\n",
            "Iteration 2289, loss = 0.40844611\n",
            "Iteration 2290, loss = 0.40827845\n",
            "Iteration 2291, loss = 0.40811991\n",
            "Iteration 2292, loss = 0.40793191\n",
            "Iteration 2293, loss = 0.40775227\n",
            "Iteration 2294, loss = 0.40757948\n",
            "Iteration 2295, loss = 0.40740269\n",
            "Iteration 2296, loss = 0.40721867\n",
            "Iteration 2297, loss = 0.40705231\n",
            "Iteration 2298, loss = 0.40684182\n",
            "Iteration 2299, loss = 0.40665608\n",
            "Iteration 2300, loss = 0.40650401\n",
            "Iteration 2301, loss = 0.40634100\n",
            "Iteration 2302, loss = 0.40617134\n",
            "Iteration 2303, loss = 0.40598603\n",
            "Iteration 2304, loss = 0.40580644\n",
            "Iteration 2305, loss = 0.40570898\n",
            "Iteration 2306, loss = 0.40550125\n",
            "Iteration 2307, loss = 0.40531937\n",
            "Iteration 2308, loss = 0.40517243\n",
            "Iteration 2309, loss = 0.40493270\n",
            "Iteration 2310, loss = 0.40476933\n",
            "Iteration 2311, loss = 0.40458095\n",
            "Iteration 2312, loss = 0.40441118\n",
            "Iteration 2313, loss = 0.40426286\n",
            "Iteration 2314, loss = 0.40405415\n",
            "Iteration 2315, loss = 0.40394454\n",
            "Iteration 2316, loss = 0.40369383\n",
            "Iteration 2317, loss = 0.40352545\n",
            "Iteration 2318, loss = 0.40336283\n",
            "Iteration 2319, loss = 0.40324921\n",
            "Iteration 2320, loss = 0.40304475\n",
            "Iteration 2321, loss = 0.40286245\n",
            "Iteration 2322, loss = 0.40270295\n",
            "Iteration 2323, loss = 0.40251604\n",
            "Iteration 2324, loss = 0.40234533\n",
            "Iteration 2325, loss = 0.40215543\n",
            "Iteration 2326, loss = 0.40199599\n",
            "Iteration 2327, loss = 0.40183753\n",
            "Iteration 2328, loss = 0.40163721\n",
            "Iteration 2329, loss = 0.40148281\n",
            "Iteration 2330, loss = 0.40132842\n",
            "Iteration 2331, loss = 0.40112857\n",
            "Iteration 2332, loss = 0.40095228\n",
            "Iteration 2333, loss = 0.40075883\n",
            "Iteration 2334, loss = 0.40062014\n",
            "Iteration 2335, loss = 0.40043642\n",
            "Iteration 2336, loss = 0.40024339\n",
            "Iteration 2337, loss = 0.40014236\n",
            "Iteration 2338, loss = 0.39992585\n",
            "Iteration 2339, loss = 0.39973673\n",
            "Iteration 2340, loss = 0.39956804\n",
            "Iteration 2341, loss = 0.39939679\n",
            "Iteration 2342, loss = 0.39922307\n",
            "Iteration 2343, loss = 0.39904021\n",
            "Iteration 2344, loss = 0.39889630\n",
            "Iteration 2345, loss = 0.39872828\n",
            "Iteration 2346, loss = 0.39854783\n",
            "Iteration 2347, loss = 0.39840157\n",
            "Iteration 2348, loss = 0.39823284\n",
            "Iteration 2349, loss = 0.39805566\n",
            "Iteration 2350, loss = 0.39787296\n",
            "Iteration 2351, loss = 0.39772629\n",
            "Iteration 2352, loss = 0.39753548\n",
            "Iteration 2353, loss = 0.39737183\n",
            "Iteration 2354, loss = 0.39720507\n",
            "Iteration 2355, loss = 0.39702562\n",
            "Iteration 2356, loss = 0.39688676\n",
            "Iteration 2357, loss = 0.39672212\n",
            "Iteration 2358, loss = 0.39654711\n",
            "Iteration 2359, loss = 0.39637764\n",
            "Iteration 2360, loss = 0.39624711\n",
            "Iteration 2361, loss = 0.39604988\n",
            "Iteration 2362, loss = 0.39586981\n",
            "Iteration 2363, loss = 0.39568376\n",
            "Iteration 2364, loss = 0.39553454\n",
            "Iteration 2365, loss = 0.39536154\n",
            "Iteration 2366, loss = 0.39524819\n",
            "Iteration 2367, loss = 0.39503518\n",
            "Iteration 2368, loss = 0.39488207\n",
            "Iteration 2369, loss = 0.39470872\n",
            "Iteration 2370, loss = 0.39451669\n",
            "Iteration 2371, loss = 0.39435712\n",
            "Iteration 2372, loss = 0.39420011\n",
            "Iteration 2373, loss = 0.39403311\n",
            "Iteration 2374, loss = 0.39385799\n",
            "Iteration 2375, loss = 0.39369590\n",
            "Iteration 2376, loss = 0.39352530\n",
            "Iteration 2377, loss = 0.39333737\n",
            "Iteration 2378, loss = 0.39318312\n",
            "Iteration 2379, loss = 0.39303282\n",
            "Iteration 2380, loss = 0.39284444\n",
            "Iteration 2381, loss = 0.39268286\n",
            "Iteration 2382, loss = 0.39254714\n",
            "Iteration 2383, loss = 0.39235830\n",
            "Iteration 2384, loss = 0.39217784\n",
            "Iteration 2385, loss = 0.39202764\n",
            "Iteration 2386, loss = 0.39186656\n",
            "Iteration 2387, loss = 0.39171323\n",
            "Iteration 2388, loss = 0.39153196\n",
            "Iteration 2389, loss = 0.39137419\n",
            "Iteration 2390, loss = 0.39124674\n",
            "Iteration 2391, loss = 0.39106493\n",
            "Iteration 2392, loss = 0.39086741\n",
            "Iteration 2393, loss = 0.39070490\n",
            "Iteration 2394, loss = 0.39054344\n",
            "Iteration 2395, loss = 0.39036203\n",
            "Iteration 2396, loss = 0.39022613\n",
            "Iteration 2397, loss = 0.39005582\n",
            "Iteration 2398, loss = 0.38989884\n",
            "Iteration 2399, loss = 0.38973622\n",
            "Iteration 2400, loss = 0.38958175\n",
            "Iteration 2401, loss = 0.38941717\n",
            "Iteration 2402, loss = 0.38931311\n",
            "Iteration 2403, loss = 0.38913078\n",
            "Iteration 2404, loss = 0.38896184\n",
            "Iteration 2405, loss = 0.38877137\n",
            "Iteration 2406, loss = 0.38860025\n",
            "Iteration 2407, loss = 0.38846007\n",
            "Iteration 2408, loss = 0.38827521\n",
            "Iteration 2409, loss = 0.38810829\n",
            "Iteration 2410, loss = 0.38797843\n",
            "Iteration 2411, loss = 0.38779855\n",
            "Iteration 2412, loss = 0.38764109\n",
            "Iteration 2413, loss = 0.38747687\n",
            "Iteration 2414, loss = 0.38731838\n",
            "Iteration 2415, loss = 0.38715465\n",
            "Iteration 2416, loss = 0.38698914\n",
            "Iteration 2417, loss = 0.38685146\n",
            "Iteration 2418, loss = 0.38667556\n",
            "Iteration 2419, loss = 0.38649967\n",
            "Iteration 2420, loss = 0.38634068\n",
            "Iteration 2421, loss = 0.38618230\n",
            "Iteration 2422, loss = 0.38601106\n",
            "Iteration 2423, loss = 0.38584672\n",
            "Iteration 2424, loss = 0.38568247\n",
            "Iteration 2425, loss = 0.38553856\n",
            "Iteration 2426, loss = 0.38536680\n",
            "Iteration 2427, loss = 0.38522797\n",
            "Iteration 2428, loss = 0.38509114\n",
            "Iteration 2429, loss = 0.38490185\n",
            "Iteration 2430, loss = 0.38473272\n",
            "Iteration 2431, loss = 0.38456494\n",
            "Iteration 2432, loss = 0.38446710\n",
            "Iteration 2433, loss = 0.38425737\n",
            "Iteration 2434, loss = 0.38411949\n",
            "Iteration 2435, loss = 0.38394258\n",
            "Iteration 2436, loss = 0.38378553\n",
            "Iteration 2437, loss = 0.38361081\n",
            "Iteration 2438, loss = 0.38346639\n",
            "Iteration 2439, loss = 0.38332638\n",
            "Iteration 2440, loss = 0.38314174\n",
            "Iteration 2441, loss = 0.38299525\n",
            "Iteration 2442, loss = 0.38282346\n",
            "Iteration 2443, loss = 0.38266115\n",
            "Iteration 2444, loss = 0.38253056\n",
            "Iteration 2445, loss = 0.38236529\n",
            "Iteration 2446, loss = 0.38219658\n",
            "Iteration 2447, loss = 0.38203608\n",
            "Iteration 2448, loss = 0.38185975\n",
            "Iteration 2449, loss = 0.38171662\n",
            "Iteration 2450, loss = 0.38153414\n",
            "Iteration 2451, loss = 0.38140388\n",
            "Iteration 2452, loss = 0.38122004\n",
            "Iteration 2453, loss = 0.38107280\n",
            "Iteration 2454, loss = 0.38090943\n",
            "Iteration 2455, loss = 0.38073444\n",
            "Iteration 2456, loss = 0.38058885\n",
            "Iteration 2457, loss = 0.38042170\n",
            "Iteration 2458, loss = 0.38026271\n",
            "Iteration 2459, loss = 0.38010479\n",
            "Iteration 2460, loss = 0.37995928\n",
            "Iteration 2461, loss = 0.37980948\n",
            "Iteration 2462, loss = 0.37968458\n",
            "Iteration 2463, loss = 0.37950971\n",
            "Iteration 2464, loss = 0.37933021\n",
            "Iteration 2465, loss = 0.37915699\n",
            "Iteration 2466, loss = 0.37901892\n",
            "Iteration 2467, loss = 0.37884907\n",
            "Iteration 2468, loss = 0.37869589\n",
            "Iteration 2469, loss = 0.37858577\n",
            "Iteration 2470, loss = 0.37837359\n",
            "Iteration 2471, loss = 0.37823079\n",
            "Iteration 2472, loss = 0.37808741\n",
            "Iteration 2473, loss = 0.37787744\n",
            "Iteration 2474, loss = 0.37770729\n",
            "Iteration 2475, loss = 0.37759859\n",
            "Iteration 2476, loss = 0.37745504\n",
            "Iteration 2477, loss = 0.37729694\n",
            "Iteration 2478, loss = 0.37714031\n",
            "Iteration 2479, loss = 0.37692279\n",
            "Iteration 2480, loss = 0.37680172\n",
            "Iteration 2481, loss = 0.37662274\n",
            "Iteration 2482, loss = 0.37647259\n",
            "Iteration 2483, loss = 0.37632791\n",
            "Iteration 2484, loss = 0.37619317\n",
            "Iteration 2485, loss = 0.37601965\n",
            "Iteration 2486, loss = 0.37584971\n",
            "Iteration 2487, loss = 0.37566732\n",
            "Iteration 2488, loss = 0.37550693\n",
            "Iteration 2489, loss = 0.37534885\n",
            "Iteration 2490, loss = 0.37522003\n",
            "Iteration 2491, loss = 0.37510845\n",
            "Iteration 2492, loss = 0.37494956\n",
            "Iteration 2493, loss = 0.37477801\n",
            "Iteration 2494, loss = 0.37461859\n",
            "Iteration 2495, loss = 0.37445415\n",
            "Iteration 2496, loss = 0.37428410\n",
            "Iteration 2497, loss = 0.37412080\n",
            "Iteration 2498, loss = 0.37397178\n",
            "Iteration 2499, loss = 0.37379779\n",
            "Iteration 2500, loss = 0.37365317\n",
            "Iteration 2501, loss = 0.37348459\n",
            "Iteration 2502, loss = 0.37334546\n",
            "Iteration 2503, loss = 0.37321675\n",
            "Iteration 2504, loss = 0.37301973\n",
            "Iteration 2505, loss = 0.37288514\n",
            "Iteration 2506, loss = 0.37268928\n",
            "Iteration 2507, loss = 0.37254660\n",
            "Iteration 2508, loss = 0.37238814\n",
            "Iteration 2509, loss = 0.37224288\n",
            "Iteration 2510, loss = 0.37209363\n",
            "Iteration 2511, loss = 0.37192997\n",
            "Iteration 2512, loss = 0.37177585\n",
            "Iteration 2513, loss = 0.37163064\n",
            "Iteration 2514, loss = 0.37151263\n",
            "Iteration 2515, loss = 0.37135526\n",
            "Iteration 2516, loss = 0.37120529\n",
            "Iteration 2517, loss = 0.37105525\n",
            "Iteration 2518, loss = 0.37090900\n",
            "Iteration 2519, loss = 0.37071778\n",
            "Iteration 2520, loss = 0.37055652\n",
            "Iteration 2521, loss = 0.37042802\n",
            "Iteration 2522, loss = 0.37024525\n",
            "Iteration 2523, loss = 0.37015885\n",
            "Iteration 2524, loss = 0.36993905\n",
            "Iteration 2525, loss = 0.36979155\n",
            "Iteration 2526, loss = 0.36964895\n",
            "Iteration 2527, loss = 0.36948029\n",
            "Iteration 2528, loss = 0.36930293\n",
            "Iteration 2529, loss = 0.36916347\n",
            "Iteration 2530, loss = 0.36899315\n",
            "Iteration 2531, loss = 0.36883400\n",
            "Iteration 2532, loss = 0.36869802\n",
            "Iteration 2533, loss = 0.36853768\n",
            "Iteration 2534, loss = 0.36837286\n",
            "Iteration 2535, loss = 0.36821525\n",
            "Iteration 2536, loss = 0.36806559\n",
            "Iteration 2537, loss = 0.36792240\n",
            "Iteration 2538, loss = 0.36779154\n",
            "Iteration 2539, loss = 0.36766600\n",
            "Iteration 2540, loss = 0.36749841\n",
            "Iteration 2541, loss = 0.36732050\n",
            "Iteration 2542, loss = 0.36718220\n",
            "Iteration 2543, loss = 0.36703191\n",
            "Iteration 2544, loss = 0.36690708\n",
            "Iteration 2545, loss = 0.36676566\n",
            "Iteration 2546, loss = 0.36657619\n",
            "Iteration 2547, loss = 0.36643477\n",
            "Iteration 2548, loss = 0.36630626\n",
            "Iteration 2549, loss = 0.36616168\n",
            "Iteration 2550, loss = 0.36600854\n",
            "Iteration 2551, loss = 0.36588469\n",
            "Iteration 2552, loss = 0.36572605\n",
            "Iteration 2553, loss = 0.36556667\n",
            "Iteration 2554, loss = 0.36542410\n",
            "Iteration 2555, loss = 0.36525726\n",
            "Iteration 2556, loss = 0.36515448\n",
            "Iteration 2557, loss = 0.36497211\n",
            "Iteration 2558, loss = 0.36482838\n",
            "Iteration 2559, loss = 0.36464286\n",
            "Iteration 2560, loss = 0.36449626\n",
            "Iteration 2561, loss = 0.36433815\n",
            "Iteration 2562, loss = 0.36419662\n",
            "Iteration 2563, loss = 0.36403774\n",
            "Iteration 2564, loss = 0.36392589\n",
            "Iteration 2565, loss = 0.36374083\n",
            "Iteration 2566, loss = 0.36359346\n",
            "Iteration 2567, loss = 0.36349045\n",
            "Iteration 2568, loss = 0.36331466\n",
            "Iteration 2569, loss = 0.36313360\n",
            "Iteration 2570, loss = 0.36301783\n",
            "Iteration 2571, loss = 0.36286085\n",
            "Iteration 2572, loss = 0.36273948\n",
            "Iteration 2573, loss = 0.36257155\n",
            "Iteration 2574, loss = 0.36243158\n",
            "Iteration 2575, loss = 0.36226318\n",
            "Iteration 2576, loss = 0.36209875\n",
            "Iteration 2577, loss = 0.36193136\n",
            "Iteration 2578, loss = 0.36181030\n",
            "Iteration 2579, loss = 0.36164784\n",
            "Iteration 2580, loss = 0.36149469\n",
            "Iteration 2581, loss = 0.36133653\n",
            "Iteration 2582, loss = 0.36118522\n",
            "Iteration 2583, loss = 0.36103530\n",
            "Iteration 2584, loss = 0.36091082\n",
            "Iteration 2585, loss = 0.36073430\n",
            "Iteration 2586, loss = 0.36057665\n",
            "Iteration 2587, loss = 0.36048683\n",
            "Iteration 2588, loss = 0.36029639\n",
            "Iteration 2589, loss = 0.36014504\n",
            "Iteration 2590, loss = 0.35998786\n",
            "Iteration 2591, loss = 0.35985799\n",
            "Iteration 2592, loss = 0.35969691\n",
            "Iteration 2593, loss = 0.35956845\n",
            "Iteration 2594, loss = 0.35939883\n",
            "Iteration 2595, loss = 0.35925557\n",
            "Iteration 2596, loss = 0.35909349\n",
            "Iteration 2597, loss = 0.35897789\n",
            "Iteration 2598, loss = 0.35880054\n",
            "Iteration 2599, loss = 0.35865798\n",
            "Iteration 2600, loss = 0.35851580\n",
            "Iteration 2601, loss = 0.35836634\n",
            "Iteration 2602, loss = 0.35822667\n",
            "Iteration 2603, loss = 0.35806928\n",
            "Iteration 2604, loss = 0.35791759\n",
            "Iteration 2605, loss = 0.35777209\n",
            "Iteration 2606, loss = 0.35762949\n",
            "Iteration 2607, loss = 0.35747546\n",
            "Iteration 2608, loss = 0.35732844\n",
            "Iteration 2609, loss = 0.35720039\n",
            "Iteration 2610, loss = 0.35707054\n",
            "Iteration 2611, loss = 0.35689523\n",
            "Iteration 2612, loss = 0.35672842\n",
            "Iteration 2613, loss = 0.35658952\n",
            "Iteration 2614, loss = 0.35650525\n",
            "Iteration 2615, loss = 0.35634827\n",
            "Iteration 2616, loss = 0.35619766\n",
            "Iteration 2617, loss = 0.35606185\n",
            "Iteration 2618, loss = 0.35591513\n",
            "Iteration 2619, loss = 0.35578412\n",
            "Iteration 2620, loss = 0.35563224\n",
            "Iteration 2621, loss = 0.35549625\n",
            "Iteration 2622, loss = 0.35539962\n",
            "Iteration 2623, loss = 0.35523526\n",
            "Iteration 2624, loss = 0.35507607\n",
            "Iteration 2625, loss = 0.35492787\n",
            "Iteration 2626, loss = 0.35477022\n",
            "Iteration 2627, loss = 0.35464366\n",
            "Iteration 2628, loss = 0.35450224\n",
            "Iteration 2629, loss = 0.35433210\n",
            "Iteration 2630, loss = 0.35416361\n",
            "Iteration 2631, loss = 0.35403048\n",
            "Iteration 2632, loss = 0.35386621\n",
            "Iteration 2633, loss = 0.35373184\n",
            "Iteration 2634, loss = 0.35359690\n",
            "Iteration 2635, loss = 0.35345587\n",
            "Iteration 2636, loss = 0.35331319\n",
            "Iteration 2637, loss = 0.35315038\n",
            "Iteration 2638, loss = 0.35301084\n",
            "Iteration 2639, loss = 0.35287073\n",
            "Iteration 2640, loss = 0.35271202\n",
            "Iteration 2641, loss = 0.35257934\n",
            "Iteration 2642, loss = 0.35241955\n",
            "Iteration 2643, loss = 0.35228619\n",
            "Iteration 2644, loss = 0.35214637\n",
            "Iteration 2645, loss = 0.35198084\n",
            "Iteration 2646, loss = 0.35187223\n",
            "Iteration 2647, loss = 0.35173390\n",
            "Iteration 2648, loss = 0.35156485\n",
            "Iteration 2649, loss = 0.35146450\n",
            "Iteration 2650, loss = 0.35132162\n",
            "Iteration 2651, loss = 0.35118254\n",
            "Iteration 2652, loss = 0.35106851\n",
            "Iteration 2653, loss = 0.35086273\n",
            "Iteration 2654, loss = 0.35074606\n",
            "Iteration 2655, loss = 0.35060257\n",
            "Iteration 2656, loss = 0.35043157\n",
            "Iteration 2657, loss = 0.35030115\n",
            "Iteration 2658, loss = 0.35015697\n",
            "Iteration 2659, loss = 0.35003884\n",
            "Iteration 2660, loss = 0.34986539\n",
            "Iteration 2661, loss = 0.34972060\n",
            "Iteration 2662, loss = 0.34959212\n",
            "Iteration 2663, loss = 0.34945422\n",
            "Iteration 2664, loss = 0.34929871\n",
            "Iteration 2665, loss = 0.34915703\n",
            "Iteration 2666, loss = 0.34901704\n",
            "Iteration 2667, loss = 0.34887243\n",
            "Iteration 2668, loss = 0.34875847\n",
            "Iteration 2669, loss = 0.34864574\n",
            "Iteration 2670, loss = 0.34846708\n",
            "Iteration 2671, loss = 0.34834683\n",
            "Iteration 2672, loss = 0.34820610\n",
            "Iteration 2673, loss = 0.34809296\n",
            "Iteration 2674, loss = 0.34789601\n",
            "Iteration 2675, loss = 0.34775280\n",
            "Iteration 2676, loss = 0.34763777\n",
            "Iteration 2677, loss = 0.34748112\n",
            "Iteration 2678, loss = 0.34735542\n",
            "Iteration 2679, loss = 0.34720261\n",
            "Iteration 2680, loss = 0.34704951\n",
            "Iteration 2681, loss = 0.34694822\n",
            "Iteration 2682, loss = 0.34680144\n",
            "Iteration 2683, loss = 0.34666330\n",
            "Iteration 2684, loss = 0.34653824\n",
            "Iteration 2685, loss = 0.34638756\n",
            "Iteration 2686, loss = 0.34626705\n",
            "Iteration 2687, loss = 0.34611662\n",
            "Iteration 2688, loss = 0.34599039\n",
            "Iteration 2689, loss = 0.34585262\n",
            "Iteration 2690, loss = 0.34571624\n",
            "Iteration 2691, loss = 0.34557687\n",
            "Iteration 2692, loss = 0.34544597\n",
            "Iteration 2693, loss = 0.34528861\n",
            "Iteration 2694, loss = 0.34514881\n",
            "Iteration 2695, loss = 0.34501209\n",
            "Iteration 2696, loss = 0.34490320\n",
            "Iteration 2697, loss = 0.34475421\n",
            "Iteration 2698, loss = 0.34462067\n",
            "Iteration 2699, loss = 0.34450459\n",
            "Iteration 2700, loss = 0.34435286\n",
            "Iteration 2701, loss = 0.34422141\n",
            "Iteration 2702, loss = 0.34409706\n",
            "Iteration 2703, loss = 0.34394281\n",
            "Iteration 2704, loss = 0.34379190\n",
            "Iteration 2705, loss = 0.34367026\n",
            "Iteration 2706, loss = 0.34355086\n",
            "Iteration 2707, loss = 0.34340805\n",
            "Iteration 2708, loss = 0.34329869\n",
            "Iteration 2709, loss = 0.34313423\n",
            "Iteration 2710, loss = 0.34300175\n",
            "Iteration 2711, loss = 0.34286220\n",
            "Iteration 2712, loss = 0.34273894\n",
            "Iteration 2713, loss = 0.34265834\n",
            "Iteration 2714, loss = 0.34246966\n",
            "Iteration 2715, loss = 0.34233177\n",
            "Iteration 2716, loss = 0.34217124\n",
            "Iteration 2717, loss = 0.34205664\n",
            "Iteration 2718, loss = 0.34188934\n",
            "Iteration 2719, loss = 0.34174884\n",
            "Iteration 2720, loss = 0.34163590\n",
            "Iteration 2721, loss = 0.34150381\n",
            "Iteration 2722, loss = 0.34132924\n",
            "Iteration 2723, loss = 0.34118389\n",
            "Iteration 2724, loss = 0.34105892\n",
            "Iteration 2725, loss = 0.34090168\n",
            "Iteration 2726, loss = 0.34078602\n",
            "Iteration 2727, loss = 0.34066700\n",
            "Iteration 2728, loss = 0.34052444\n",
            "Iteration 2729, loss = 0.34036909\n",
            "Iteration 2730, loss = 0.34030508\n",
            "Iteration 2731, loss = 0.34014087\n",
            "Iteration 2732, loss = 0.34010769\n",
            "Iteration 2733, loss = 0.33990530\n",
            "Iteration 2734, loss = 0.33974690\n",
            "Iteration 2735, loss = 0.33956951\n",
            "Iteration 2736, loss = 0.33942025\n",
            "Iteration 2737, loss = 0.33925019\n",
            "Iteration 2738, loss = 0.33911775\n",
            "Iteration 2739, loss = 0.33905282\n",
            "Iteration 2740, loss = 0.33889174\n",
            "Iteration 2741, loss = 0.33875548\n",
            "Iteration 2742, loss = 0.33865204\n",
            "Iteration 2743, loss = 0.33848453\n",
            "Iteration 2744, loss = 0.33833746\n",
            "Iteration 2745, loss = 0.33820120\n",
            "Iteration 2746, loss = 0.33804731\n",
            "Iteration 2747, loss = 0.33790203\n",
            "Iteration 2748, loss = 0.33780649\n",
            "Iteration 2749, loss = 0.33768174\n",
            "Iteration 2750, loss = 0.33757029\n",
            "Iteration 2751, loss = 0.33737255\n",
            "Iteration 2752, loss = 0.33724549\n",
            "Iteration 2753, loss = 0.33713334\n",
            "Iteration 2754, loss = 0.33704136\n",
            "Iteration 2755, loss = 0.33693930\n",
            "Iteration 2756, loss = 0.33679004\n",
            "Iteration 2757, loss = 0.33663088\n",
            "Iteration 2758, loss = 0.33653325\n",
            "Iteration 2759, loss = 0.33638537\n",
            "Iteration 2760, loss = 0.33624118\n",
            "Iteration 2761, loss = 0.33611260\n",
            "Iteration 2762, loss = 0.33592519\n",
            "Iteration 2763, loss = 0.33575386\n",
            "Iteration 2764, loss = 0.33561191\n",
            "Iteration 2765, loss = 0.33547891\n",
            "Iteration 2766, loss = 0.33537900\n",
            "Iteration 2767, loss = 0.33523947\n",
            "Iteration 2768, loss = 0.33513860\n",
            "Iteration 2769, loss = 0.33497483\n",
            "Iteration 2770, loss = 0.33480632\n",
            "Iteration 2771, loss = 0.33467869\n",
            "Iteration 2772, loss = 0.33453266\n",
            "Iteration 2773, loss = 0.33438466\n",
            "Iteration 2774, loss = 0.33425931\n",
            "Iteration 2775, loss = 0.33412321\n",
            "Iteration 2776, loss = 0.33400783\n",
            "Iteration 2777, loss = 0.33385603\n",
            "Iteration 2778, loss = 0.33370889\n",
            "Iteration 2779, loss = 0.33357983\n",
            "Iteration 2780, loss = 0.33343999\n",
            "Iteration 2781, loss = 0.33332369\n",
            "Iteration 2782, loss = 0.33319750\n",
            "Iteration 2783, loss = 0.33307749\n",
            "Iteration 2784, loss = 0.33295082\n",
            "Iteration 2785, loss = 0.33280155\n",
            "Iteration 2786, loss = 0.33268552\n",
            "Iteration 2787, loss = 0.33254725\n",
            "Iteration 2788, loss = 0.33243824\n",
            "Iteration 2789, loss = 0.33230652\n",
            "Iteration 2790, loss = 0.33222550\n",
            "Iteration 2791, loss = 0.33208835\n",
            "Iteration 2792, loss = 0.33194438\n",
            "Iteration 2793, loss = 0.33181481\n",
            "Iteration 2794, loss = 0.33169655\n",
            "Iteration 2795, loss = 0.33155685\n",
            "Iteration 2796, loss = 0.33140894\n",
            "Iteration 2797, loss = 0.33127667\n",
            "Iteration 2798, loss = 0.33113458\n",
            "Iteration 2799, loss = 0.33099832\n",
            "Iteration 2800, loss = 0.33087738\n",
            "Iteration 2801, loss = 0.33076854\n",
            "Iteration 2802, loss = 0.33064157\n",
            "Iteration 2803, loss = 0.33055444\n",
            "Iteration 2804, loss = 0.33041158\n",
            "Iteration 2805, loss = 0.33024898\n",
            "Iteration 2806, loss = 0.33015796\n",
            "Iteration 2807, loss = 0.32997084\n",
            "Iteration 2808, loss = 0.32984520\n",
            "Iteration 2809, loss = 0.32970680\n",
            "Iteration 2810, loss = 0.32956034\n",
            "Iteration 2811, loss = 0.32946508\n",
            "Iteration 2812, loss = 0.32927460\n",
            "Iteration 2813, loss = 0.32916129\n",
            "Iteration 2814, loss = 0.32903674\n",
            "Iteration 2815, loss = 0.32889982\n",
            "Iteration 2816, loss = 0.32876651\n",
            "Iteration 2817, loss = 0.32865679\n",
            "Iteration 2818, loss = 0.32849871\n",
            "Iteration 2819, loss = 0.32835976\n",
            "Iteration 2820, loss = 0.32823857\n",
            "Iteration 2821, loss = 0.32811515\n",
            "Iteration 2822, loss = 0.32795851\n",
            "Iteration 2823, loss = 0.32783811\n",
            "Iteration 2824, loss = 0.32774581\n",
            "Iteration 2825, loss = 0.32765644\n",
            "Iteration 2826, loss = 0.32748681\n",
            "Iteration 2827, loss = 0.32733308\n",
            "Iteration 2828, loss = 0.32720992\n",
            "Iteration 2829, loss = 0.32709025\n",
            "Iteration 2830, loss = 0.32694736\n",
            "Iteration 2831, loss = 0.32681554\n",
            "Iteration 2832, loss = 0.32670654\n",
            "Iteration 2833, loss = 0.32656209\n",
            "Iteration 2834, loss = 0.32643204\n",
            "Iteration 2835, loss = 0.32636655\n",
            "Iteration 2836, loss = 0.32624447\n",
            "Iteration 2837, loss = 0.32608950\n",
            "Iteration 2838, loss = 0.32597626\n",
            "Iteration 2839, loss = 0.32582336\n",
            "Iteration 2840, loss = 0.32568354\n",
            "Iteration 2841, loss = 0.32554082\n",
            "Iteration 2842, loss = 0.32541103\n",
            "Iteration 2843, loss = 0.32527916\n",
            "Iteration 2844, loss = 0.32513961\n",
            "Iteration 2845, loss = 0.32499467\n",
            "Iteration 2846, loss = 0.32487362\n",
            "Iteration 2847, loss = 0.32473737\n",
            "Iteration 2848, loss = 0.32459788\n",
            "Iteration 2849, loss = 0.32448321\n",
            "Iteration 2850, loss = 0.32435386\n",
            "Iteration 2851, loss = 0.32420158\n",
            "Iteration 2852, loss = 0.32408376\n",
            "Iteration 2853, loss = 0.32401625\n",
            "Iteration 2854, loss = 0.32382518\n",
            "Iteration 2855, loss = 0.32368186\n",
            "Iteration 2856, loss = 0.32358536\n",
            "Iteration 2857, loss = 0.32343635\n",
            "Iteration 2858, loss = 0.32332639\n",
            "Iteration 2859, loss = 0.32318161\n",
            "Iteration 2860, loss = 0.32305569\n",
            "Iteration 2861, loss = 0.32291835\n",
            "Iteration 2862, loss = 0.32279363\n",
            "Iteration 2863, loss = 0.32267813\n",
            "Iteration 2864, loss = 0.32255853\n",
            "Iteration 2865, loss = 0.32243655\n",
            "Iteration 2866, loss = 0.32232593\n",
            "Iteration 2867, loss = 0.32220415\n",
            "Iteration 2868, loss = 0.32207809\n",
            "Iteration 2869, loss = 0.32195507\n",
            "Iteration 2870, loss = 0.32180386\n",
            "Iteration 2871, loss = 0.32172727\n",
            "Iteration 2872, loss = 0.32156318\n",
            "Iteration 2873, loss = 0.32142951\n",
            "Iteration 2874, loss = 0.32131649\n",
            "Iteration 2875, loss = 0.32117861\n",
            "Iteration 2876, loss = 0.32104383\n",
            "Iteration 2877, loss = 0.32090994\n",
            "Iteration 2878, loss = 0.32078924\n",
            "Iteration 2879, loss = 0.32065617\n",
            "Iteration 2880, loss = 0.32055089\n",
            "Iteration 2881, loss = 0.32041901\n",
            "Iteration 2882, loss = 0.32030074\n",
            "Iteration 2883, loss = 0.32016482\n",
            "Iteration 2884, loss = 0.32003797\n",
            "Iteration 2885, loss = 0.31990672\n",
            "Iteration 2886, loss = 0.31980251\n",
            "Iteration 2887, loss = 0.31963556\n",
            "Iteration 2888, loss = 0.31950220\n",
            "Iteration 2889, loss = 0.31941969\n",
            "Iteration 2890, loss = 0.31926214\n",
            "Iteration 2891, loss = 0.31914409\n",
            "Iteration 2892, loss = 0.31899664\n",
            "Iteration 2893, loss = 0.31888277\n",
            "Iteration 2894, loss = 0.31874304\n",
            "Iteration 2895, loss = 0.31862614\n",
            "Iteration 2896, loss = 0.31850235\n",
            "Iteration 2897, loss = 0.31836610\n",
            "Iteration 2898, loss = 0.31824425\n",
            "Iteration 2899, loss = 0.31811841\n",
            "Iteration 2900, loss = 0.31800341\n",
            "Iteration 2901, loss = 0.31787895\n",
            "Iteration 2902, loss = 0.31776411\n",
            "Iteration 2903, loss = 0.31764361\n",
            "Iteration 2904, loss = 0.31752257\n",
            "Iteration 2905, loss = 0.31737324\n",
            "Iteration 2906, loss = 0.31724986\n",
            "Iteration 2907, loss = 0.31713779\n",
            "Iteration 2908, loss = 0.31706213\n",
            "Iteration 2909, loss = 0.31689981\n",
            "Iteration 2910, loss = 0.31679555\n",
            "Iteration 2911, loss = 0.31666799\n",
            "Iteration 2912, loss = 0.31651342\n",
            "Iteration 2913, loss = 0.31637310\n",
            "Iteration 2914, loss = 0.31627280\n",
            "Iteration 2915, loss = 0.31616216\n",
            "Iteration 2916, loss = 0.31597718\n",
            "Iteration 2917, loss = 0.31592625\n",
            "Iteration 2918, loss = 0.31580605\n",
            "Iteration 2919, loss = 0.31567315\n",
            "Iteration 2920, loss = 0.31557766\n",
            "Iteration 2921, loss = 0.31546534\n",
            "Iteration 2922, loss = 0.31533186\n",
            "Iteration 2923, loss = 0.31520700\n",
            "Iteration 2924, loss = 0.31508700\n",
            "Iteration 2925, loss = 0.31495473\n",
            "Iteration 2926, loss = 0.31483469\n",
            "Iteration 2927, loss = 0.31470869\n",
            "Iteration 2928, loss = 0.31455986\n",
            "Iteration 2929, loss = 0.31444004\n",
            "Iteration 2930, loss = 0.31430612\n",
            "Iteration 2931, loss = 0.31414663\n",
            "Iteration 2932, loss = 0.31410341\n",
            "Iteration 2933, loss = 0.31396675\n",
            "Iteration 2934, loss = 0.31384528\n",
            "Iteration 2935, loss = 0.31373214\n",
            "Iteration 2936, loss = 0.31363103\n",
            "Iteration 2937, loss = 0.31350693\n",
            "Iteration 2938, loss = 0.31339879\n",
            "Iteration 2939, loss = 0.31329667\n",
            "Iteration 2940, loss = 0.31316871\n",
            "Iteration 2941, loss = 0.31304611\n",
            "Iteration 2942, loss = 0.31292797\n",
            "Iteration 2943, loss = 0.31279991\n",
            "Iteration 2944, loss = 0.31267027\n",
            "Iteration 2945, loss = 0.31254489\n",
            "Iteration 2946, loss = 0.31241520\n",
            "Iteration 2947, loss = 0.31231645\n",
            "Iteration 2948, loss = 0.31218908\n",
            "Iteration 2949, loss = 0.31207682\n",
            "Iteration 2950, loss = 0.31197453\n",
            "Iteration 2951, loss = 0.31182225\n",
            "Iteration 2952, loss = 0.31169990\n",
            "Iteration 2953, loss = 0.31163908\n",
            "Iteration 2954, loss = 0.31146635\n",
            "Iteration 2955, loss = 0.31135383\n",
            "Iteration 2956, loss = 0.31120373\n",
            "Iteration 2957, loss = 0.31109628\n",
            "Iteration 2958, loss = 0.31099168\n",
            "Iteration 2959, loss = 0.31087108\n",
            "Iteration 2960, loss = 0.31074507\n",
            "Iteration 2961, loss = 0.31060520\n",
            "Iteration 2962, loss = 0.31048211\n",
            "Iteration 2963, loss = 0.31037906\n",
            "Iteration 2964, loss = 0.31027541\n",
            "Iteration 2965, loss = 0.31009750\n",
            "Iteration 2966, loss = 0.31003282\n",
            "Iteration 2967, loss = 0.30991309\n",
            "Iteration 2968, loss = 0.30980119\n",
            "Iteration 2969, loss = 0.30969653\n",
            "Iteration 2970, loss = 0.30956637\n",
            "Iteration 2971, loss = 0.30945564\n",
            "Iteration 2972, loss = 0.30935913\n",
            "Iteration 2973, loss = 0.30920544\n",
            "Iteration 2974, loss = 0.30908808\n",
            "Iteration 2975, loss = 0.30896560\n",
            "Iteration 2976, loss = 0.30883076\n",
            "Iteration 2977, loss = 0.30869232\n",
            "Iteration 2978, loss = 0.30859021\n",
            "Iteration 2979, loss = 0.30845183\n",
            "Iteration 2980, loss = 0.30830206\n",
            "Iteration 2981, loss = 0.30828511\n",
            "Iteration 2982, loss = 0.30812496\n",
            "Iteration 2983, loss = 0.30802588\n",
            "Iteration 2984, loss = 0.30789836\n",
            "Iteration 2985, loss = 0.30777785\n",
            "Iteration 2986, loss = 0.30765900\n",
            "Iteration 2987, loss = 0.30753557\n",
            "Iteration 2988, loss = 0.30743322\n",
            "Iteration 2989, loss = 0.30732698\n",
            "Iteration 2990, loss = 0.30718770\n",
            "Iteration 2991, loss = 0.30707182\n",
            "Iteration 2992, loss = 0.30693652\n",
            "Iteration 2993, loss = 0.30685686\n",
            "Iteration 2994, loss = 0.30670743\n",
            "Iteration 2995, loss = 0.30659309\n",
            "Iteration 2996, loss = 0.30646539\n",
            "Iteration 2997, loss = 0.30641946\n",
            "Iteration 2998, loss = 0.30630474\n",
            "Iteration 2999, loss = 0.30618721\n",
            "Iteration 3000, loss = 0.30608608\n",
            "Iteration 3001, loss = 0.30596025\n",
            "Iteration 3002, loss = 0.30586009\n",
            "Iteration 3003, loss = 0.30575017\n",
            "Iteration 3004, loss = 0.30561434\n",
            "Iteration 3005, loss = 0.30544747\n",
            "Iteration 3006, loss = 0.30537989\n",
            "Iteration 3007, loss = 0.30524006\n",
            "Iteration 3008, loss = 0.30511508\n",
            "Iteration 3009, loss = 0.30506130\n",
            "Iteration 3010, loss = 0.30490017\n",
            "Iteration 3011, loss = 0.30478745\n",
            "Iteration 3012, loss = 0.30465655\n",
            "Iteration 3013, loss = 0.30452922\n",
            "Iteration 3014, loss = 0.30441398\n",
            "Iteration 3015, loss = 0.30430031\n",
            "Iteration 3016, loss = 0.30418262\n",
            "Iteration 3017, loss = 0.30407284\n",
            "Iteration 3018, loss = 0.30397248\n",
            "Iteration 3019, loss = 0.30386763\n",
            "Iteration 3020, loss = 0.30373738\n",
            "Iteration 3021, loss = 0.30362602\n",
            "Iteration 3022, loss = 0.30351473\n",
            "Iteration 3023, loss = 0.30342153\n",
            "Iteration 3024, loss = 0.30330883\n",
            "Iteration 3025, loss = 0.30319590\n",
            "Iteration 3026, loss = 0.30308532\n",
            "Iteration 3027, loss = 0.30298398\n",
            "Iteration 3028, loss = 0.30284347\n",
            "Iteration 3029, loss = 0.30272349\n",
            "Iteration 3030, loss = 0.30261223\n",
            "Iteration 3031, loss = 0.30248917\n",
            "Iteration 3032, loss = 0.30238244\n",
            "Iteration 3033, loss = 0.30223485\n",
            "Iteration 3034, loss = 0.30213662\n",
            "Iteration 3035, loss = 0.30200651\n",
            "Iteration 3036, loss = 0.30191352\n",
            "Iteration 3037, loss = 0.30178346\n",
            "Iteration 3038, loss = 0.30164530\n",
            "Iteration 3039, loss = 0.30157938\n",
            "Iteration 3040, loss = 0.30145222\n",
            "Iteration 3041, loss = 0.30131219\n",
            "Iteration 3042, loss = 0.30119166\n",
            "Iteration 3043, loss = 0.30108016\n",
            "Iteration 3044, loss = 0.30098977\n",
            "Iteration 3045, loss = 0.30085771\n",
            "Iteration 3046, loss = 0.30072099\n",
            "Iteration 3047, loss = 0.30064561\n",
            "Iteration 3048, loss = 0.30056275\n",
            "Iteration 3049, loss = 0.30044435\n",
            "Iteration 3050, loss = 0.30035552\n",
            "Iteration 3051, loss = 0.30027892\n",
            "Iteration 3052, loss = 0.30015208\n",
            "Iteration 3053, loss = 0.30003536\n",
            "Iteration 3054, loss = 0.29990996\n",
            "Iteration 3055, loss = 0.29979331\n",
            "Iteration 3056, loss = 0.29967274\n",
            "Iteration 3057, loss = 0.29955435\n",
            "Iteration 3058, loss = 0.29945277\n",
            "Iteration 3059, loss = 0.29932169\n",
            "Iteration 3060, loss = 0.29921521\n",
            "Iteration 3061, loss = 0.29909931\n",
            "Iteration 3062, loss = 0.29895447\n",
            "Iteration 3063, loss = 0.29883093\n",
            "Iteration 3064, loss = 0.29872140\n",
            "Iteration 3065, loss = 0.29861556\n",
            "Iteration 3066, loss = 0.29851677\n",
            "Iteration 3067, loss = 0.29842307\n",
            "Iteration 3068, loss = 0.29831376\n",
            "Iteration 3069, loss = 0.29820174\n",
            "Iteration 3070, loss = 0.29808829\n",
            "Iteration 3071, loss = 0.29796655\n",
            "Iteration 3072, loss = 0.29787493\n",
            "Iteration 3073, loss = 0.29777018\n",
            "Iteration 3074, loss = 0.29765364\n",
            "Iteration 3075, loss = 0.29753185\n",
            "Iteration 3076, loss = 0.29741556\n",
            "Iteration 3077, loss = 0.29730742\n",
            "Iteration 3078, loss = 0.29722879\n",
            "Iteration 3079, loss = 0.29707408\n",
            "Iteration 3080, loss = 0.29694742\n",
            "Iteration 3081, loss = 0.29683509\n",
            "Iteration 3082, loss = 0.29674418\n",
            "Iteration 3083, loss = 0.29661234\n",
            "Iteration 3084, loss = 0.29652285\n",
            "Iteration 3085, loss = 0.29639443\n",
            "Iteration 3086, loss = 0.29631249\n",
            "Iteration 3087, loss = 0.29618317\n",
            "Iteration 3088, loss = 0.29607130\n",
            "Iteration 3089, loss = 0.29594285\n",
            "Iteration 3090, loss = 0.29583544\n",
            "Iteration 3091, loss = 0.29574460\n",
            "Iteration 3092, loss = 0.29560674\n",
            "Iteration 3093, loss = 0.29552325\n",
            "Iteration 3094, loss = 0.29543940\n",
            "Iteration 3095, loss = 0.29530833\n",
            "Iteration 3096, loss = 0.29520935\n",
            "Iteration 3097, loss = 0.29509858\n",
            "Iteration 3098, loss = 0.29494592\n",
            "Iteration 3099, loss = 0.29485467\n",
            "Iteration 3100, loss = 0.29474247\n",
            "Iteration 3101, loss = 0.29465857\n",
            "Iteration 3102, loss = 0.29453223\n",
            "Iteration 3103, loss = 0.29440617\n",
            "Iteration 3104, loss = 0.29427086\n",
            "Iteration 3105, loss = 0.29420659\n",
            "Iteration 3106, loss = 0.29404244\n",
            "Iteration 3107, loss = 0.29394201\n",
            "Iteration 3108, loss = 0.29383419\n",
            "Iteration 3109, loss = 0.29371377\n",
            "Iteration 3110, loss = 0.29359712\n",
            "Iteration 3111, loss = 0.29350743\n",
            "Iteration 3112, loss = 0.29337480\n",
            "Iteration 3113, loss = 0.29328925\n",
            "Iteration 3114, loss = 0.29318453\n",
            "Iteration 3115, loss = 0.29307279\n",
            "Iteration 3116, loss = 0.29296756\n",
            "Iteration 3117, loss = 0.29285794\n",
            "Iteration 3118, loss = 0.29274101\n",
            "Iteration 3119, loss = 0.29262344\n",
            "Iteration 3120, loss = 0.29252520\n",
            "Iteration 3121, loss = 0.29244385\n",
            "Iteration 3122, loss = 0.29232150\n",
            "Iteration 3123, loss = 0.29220416\n",
            "Iteration 3124, loss = 0.29209107\n",
            "Iteration 3125, loss = 0.29199532\n",
            "Iteration 3126, loss = 0.29187146\n",
            "Iteration 3127, loss = 0.29176961\n",
            "Iteration 3128, loss = 0.29168679\n",
            "Iteration 3129, loss = 0.29157582\n",
            "Iteration 3130, loss = 0.29145142\n",
            "Iteration 3131, loss = 0.29134321\n",
            "Iteration 3132, loss = 0.29124808\n",
            "Iteration 3133, loss = 0.29112659\n",
            "Iteration 3134, loss = 0.29102000\n",
            "Iteration 3135, loss = 0.29093846\n",
            "Iteration 3136, loss = 0.29079749\n",
            "Iteration 3137, loss = 0.29067877\n",
            "Iteration 3138, loss = 0.29059360\n",
            "Iteration 3139, loss = 0.29046575\n",
            "Iteration 3140, loss = 0.29035499\n",
            "Iteration 3141, loss = 0.29026349\n",
            "Iteration 3142, loss = 0.29018735\n",
            "Iteration 3143, loss = 0.29007502\n",
            "Iteration 3144, loss = 0.28992800\n",
            "Iteration 3145, loss = 0.28984981\n",
            "Iteration 3146, loss = 0.28973962\n",
            "Iteration 3147, loss = 0.28960188\n",
            "Iteration 3148, loss = 0.28949220\n",
            "Iteration 3149, loss = 0.28939659\n",
            "Iteration 3150, loss = 0.28928607\n",
            "Iteration 3151, loss = 0.28919835\n",
            "Iteration 3152, loss = 0.28908093\n",
            "Iteration 3153, loss = 0.28895548\n",
            "Iteration 3154, loss = 0.28887282\n",
            "Iteration 3155, loss = 0.28877454\n",
            "Iteration 3156, loss = 0.28870674\n",
            "Iteration 3157, loss = 0.28860726\n",
            "Iteration 3158, loss = 0.28848721\n",
            "Iteration 3159, loss = 0.28851872\n",
            "Iteration 3160, loss = 0.28833345\n",
            "Iteration 3161, loss = 0.28820354\n",
            "Iteration 3162, loss = 0.28806505\n",
            "Iteration 3163, loss = 0.28800546\n",
            "Iteration 3164, loss = 0.28784017\n",
            "Iteration 3165, loss = 0.28773808\n",
            "Iteration 3166, loss = 0.28764629\n",
            "Iteration 3167, loss = 0.28754446\n",
            "Iteration 3168, loss = 0.28742151\n",
            "Iteration 3169, loss = 0.28731427\n",
            "Iteration 3170, loss = 0.28719445\n",
            "Iteration 3171, loss = 0.28708793\n",
            "Iteration 3172, loss = 0.28697016\n",
            "Iteration 3173, loss = 0.28685848\n",
            "Iteration 3174, loss = 0.28684406\n",
            "Iteration 3175, loss = 0.28666423\n",
            "Iteration 3176, loss = 0.28658010\n",
            "Iteration 3177, loss = 0.28644277\n",
            "Iteration 3178, loss = 0.28634102\n",
            "Iteration 3179, loss = 0.28622423\n",
            "Iteration 3180, loss = 0.28611555\n",
            "Iteration 3181, loss = 0.28602569\n",
            "Iteration 3182, loss = 0.28590064\n",
            "Iteration 3183, loss = 0.28579792\n",
            "Iteration 3184, loss = 0.28570286\n",
            "Iteration 3185, loss = 0.28560305\n",
            "Iteration 3186, loss = 0.28549437\n",
            "Iteration 3187, loss = 0.28541203\n",
            "Iteration 3188, loss = 0.28531714\n",
            "Iteration 3189, loss = 0.28520708\n",
            "Iteration 3190, loss = 0.28512237\n",
            "Iteration 3191, loss = 0.28499157\n",
            "Iteration 3192, loss = 0.28496187\n",
            "Iteration 3193, loss = 0.28481664\n",
            "Iteration 3194, loss = 0.28472472\n",
            "Iteration 3195, loss = 0.28460565\n",
            "Iteration 3196, loss = 0.28455960\n",
            "Iteration 3197, loss = 0.28442227\n",
            "Iteration 3198, loss = 0.28430418\n",
            "Iteration 3199, loss = 0.28418913\n",
            "Iteration 3200, loss = 0.28416066\n",
            "Iteration 3201, loss = 0.28401970\n",
            "Iteration 3202, loss = 0.28393765\n",
            "Iteration 3203, loss = 0.28384245\n",
            "Iteration 3204, loss = 0.28370647\n",
            "Iteration 3205, loss = 0.28357793\n",
            "Iteration 3206, loss = 0.28348711\n",
            "Iteration 3207, loss = 0.28340306\n",
            "Iteration 3208, loss = 0.28329676\n",
            "Iteration 3209, loss = 0.28321166\n",
            "Iteration 3210, loss = 0.28307071\n",
            "Iteration 3211, loss = 0.28295406\n",
            "Iteration 3212, loss = 0.28287666\n",
            "Iteration 3213, loss = 0.28276039\n",
            "Iteration 3214, loss = 0.28269371\n",
            "Iteration 3215, loss = 0.28257130\n",
            "Iteration 3216, loss = 0.28246207\n",
            "Iteration 3217, loss = 0.28235571\n",
            "Iteration 3218, loss = 0.28227435\n",
            "Iteration 3219, loss = 0.28214835\n",
            "Iteration 3220, loss = 0.28203989\n",
            "Iteration 3221, loss = 0.28194108\n",
            "Iteration 3222, loss = 0.28187005\n",
            "Iteration 3223, loss = 0.28174727\n",
            "Iteration 3224, loss = 0.28163787\n",
            "Iteration 3225, loss = 0.28154859\n",
            "Iteration 3226, loss = 0.28148934\n",
            "Iteration 3227, loss = 0.28139248\n",
            "Iteration 3228, loss = 0.28126679\n",
            "Iteration 3229, loss = 0.28115927\n",
            "Iteration 3230, loss = 0.28106535\n",
            "Iteration 3231, loss = 0.28094756\n",
            "Iteration 3232, loss = 0.28085390\n",
            "Iteration 3233, loss = 0.28073703\n",
            "Iteration 3234, loss = 0.28067092\n",
            "Iteration 3235, loss = 0.28055212\n",
            "Iteration 3236, loss = 0.28045556\n",
            "Iteration 3237, loss = 0.28035156\n",
            "Iteration 3238, loss = 0.28026127\n",
            "Iteration 3239, loss = 0.28013552\n",
            "Iteration 3240, loss = 0.28005038\n",
            "Iteration 3241, loss = 0.28002814\n",
            "Iteration 3242, loss = 0.27984389\n",
            "Iteration 3243, loss = 0.27974259\n",
            "Iteration 3244, loss = 0.27965907\n",
            "Iteration 3245, loss = 0.27954125\n",
            "Iteration 3246, loss = 0.27943248\n",
            "Iteration 3247, loss = 0.27932434\n",
            "Iteration 3248, loss = 0.27922310\n",
            "Iteration 3249, loss = 0.27919523\n",
            "Iteration 3250, loss = 0.27908476\n",
            "Iteration 3251, loss = 0.27899793\n",
            "Iteration 3252, loss = 0.27890555\n",
            "Iteration 3253, loss = 0.27880132\n",
            "Iteration 3254, loss = 0.27866986\n",
            "Iteration 3255, loss = 0.27857117\n",
            "Iteration 3256, loss = 0.27847692\n",
            "Iteration 3257, loss = 0.27836028\n",
            "Iteration 3258, loss = 0.27826311\n",
            "Iteration 3259, loss = 0.27816285\n",
            "Iteration 3260, loss = 0.27809789\n",
            "Iteration 3261, loss = 0.27798422\n",
            "Iteration 3262, loss = 0.27788960\n",
            "Iteration 3263, loss = 0.27779487\n",
            "Iteration 3264, loss = 0.27769558\n",
            "Iteration 3265, loss = 0.27760004\n",
            "Iteration 3266, loss = 0.27749591\n",
            "Iteration 3267, loss = 0.27737275\n",
            "Iteration 3268, loss = 0.27734538\n",
            "Iteration 3269, loss = 0.27717340\n",
            "Iteration 3270, loss = 0.27705677\n",
            "Iteration 3271, loss = 0.27697625\n",
            "Iteration 3272, loss = 0.27686740\n",
            "Iteration 3273, loss = 0.27677003\n",
            "Iteration 3274, loss = 0.27666158\n",
            "Iteration 3275, loss = 0.27659265\n",
            "Iteration 3276, loss = 0.27650698\n",
            "Iteration 3277, loss = 0.27645836\n",
            "Iteration 3278, loss = 0.27630850\n",
            "Iteration 3279, loss = 0.27620381\n",
            "Iteration 3280, loss = 0.27609930\n",
            "Iteration 3281, loss = 0.27599625\n",
            "Iteration 3282, loss = 0.27591292\n",
            "Iteration 3283, loss = 0.27578020\n",
            "Iteration 3284, loss = 0.27571951\n",
            "Iteration 3285, loss = 0.27561910\n",
            "Iteration 3286, loss = 0.27550002\n",
            "Iteration 3287, loss = 0.27539040\n",
            "Iteration 3288, loss = 0.27528115\n",
            "Iteration 3289, loss = 0.27518118\n",
            "Iteration 3290, loss = 0.27514137\n",
            "Iteration 3291, loss = 0.27497938\n",
            "Iteration 3292, loss = 0.27488019\n",
            "Iteration 3293, loss = 0.27480632\n",
            "Iteration 3294, loss = 0.27467793\n",
            "Iteration 3295, loss = 0.27458166\n",
            "Iteration 3296, loss = 0.27450231\n",
            "Iteration 3297, loss = 0.27437669\n",
            "Iteration 3298, loss = 0.27427953\n",
            "Iteration 3299, loss = 0.27419003\n",
            "Iteration 3300, loss = 0.27409765\n",
            "Iteration 3301, loss = 0.27399449\n",
            "Iteration 3302, loss = 0.27389343\n",
            "Iteration 3303, loss = 0.27379962\n",
            "Iteration 3304, loss = 0.27370019\n",
            "Iteration 3305, loss = 0.27361930\n",
            "Iteration 3306, loss = 0.27349285\n",
            "Iteration 3307, loss = 0.27340650\n",
            "Iteration 3308, loss = 0.27328960\n",
            "Iteration 3309, loss = 0.27319422\n",
            "Iteration 3310, loss = 0.27311304\n",
            "Iteration 3311, loss = 0.27302947\n",
            "Iteration 3312, loss = 0.27298593\n",
            "Iteration 3313, loss = 0.27281249\n",
            "Iteration 3314, loss = 0.27272100\n",
            "Iteration 3315, loss = 0.27265218\n",
            "Iteration 3316, loss = 0.27255668\n",
            "Iteration 3317, loss = 0.27245207\n",
            "Iteration 3318, loss = 0.27234313\n",
            "Iteration 3319, loss = 0.27222344\n",
            "Iteration 3320, loss = 0.27211706\n",
            "Iteration 3321, loss = 0.27203414\n",
            "Iteration 3322, loss = 0.27198015\n",
            "Iteration 3323, loss = 0.27192884\n",
            "Iteration 3324, loss = 0.27183351\n",
            "Iteration 3325, loss = 0.27176394\n",
            "Iteration 3326, loss = 0.27168326\n",
            "Iteration 3327, loss = 0.27159192\n",
            "Iteration 3328, loss = 0.27147706\n",
            "Iteration 3329, loss = 0.27137792\n",
            "Iteration 3330, loss = 0.27127434\n",
            "Iteration 3331, loss = 0.27116451\n",
            "Iteration 3332, loss = 0.27109826\n",
            "Iteration 3333, loss = 0.27104153\n",
            "Iteration 3334, loss = 0.27093330\n",
            "Iteration 3335, loss = 0.27082311\n",
            "Iteration 3336, loss = 0.27074379\n",
            "Iteration 3337, loss = 0.27064195\n",
            "Iteration 3338, loss = 0.27052984\n",
            "Iteration 3339, loss = 0.27044192\n",
            "Iteration 3340, loss = 0.27036216\n",
            "Iteration 3341, loss = 0.27023991\n",
            "Iteration 3342, loss = 0.27014039\n",
            "Iteration 3343, loss = 0.27003098\n",
            "Iteration 3344, loss = 0.26991800\n",
            "Iteration 3345, loss = 0.26986511\n",
            "Iteration 3346, loss = 0.26972048\n",
            "Iteration 3347, loss = 0.26961846\n",
            "Iteration 3348, loss = 0.26952553\n",
            "Iteration 3349, loss = 0.26943383\n",
            "Iteration 3350, loss = 0.26937849\n",
            "Iteration 3351, loss = 0.26928209\n",
            "Iteration 3352, loss = 0.26916906\n",
            "Iteration 3353, loss = 0.26905567\n",
            "Iteration 3354, loss = 0.26896130\n",
            "Iteration 3355, loss = 0.26885344\n",
            "Iteration 3356, loss = 0.26876238\n",
            "Iteration 3357, loss = 0.26873386\n",
            "Iteration 3358, loss = 0.26863672\n",
            "Iteration 3359, loss = 0.26859105\n",
            "Iteration 3360, loss = 0.26846844\n",
            "Iteration 3361, loss = 0.26838634\n",
            "Iteration 3362, loss = 0.26827319\n",
            "Iteration 3363, loss = 0.26816372\n",
            "Iteration 3364, loss = 0.26810412\n",
            "Iteration 3365, loss = 0.26798005\n",
            "Iteration 3366, loss = 0.26787467\n",
            "Iteration 3367, loss = 0.26776272\n",
            "Iteration 3368, loss = 0.26769162\n",
            "Iteration 3369, loss = 0.26757263\n",
            "Iteration 3370, loss = 0.26747492\n",
            "Iteration 3371, loss = 0.26738307\n",
            "Iteration 3372, loss = 0.26730787\n",
            "Iteration 3373, loss = 0.26722390\n",
            "Iteration 3374, loss = 0.26711603\n",
            "Iteration 3375, loss = 0.26704332\n",
            "Iteration 3376, loss = 0.26694519\n",
            "Iteration 3377, loss = 0.26685029\n",
            "Iteration 3378, loss = 0.26685848\n",
            "Iteration 3379, loss = 0.26666434\n",
            "Iteration 3380, loss = 0.26653518\n",
            "Iteration 3381, loss = 0.26650611\n",
            "Iteration 3382, loss = 0.26643979\n",
            "Iteration 3383, loss = 0.26630090\n",
            "Iteration 3384, loss = 0.26617653\n",
            "Iteration 3385, loss = 0.26610586\n",
            "Iteration 3386, loss = 0.26601875\n",
            "Iteration 3387, loss = 0.26593328\n",
            "Iteration 3388, loss = 0.26579738\n",
            "Iteration 3389, loss = 0.26573415\n",
            "Iteration 3390, loss = 0.26563522\n",
            "Iteration 3391, loss = 0.26554845\n",
            "Iteration 3392, loss = 0.26547605\n",
            "Iteration 3393, loss = 0.26534826\n",
            "Iteration 3394, loss = 0.26526923\n",
            "Iteration 3395, loss = 0.26515405\n",
            "Iteration 3396, loss = 0.26508344\n",
            "Iteration 3397, loss = 0.26498430\n",
            "Iteration 3398, loss = 0.26497834\n",
            "Iteration 3399, loss = 0.26479707\n",
            "Iteration 3400, loss = 0.26470317\n",
            "Iteration 3401, loss = 0.26461817\n",
            "Iteration 3402, loss = 0.26455312\n",
            "Iteration 3403, loss = 0.26444895\n",
            "Iteration 3404, loss = 0.26433657\n",
            "Iteration 3405, loss = 0.26431861\n",
            "Iteration 3406, loss = 0.26416571\n",
            "Iteration 3407, loss = 0.26410343\n",
            "Iteration 3408, loss = 0.26396742\n",
            "Iteration 3409, loss = 0.26388342\n",
            "Iteration 3410, loss = 0.26379941\n",
            "Iteration 3411, loss = 0.26372281\n",
            "Iteration 3412, loss = 0.26359968\n",
            "Iteration 3413, loss = 0.26359056\n",
            "Iteration 3414, loss = 0.26347599\n",
            "Iteration 3415, loss = 0.26336771\n",
            "Iteration 3416, loss = 0.26333318\n",
            "Iteration 3417, loss = 0.26322315\n",
            "Iteration 3418, loss = 0.26316029\n",
            "Iteration 3419, loss = 0.26302718\n",
            "Iteration 3420, loss = 0.26292875\n",
            "Iteration 3421, loss = 0.26285690\n",
            "Iteration 3422, loss = 0.26275820\n",
            "Iteration 3423, loss = 0.26266397\n",
            "Iteration 3424, loss = 0.26256945\n",
            "Iteration 3425, loss = 0.26251426\n",
            "Iteration 3426, loss = 0.26240496\n",
            "Iteration 3427, loss = 0.26228947\n",
            "Iteration 3428, loss = 0.26221420\n",
            "Iteration 3429, loss = 0.26215523\n",
            "Iteration 3430, loss = 0.26205525\n",
            "Iteration 3431, loss = 0.26193795\n",
            "Iteration 3432, loss = 0.26189606\n",
            "Iteration 3433, loss = 0.26177326\n",
            "Iteration 3434, loss = 0.26167394\n",
            "Iteration 3435, loss = 0.26159157\n",
            "Iteration 3436, loss = 0.26151108\n",
            "Iteration 3437, loss = 0.26141650\n",
            "Iteration 3438, loss = 0.26133224\n",
            "Iteration 3439, loss = 0.26124798\n",
            "Iteration 3440, loss = 0.26114045\n",
            "Iteration 3441, loss = 0.26111731\n",
            "Iteration 3442, loss = 0.26108333\n",
            "Iteration 3443, loss = 0.26088425\n",
            "Iteration 3444, loss = 0.26090845\n",
            "Iteration 3445, loss = 0.26071407\n",
            "Iteration 3446, loss = 0.26062794\n",
            "Iteration 3447, loss = 0.26054547\n",
            "Iteration 3448, loss = 0.26047342\n",
            "Iteration 3449, loss = 0.26037665\n",
            "Iteration 3450, loss = 0.26026997\n",
            "Iteration 3451, loss = 0.26016468\n",
            "Iteration 3452, loss = 0.26008974\n",
            "Iteration 3453, loss = 0.26002161\n",
            "Iteration 3454, loss = 0.25992928\n",
            "Iteration 3455, loss = 0.25987801\n",
            "Iteration 3456, loss = 0.25976605\n",
            "Iteration 3457, loss = 0.25967537\n",
            "Iteration 3458, loss = 0.25956424\n",
            "Iteration 3459, loss = 0.25947282\n",
            "Iteration 3460, loss = 0.25940773\n",
            "Iteration 3461, loss = 0.25933852\n",
            "Iteration 3462, loss = 0.25924968\n",
            "Iteration 3463, loss = 0.25914394\n",
            "Iteration 3464, loss = 0.25903345\n",
            "Iteration 3465, loss = 0.25898288\n",
            "Iteration 3466, loss = 0.25887146\n",
            "Iteration 3467, loss = 0.25878242\n",
            "Iteration 3468, loss = 0.25870944\n",
            "Iteration 3469, loss = 0.25872550\n",
            "Iteration 3470, loss = 0.25855751\n",
            "Iteration 3471, loss = 0.25849077\n",
            "Iteration 3472, loss = 0.25837129\n",
            "Iteration 3473, loss = 0.25826078\n",
            "Iteration 3474, loss = 0.25819141\n",
            "Iteration 3475, loss = 0.25812637\n",
            "Iteration 3476, loss = 0.25804041\n",
            "Iteration 3477, loss = 0.25794117\n",
            "Iteration 3478, loss = 0.25788088\n",
            "Iteration 3479, loss = 0.25779170\n",
            "Iteration 3480, loss = 0.25771015\n",
            "Iteration 3481, loss = 0.25761895\n",
            "Iteration 3482, loss = 0.25751797\n",
            "Iteration 3483, loss = 0.25745664\n",
            "Iteration 3484, loss = 0.25737003\n",
            "Iteration 3485, loss = 0.25726758\n",
            "Iteration 3486, loss = 0.25717848\n",
            "Iteration 3487, loss = 0.25711354\n",
            "Iteration 3488, loss = 0.25702880\n",
            "Iteration 3489, loss = 0.25694245\n",
            "Iteration 3490, loss = 0.25686530\n",
            "Iteration 3491, loss = 0.25676577\n",
            "Iteration 3492, loss = 0.25668682\n",
            "Iteration 3493, loss = 0.25661345\n",
            "Iteration 3494, loss = 0.25650857\n",
            "Iteration 3495, loss = 0.25642983\n",
            "Iteration 3496, loss = 0.25634926\n",
            "Iteration 3497, loss = 0.25625018\n",
            "Iteration 3498, loss = 0.25618307\n",
            "Iteration 3499, loss = 0.25609919\n",
            "Iteration 3500, loss = 0.25602208\n",
            "Iteration 3501, loss = 0.25591848\n",
            "Iteration 3502, loss = 0.25582597\n",
            "Iteration 3503, loss = 0.25573431\n",
            "Iteration 3504, loss = 0.25564014\n",
            "Iteration 3505, loss = 0.25556311\n",
            "Iteration 3506, loss = 0.25547765\n",
            "Iteration 3507, loss = 0.25538216\n",
            "Iteration 3508, loss = 0.25528776\n",
            "Iteration 3509, loss = 0.25524031\n",
            "Iteration 3510, loss = 0.25512940\n",
            "Iteration 3511, loss = 0.25503590\n",
            "Iteration 3512, loss = 0.25495944\n",
            "Iteration 3513, loss = 0.25486632\n",
            "Iteration 3514, loss = 0.25478792\n",
            "Iteration 3515, loss = 0.25471064\n",
            "Iteration 3516, loss = 0.25465660\n",
            "Iteration 3517, loss = 0.25452335\n",
            "Iteration 3518, loss = 0.25443397\n",
            "Iteration 3519, loss = 0.25435949\n",
            "Iteration 3520, loss = 0.25429095\n",
            "Iteration 3521, loss = 0.25420340\n",
            "Iteration 3522, loss = 0.25410244\n",
            "Iteration 3523, loss = 0.25401914\n",
            "Iteration 3524, loss = 0.25395251\n",
            "Iteration 3525, loss = 0.25384330\n",
            "Iteration 3526, loss = 0.25383192\n",
            "Iteration 3527, loss = 0.25370775\n",
            "Iteration 3528, loss = 0.25360194\n",
            "Iteration 3529, loss = 0.25353843\n",
            "Iteration 3530, loss = 0.25340668\n",
            "Iteration 3531, loss = 0.25337926\n",
            "Iteration 3532, loss = 0.25323673\n",
            "Iteration 3533, loss = 0.25317865\n",
            "Iteration 3534, loss = 0.25315776\n",
            "Iteration 3535, loss = 0.25298710\n",
            "Iteration 3536, loss = 0.25289373\n",
            "Iteration 3537, loss = 0.25286634\n",
            "Iteration 3538, loss = 0.25273391\n",
            "Iteration 3539, loss = 0.25269990\n",
            "Iteration 3540, loss = 0.25256213\n",
            "Iteration 3541, loss = 0.25247472\n",
            "Iteration 3542, loss = 0.25241234\n",
            "Iteration 3543, loss = 0.25231884\n",
            "Iteration 3544, loss = 0.25223891\n",
            "Iteration 3545, loss = 0.25214079\n",
            "Iteration 3546, loss = 0.25209229\n",
            "Iteration 3547, loss = 0.25201190\n",
            "Iteration 3548, loss = 0.25192826\n",
            "Iteration 3549, loss = 0.25188151\n",
            "Iteration 3550, loss = 0.25188680\n",
            "Iteration 3551, loss = 0.25168454\n",
            "Iteration 3552, loss = 0.25159431\n",
            "Iteration 3553, loss = 0.25155186\n",
            "Iteration 3554, loss = 0.25141065\n",
            "Iteration 3555, loss = 0.25133130\n",
            "Iteration 3556, loss = 0.25131950\n",
            "Iteration 3557, loss = 0.25115996\n",
            "Iteration 3558, loss = 0.25105205\n",
            "Iteration 3559, loss = 0.25107483\n",
            "Iteration 3560, loss = 0.25095553\n",
            "Iteration 3561, loss = 0.25089185\n",
            "Iteration 3562, loss = 0.25086510\n",
            "Iteration 3563, loss = 0.25075999\n",
            "Iteration 3564, loss = 0.25069106\n",
            "Iteration 3565, loss = 0.25057292\n",
            "Iteration 3566, loss = 0.25051422\n",
            "Iteration 3567, loss = 0.25041099\n",
            "Iteration 3568, loss = 0.25030150\n",
            "Iteration 3569, loss = 0.25022140\n",
            "Iteration 3570, loss = 0.25012789\n",
            "Iteration 3571, loss = 0.25007202\n",
            "Iteration 3572, loss = 0.24996462\n",
            "Iteration 3573, loss = 0.24988438\n",
            "Iteration 3574, loss = 0.24985008\n",
            "Iteration 3575, loss = 0.24977318\n",
            "Iteration 3576, loss = 0.24971267\n",
            "Iteration 3577, loss = 0.24962131\n",
            "Iteration 3578, loss = 0.24956778\n",
            "Iteration 3579, loss = 0.24944831\n",
            "Iteration 3580, loss = 0.24937204\n",
            "Iteration 3581, loss = 0.24926693\n",
            "Iteration 3582, loss = 0.24916073\n",
            "Iteration 3583, loss = 0.24909007\n",
            "Iteration 3584, loss = 0.24899076\n",
            "Iteration 3585, loss = 0.24891628\n",
            "Iteration 3586, loss = 0.24884512\n",
            "Iteration 3587, loss = 0.24873105\n",
            "Iteration 3588, loss = 0.24867406\n",
            "Iteration 3589, loss = 0.24859224\n",
            "Iteration 3590, loss = 0.24848575\n",
            "Iteration 3591, loss = 0.24841712\n",
            "Iteration 3592, loss = 0.24835825\n",
            "Iteration 3593, loss = 0.24826551\n",
            "Iteration 3594, loss = 0.24818409\n",
            "Iteration 3595, loss = 0.24810402\n",
            "Iteration 3596, loss = 0.24801531\n",
            "Iteration 3597, loss = 0.24796521\n",
            "Iteration 3598, loss = 0.24790417\n",
            "Iteration 3599, loss = 0.24779657\n",
            "Iteration 3600, loss = 0.24770183\n",
            "Iteration 3601, loss = 0.24760537\n",
            "Iteration 3602, loss = 0.24757866\n",
            "Iteration 3603, loss = 0.24746044\n",
            "Iteration 3604, loss = 0.24739359\n",
            "Iteration 3605, loss = 0.24732641\n",
            "Iteration 3606, loss = 0.24724614\n",
            "Iteration 3607, loss = 0.24716717\n",
            "Iteration 3608, loss = 0.24709391\n",
            "Iteration 3609, loss = 0.24704756\n",
            "Iteration 3610, loss = 0.24696599\n",
            "Iteration 3611, loss = 0.24687323\n",
            "Iteration 3612, loss = 0.24679320\n",
            "Iteration 3613, loss = 0.24674006\n",
            "Iteration 3614, loss = 0.24664496\n",
            "Iteration 3615, loss = 0.24661454\n",
            "Iteration 3616, loss = 0.24652108\n",
            "Iteration 3617, loss = 0.24642432\n",
            "Iteration 3618, loss = 0.24633346\n",
            "Iteration 3619, loss = 0.24624110\n",
            "Iteration 3620, loss = 0.24614912\n",
            "Iteration 3621, loss = 0.24607003\n",
            "Iteration 3622, loss = 0.24599057\n",
            "Iteration 3623, loss = 0.24588191\n",
            "Iteration 3624, loss = 0.24582923\n",
            "Iteration 3625, loss = 0.24573676\n",
            "Iteration 3626, loss = 0.24565600\n",
            "Iteration 3627, loss = 0.24557573\n",
            "Iteration 3628, loss = 0.24549448\n",
            "Iteration 3629, loss = 0.24538739\n",
            "Iteration 3630, loss = 0.24531988\n",
            "Iteration 3631, loss = 0.24524598\n",
            "Iteration 3632, loss = 0.24517100\n",
            "Iteration 3633, loss = 0.24509073\n",
            "Iteration 3634, loss = 0.24502310\n",
            "Iteration 3635, loss = 0.24494657\n",
            "Iteration 3636, loss = 0.24485273\n",
            "Iteration 3637, loss = 0.24477621\n",
            "Iteration 3638, loss = 0.24468040\n",
            "Iteration 3639, loss = 0.24461054\n",
            "Iteration 3640, loss = 0.24452794\n",
            "Iteration 3641, loss = 0.24446141\n",
            "Iteration 3642, loss = 0.24438880\n",
            "Iteration 3643, loss = 0.24432675\n",
            "Iteration 3644, loss = 0.24426396\n",
            "Iteration 3645, loss = 0.24416809\n",
            "Iteration 3646, loss = 0.24408694\n",
            "Iteration 3647, loss = 0.24401534\n",
            "Iteration 3648, loss = 0.24392371\n",
            "Iteration 3649, loss = 0.24382827\n",
            "Iteration 3650, loss = 0.24377336\n",
            "Iteration 3651, loss = 0.24370789\n",
            "Iteration 3652, loss = 0.24361240\n",
            "Iteration 3653, loss = 0.24353090\n",
            "Iteration 3654, loss = 0.24345099\n",
            "Iteration 3655, loss = 0.24340534\n",
            "Iteration 3656, loss = 0.24330451\n",
            "Iteration 3657, loss = 0.24321276\n",
            "Iteration 3658, loss = 0.24313620\n",
            "Iteration 3659, loss = 0.24306750\n",
            "Iteration 3660, loss = 0.24297993\n",
            "Iteration 3661, loss = 0.24291952\n",
            "Iteration 3662, loss = 0.24294963\n",
            "Iteration 3663, loss = 0.24279604\n",
            "Iteration 3664, loss = 0.24270358\n",
            "Iteration 3665, loss = 0.24261144\n",
            "Iteration 3666, loss = 0.24253297\n",
            "Iteration 3667, loss = 0.24251808\n",
            "Iteration 3668, loss = 0.24238071\n",
            "Iteration 3669, loss = 0.24231512\n",
            "Iteration 3670, loss = 0.24223057\n",
            "Iteration 3671, loss = 0.24215442\n",
            "Iteration 3672, loss = 0.24207118\n",
            "Iteration 3673, loss = 0.24199791\n",
            "Iteration 3674, loss = 0.24192326\n",
            "Iteration 3675, loss = 0.24185284\n",
            "Iteration 3676, loss = 0.24177358\n",
            "Iteration 3677, loss = 0.24173789\n",
            "Iteration 3678, loss = 0.24162513\n",
            "Iteration 3679, loss = 0.24153212\n",
            "Iteration 3680, loss = 0.24148018\n",
            "Iteration 3681, loss = 0.24138473\n",
            "Iteration 3682, loss = 0.24132830\n",
            "Iteration 3683, loss = 0.24122818\n",
            "Iteration 3684, loss = 0.24119303\n",
            "Iteration 3685, loss = 0.24110404\n",
            "Iteration 3686, loss = 0.24101486\n",
            "Iteration 3687, loss = 0.24097690\n",
            "Iteration 3688, loss = 0.24088131\n",
            "Iteration 3689, loss = 0.24080710\n",
            "Iteration 3690, loss = 0.24072247\n",
            "Iteration 3691, loss = 0.24063906\n",
            "Iteration 3692, loss = 0.24058339\n",
            "Iteration 3693, loss = 0.24049690\n",
            "Iteration 3694, loss = 0.24039959\n",
            "Iteration 3695, loss = 0.24032919\n",
            "Iteration 3696, loss = 0.24027540\n",
            "Iteration 3697, loss = 0.24016210\n",
            "Iteration 3698, loss = 0.24009094\n",
            "Iteration 3699, loss = 0.24001581\n",
            "Iteration 3700, loss = 0.23995881\n",
            "Iteration 3701, loss = 0.23985651\n",
            "Iteration 3702, loss = 0.23977373\n",
            "Iteration 3703, loss = 0.23973281\n",
            "Iteration 3704, loss = 0.23964507\n",
            "Iteration 3705, loss = 0.23954543\n",
            "Iteration 3706, loss = 0.23947224\n",
            "Iteration 3707, loss = 0.23941504\n",
            "Iteration 3708, loss = 0.23932681\n",
            "Iteration 3709, loss = 0.23925009\n",
            "Iteration 3710, loss = 0.23916973\n",
            "Iteration 3711, loss = 0.23908856\n",
            "Iteration 3712, loss = 0.23903115\n",
            "Iteration 3713, loss = 0.23893394\n",
            "Iteration 3714, loss = 0.23886063\n",
            "Iteration 3715, loss = 0.23883133\n",
            "Iteration 3716, loss = 0.23871369\n",
            "Iteration 3717, loss = 0.23862248\n",
            "Iteration 3718, loss = 0.23855455\n",
            "Iteration 3719, loss = 0.23847320\n",
            "Iteration 3720, loss = 0.23840922\n",
            "Iteration 3721, loss = 0.23831941\n",
            "Iteration 3722, loss = 0.23826529\n",
            "Iteration 3723, loss = 0.23818780\n",
            "Iteration 3724, loss = 0.23814052\n",
            "Iteration 3725, loss = 0.23807034\n",
            "Iteration 3726, loss = 0.23796571\n",
            "Iteration 3727, loss = 0.23787225\n",
            "Iteration 3728, loss = 0.23779356\n",
            "Iteration 3729, loss = 0.23773966\n",
            "Iteration 3730, loss = 0.23764581\n",
            "Iteration 3731, loss = 0.23761093\n",
            "Iteration 3732, loss = 0.23752540\n",
            "Iteration 3733, loss = 0.23743928\n",
            "Iteration 3734, loss = 0.23736690\n",
            "Iteration 3735, loss = 0.23729915\n",
            "Iteration 3736, loss = 0.23720909\n",
            "Iteration 3737, loss = 0.23713007\n",
            "Iteration 3738, loss = 0.23704676\n",
            "Iteration 3739, loss = 0.23698963\n",
            "Iteration 3740, loss = 0.23694038\n",
            "Iteration 3741, loss = 0.23684062\n",
            "Iteration 3742, loss = 0.23679889\n",
            "Iteration 3743, loss = 0.23669038\n",
            "Iteration 3744, loss = 0.23661882\n",
            "Iteration 3745, loss = 0.23657373\n",
            "Iteration 3746, loss = 0.23645020\n",
            "Iteration 3747, loss = 0.23640735\n",
            "Iteration 3748, loss = 0.23630931\n",
            "Iteration 3749, loss = 0.23627470\n",
            "Iteration 3750, loss = 0.23615479\n",
            "Iteration 3751, loss = 0.23608678\n",
            "Iteration 3752, loss = 0.23598320\n",
            "Iteration 3753, loss = 0.23592375\n",
            "Iteration 3754, loss = 0.23583566\n",
            "Iteration 3755, loss = 0.23581755\n",
            "Iteration 3756, loss = 0.23569707\n",
            "Iteration 3757, loss = 0.23562244\n",
            "Iteration 3758, loss = 0.23553397\n",
            "Iteration 3759, loss = 0.23545310\n",
            "Iteration 3760, loss = 0.23539807\n",
            "Iteration 3761, loss = 0.23531248\n",
            "Iteration 3762, loss = 0.23526763\n",
            "Iteration 3763, loss = 0.23518484\n",
            "Iteration 3764, loss = 0.23513266\n",
            "Iteration 3765, loss = 0.23505949\n",
            "Iteration 3766, loss = 0.23496684\n",
            "Iteration 3767, loss = 0.23493025\n",
            "Iteration 3768, loss = 0.23482210\n",
            "Iteration 3769, loss = 0.23474541\n",
            "Iteration 3770, loss = 0.23468209\n",
            "Iteration 3771, loss = 0.23461104\n",
            "Iteration 3772, loss = 0.23453427\n",
            "Iteration 3773, loss = 0.23445571\n",
            "Iteration 3774, loss = 0.23440488\n",
            "Iteration 3775, loss = 0.23432082\n",
            "Iteration 3776, loss = 0.23421874\n",
            "Iteration 3777, loss = 0.23427263\n",
            "Iteration 3778, loss = 0.23411630\n",
            "Iteration 3779, loss = 0.23403865\n",
            "Iteration 3780, loss = 0.23394899\n",
            "Iteration 3781, loss = 0.23387749\n",
            "Iteration 3782, loss = 0.23379437\n",
            "Iteration 3783, loss = 0.23371389\n",
            "Iteration 3784, loss = 0.23362163\n",
            "Iteration 3785, loss = 0.23355813\n",
            "Iteration 3786, loss = 0.23348904\n",
            "Iteration 3787, loss = 0.23342467\n",
            "Iteration 3788, loss = 0.23334345\n",
            "Iteration 3789, loss = 0.23330802\n",
            "Iteration 3790, loss = 0.23319953\n",
            "Iteration 3791, loss = 0.23312163\n",
            "Iteration 3792, loss = 0.23307690\n",
            "Iteration 3793, loss = 0.23299477\n",
            "Iteration 3794, loss = 0.23296477\n",
            "Iteration 3795, loss = 0.23280971\n",
            "Iteration 3796, loss = 0.23278485\n",
            "Iteration 3797, loss = 0.23270347\n",
            "Iteration 3798, loss = 0.23262070\n",
            "Iteration 3799, loss = 0.23254333\n",
            "Iteration 3800, loss = 0.23251604\n",
            "Iteration 3801, loss = 0.23245503\n",
            "Iteration 3802, loss = 0.23237131\n",
            "Iteration 3803, loss = 0.23233984\n",
            "Iteration 3804, loss = 0.23223748\n",
            "Iteration 3805, loss = 0.23215608\n",
            "Iteration 3806, loss = 0.23208095\n",
            "Iteration 3807, loss = 0.23202169\n",
            "Iteration 3808, loss = 0.23196594\n",
            "Iteration 3809, loss = 0.23188313\n",
            "Iteration 3810, loss = 0.23178987\n",
            "Iteration 3811, loss = 0.23171795\n",
            "Iteration 3812, loss = 0.23162024\n",
            "Iteration 3813, loss = 0.23153264\n",
            "Iteration 3814, loss = 0.23153374\n",
            "Iteration 3815, loss = 0.23141872\n",
            "Iteration 3816, loss = 0.23136399\n",
            "Iteration 3817, loss = 0.23128201\n",
            "Iteration 3818, loss = 0.23120795\n",
            "Iteration 3819, loss = 0.23116343\n",
            "Iteration 3820, loss = 0.23106330\n",
            "Iteration 3821, loss = 0.23098119\n",
            "Iteration 3822, loss = 0.23093330\n",
            "Iteration 3823, loss = 0.23084481\n",
            "Iteration 3824, loss = 0.23077278\n",
            "Iteration 3825, loss = 0.23071753\n",
            "Iteration 3826, loss = 0.23062998\n",
            "Iteration 3827, loss = 0.23056730\n",
            "Iteration 3828, loss = 0.23049255\n",
            "Iteration 3829, loss = 0.23041202\n",
            "Iteration 3830, loss = 0.23036231\n",
            "Iteration 3831, loss = 0.23028764\n",
            "Iteration 3832, loss = 0.23018675\n",
            "Iteration 3833, loss = 0.23015783\n",
            "Iteration 3834, loss = 0.23006504\n",
            "Iteration 3835, loss = 0.22999303\n",
            "Iteration 3836, loss = 0.22993266\n",
            "Iteration 3837, loss = 0.22986418\n",
            "Iteration 3838, loss = 0.22977587\n",
            "Iteration 3839, loss = 0.22971635\n",
            "Iteration 3840, loss = 0.22964268\n",
            "Iteration 3841, loss = 0.22956874\n",
            "Iteration 3842, loss = 0.22949740\n",
            "Iteration 3843, loss = 0.22950139\n",
            "Iteration 3844, loss = 0.22939722\n",
            "Iteration 3845, loss = 0.22929252\n",
            "Iteration 3846, loss = 0.22925467\n",
            "Iteration 3847, loss = 0.22916375\n",
            "Iteration 3848, loss = 0.22908740\n",
            "Iteration 3849, loss = 0.22903234\n",
            "Iteration 3850, loss = 0.22892873\n",
            "Iteration 3851, loss = 0.22887578\n",
            "Iteration 3852, loss = 0.22881658\n",
            "Iteration 3853, loss = 0.22875981\n",
            "Iteration 3854, loss = 0.22875078\n",
            "Iteration 3855, loss = 0.22869621\n",
            "Iteration 3856, loss = 0.22859354\n",
            "Iteration 3857, loss = 0.22853417\n",
            "Iteration 3858, loss = 0.22843500\n",
            "Iteration 3859, loss = 0.22837522\n",
            "Iteration 3860, loss = 0.22828363\n",
            "Iteration 3861, loss = 0.22822138\n",
            "Iteration 3862, loss = 0.22814044\n",
            "Iteration 3863, loss = 0.22807035\n",
            "Iteration 3864, loss = 0.22799866\n",
            "Iteration 3865, loss = 0.22791916\n",
            "Iteration 3866, loss = 0.22785200\n",
            "Iteration 3867, loss = 0.22777940\n",
            "Iteration 3868, loss = 0.22775369\n",
            "Iteration 3869, loss = 0.22766750\n",
            "Iteration 3870, loss = 0.22762453\n",
            "Iteration 3871, loss = 0.22753055\n",
            "Iteration 3872, loss = 0.22744960\n",
            "Iteration 3873, loss = 0.22740195\n",
            "Iteration 3874, loss = 0.22730778\n",
            "Iteration 3875, loss = 0.22727071\n",
            "Iteration 3876, loss = 0.22715869\n",
            "Iteration 3877, loss = 0.22709363\n",
            "Iteration 3878, loss = 0.22701532\n",
            "Iteration 3879, loss = 0.22696940\n",
            "Iteration 3880, loss = 0.22690683\n",
            "Iteration 3881, loss = 0.22684916\n",
            "Iteration 3882, loss = 0.22675982\n",
            "Iteration 3883, loss = 0.22670471\n",
            "Iteration 3884, loss = 0.22666791\n",
            "Iteration 3885, loss = 0.22657409\n",
            "Iteration 3886, loss = 0.22646247\n",
            "Iteration 3887, loss = 0.22646710\n",
            "Iteration 3888, loss = 0.22636417\n",
            "Iteration 3889, loss = 0.22631209\n",
            "Iteration 3890, loss = 0.22627874\n",
            "Iteration 3891, loss = 0.22615585\n",
            "Iteration 3892, loss = 0.22610002\n",
            "Iteration 3893, loss = 0.22605963\n",
            "Iteration 3894, loss = 0.22592527\n",
            "Iteration 3895, loss = 0.22586356\n",
            "Iteration 3896, loss = 0.22579291\n",
            "Iteration 3897, loss = 0.22572842\n",
            "Iteration 3898, loss = 0.22565581\n",
            "Iteration 3899, loss = 0.22557243\n",
            "Iteration 3900, loss = 0.22555551\n",
            "Iteration 3901, loss = 0.22544628\n",
            "Iteration 3902, loss = 0.22537101\n",
            "Iteration 3903, loss = 0.22532804\n",
            "Iteration 3904, loss = 0.22525436\n",
            "Iteration 3905, loss = 0.22518435\n",
            "Iteration 3906, loss = 0.22512197\n",
            "Iteration 3907, loss = 0.22509146\n",
            "Iteration 3908, loss = 0.22503217\n",
            "Iteration 3909, loss = 0.22497452\n",
            "Iteration 3910, loss = 0.22489850\n",
            "Iteration 3911, loss = 0.22479583\n",
            "Iteration 3912, loss = 0.22473214\n",
            "Iteration 3913, loss = 0.22466441\n",
            "Iteration 3914, loss = 0.22460902\n",
            "Iteration 3915, loss = 0.22453915\n",
            "Iteration 3916, loss = 0.22444989\n",
            "Iteration 3917, loss = 0.22438790\n",
            "Iteration 3918, loss = 0.22432562\n",
            "Iteration 3919, loss = 0.22425077\n",
            "Iteration 3920, loss = 0.22422392\n",
            "Iteration 3921, loss = 0.22413083\n",
            "Iteration 3922, loss = 0.22406708\n",
            "Iteration 3923, loss = 0.22398308\n",
            "Iteration 3924, loss = 0.22392404\n",
            "Iteration 3925, loss = 0.22385196\n",
            "Iteration 3926, loss = 0.22380137\n",
            "Iteration 3927, loss = 0.22371540\n",
            "Iteration 3928, loss = 0.22366090\n",
            "Iteration 3929, loss = 0.22358019\n",
            "Iteration 3930, loss = 0.22353394\n",
            "Iteration 3931, loss = 0.22345706\n",
            "Iteration 3932, loss = 0.22338743\n",
            "Iteration 3933, loss = 0.22333715\n",
            "Iteration 3934, loss = 0.22326947\n",
            "Iteration 3935, loss = 0.22322033\n",
            "Iteration 3936, loss = 0.22317809\n",
            "Iteration 3937, loss = 0.22308935\n",
            "Iteration 3938, loss = 0.22302124\n",
            "Iteration 3939, loss = 0.22296372\n",
            "Iteration 3940, loss = 0.22289403\n",
            "Iteration 3941, loss = 0.22285584\n",
            "Iteration 3942, loss = 0.22280845\n",
            "Iteration 3943, loss = 0.22269151\n",
            "Iteration 3944, loss = 0.22264086\n",
            "Iteration 3945, loss = 0.22256811\n",
            "Iteration 3946, loss = 0.22250085\n",
            "Iteration 3947, loss = 0.22243036\n",
            "Iteration 3948, loss = 0.22239598\n",
            "Iteration 3949, loss = 0.22235210\n",
            "Iteration 3950, loss = 0.22226719\n",
            "Iteration 3951, loss = 0.22219144\n",
            "Iteration 3952, loss = 0.22212000\n",
            "Iteration 3953, loss = 0.22204640\n",
            "Iteration 3954, loss = 0.22201666\n",
            "Iteration 3955, loss = 0.22191910\n",
            "Iteration 3956, loss = 0.22185266\n",
            "Iteration 3957, loss = 0.22179450\n",
            "Iteration 3958, loss = 0.22173847\n",
            "Iteration 3959, loss = 0.22170500\n",
            "Iteration 3960, loss = 0.22160189\n",
            "Iteration 3961, loss = 0.22152081\n",
            "Iteration 3962, loss = 0.22145696\n",
            "Iteration 3963, loss = 0.22137189\n",
            "Iteration 3964, loss = 0.22134535\n",
            "Iteration 3965, loss = 0.22127069\n",
            "Iteration 3966, loss = 0.22122582\n",
            "Iteration 3967, loss = 0.22117575\n",
            "Iteration 3968, loss = 0.22110336\n",
            "Iteration 3969, loss = 0.22102674\n",
            "Iteration 3970, loss = 0.22095155\n",
            "Iteration 3971, loss = 0.22087956\n",
            "Iteration 3972, loss = 0.22081290\n",
            "Iteration 3973, loss = 0.22075789\n",
            "Iteration 3974, loss = 0.22069947\n",
            "Iteration 3975, loss = 0.22062444\n",
            "Iteration 3976, loss = 0.22056264\n",
            "Iteration 3977, loss = 0.22049616\n",
            "Iteration 3978, loss = 0.22043135\n",
            "Iteration 3979, loss = 0.22039135\n",
            "Iteration 3980, loss = 0.22035802\n",
            "Iteration 3981, loss = 0.22029299\n",
            "Iteration 3982, loss = 0.22019158\n",
            "Iteration 3983, loss = 0.22013037\n",
            "Iteration 3984, loss = 0.22005145\n",
            "Iteration 3985, loss = 0.22000726\n",
            "Iteration 3986, loss = 0.21998511\n",
            "Iteration 3987, loss = 0.21988306\n",
            "Iteration 3988, loss = 0.21980617\n",
            "Iteration 3989, loss = 0.21973774\n",
            "Iteration 3990, loss = 0.21965528\n",
            "Iteration 3991, loss = 0.21959637\n",
            "Iteration 3992, loss = 0.21959959\n",
            "Iteration 3993, loss = 0.21949966\n",
            "Iteration 3994, loss = 0.21940099\n",
            "Iteration 3995, loss = 0.21937069\n",
            "Iteration 3996, loss = 0.21929148\n",
            "Iteration 3997, loss = 0.21925079\n",
            "Iteration 3998, loss = 0.21918016\n",
            "Iteration 3999, loss = 0.21911375\n",
            "Iteration 4000, loss = 0.21907737\n",
            "Iteration 4001, loss = 0.21901315\n",
            "Iteration 4002, loss = 0.21894873\n",
            "Iteration 4003, loss = 0.21886460\n",
            "Iteration 4004, loss = 0.21880693\n",
            "Iteration 4005, loss = 0.21872626\n",
            "Iteration 4006, loss = 0.21868408\n",
            "Iteration 4007, loss = 0.21859700\n",
            "Iteration 4008, loss = 0.21855887\n",
            "Iteration 4009, loss = 0.21846826\n",
            "Iteration 4010, loss = 0.21842351\n",
            "Iteration 4011, loss = 0.21837284\n",
            "Iteration 4012, loss = 0.21829906\n",
            "Iteration 4013, loss = 0.21825496\n",
            "Iteration 4014, loss = 0.21817006\n",
            "Iteration 4015, loss = 0.21810807\n",
            "Iteration 4016, loss = 0.21804318\n",
            "Iteration 4017, loss = 0.21801398\n",
            "Iteration 4018, loss = 0.21793293\n",
            "Iteration 4019, loss = 0.21785779\n",
            "Iteration 4020, loss = 0.21779255\n",
            "Iteration 4021, loss = 0.21771527\n",
            "Iteration 4022, loss = 0.21765994\n",
            "Iteration 4023, loss = 0.21759614\n",
            "Iteration 4024, loss = 0.21751986\n",
            "Iteration 4025, loss = 0.21745052\n",
            "Iteration 4026, loss = 0.21739766\n",
            "Iteration 4027, loss = 0.21734374\n",
            "Iteration 4028, loss = 0.21727721\n",
            "Iteration 4029, loss = 0.21728771\n",
            "Iteration 4030, loss = 0.21717955\n",
            "Iteration 4031, loss = 0.21716287\n",
            "Iteration 4032, loss = 0.21702209\n",
            "Iteration 4033, loss = 0.21695953\n",
            "Iteration 4034, loss = 0.21696461\n",
            "Iteration 4035, loss = 0.21684382\n",
            "Iteration 4036, loss = 0.21676922\n",
            "Iteration 4037, loss = 0.21671429\n",
            "Iteration 4038, loss = 0.21667800\n",
            "Iteration 4039, loss = 0.21660245\n",
            "Iteration 4040, loss = 0.21655965\n",
            "Iteration 4041, loss = 0.21646029\n",
            "Iteration 4042, loss = 0.21640172\n",
            "Iteration 4043, loss = 0.21637960\n",
            "Iteration 4044, loss = 0.21628422\n",
            "Iteration 4045, loss = 0.21621668\n",
            "Iteration 4046, loss = 0.21615609\n",
            "Iteration 4047, loss = 0.21608557\n",
            "Iteration 4048, loss = 0.21603013\n",
            "Iteration 4049, loss = 0.21595738\n",
            "Iteration 4050, loss = 0.21590618\n",
            "Iteration 4051, loss = 0.21583508\n",
            "Iteration 4052, loss = 0.21578583\n",
            "Iteration 4053, loss = 0.21576006\n",
            "Iteration 4054, loss = 0.21569699\n",
            "Iteration 4055, loss = 0.21563712\n",
            "Iteration 4056, loss = 0.21560634\n",
            "Iteration 4057, loss = 0.21552567\n",
            "Iteration 4058, loss = 0.21547026\n",
            "Iteration 4059, loss = 0.21539311\n",
            "Iteration 4060, loss = 0.21532046\n",
            "Iteration 4061, loss = 0.21526339\n",
            "Iteration 4062, loss = 0.21521163\n",
            "Iteration 4063, loss = 0.21514303\n",
            "Iteration 4064, loss = 0.21507507\n",
            "Iteration 4065, loss = 0.21504776\n",
            "Iteration 4066, loss = 0.21494305\n",
            "Iteration 4067, loss = 0.21487917\n",
            "Iteration 4068, loss = 0.21482503\n",
            "Iteration 4069, loss = 0.21476644\n",
            "Iteration 4070, loss = 0.21469557\n",
            "Iteration 4071, loss = 0.21465220\n",
            "Iteration 4072, loss = 0.21459478\n",
            "Iteration 4073, loss = 0.21451651\n",
            "Iteration 4074, loss = 0.21443379\n",
            "Iteration 4075, loss = 0.21441812\n",
            "Iteration 4076, loss = 0.21434009\n",
            "Iteration 4077, loss = 0.21427398\n",
            "Iteration 4078, loss = 0.21424769\n",
            "Iteration 4079, loss = 0.21420196\n",
            "Iteration 4080, loss = 0.21408136\n",
            "Iteration 4081, loss = 0.21400794\n",
            "Iteration 4082, loss = 0.21395532\n",
            "Iteration 4083, loss = 0.21390968\n",
            "Iteration 4084, loss = 0.21384727\n",
            "Iteration 4085, loss = 0.21377345\n",
            "Iteration 4086, loss = 0.21373196\n",
            "Iteration 4087, loss = 0.21367508\n",
            "Iteration 4088, loss = 0.21364397\n",
            "Iteration 4089, loss = 0.21359812\n",
            "Iteration 4090, loss = 0.21353403\n",
            "Iteration 4091, loss = 0.21345145\n",
            "Iteration 4092, loss = 0.21339944\n",
            "Iteration 4093, loss = 0.21334216\n",
            "Iteration 4094, loss = 0.21326556\n",
            "Iteration 4095, loss = 0.21319280\n",
            "Iteration 4096, loss = 0.21315922\n",
            "Iteration 4097, loss = 0.21309225\n",
            "Iteration 4098, loss = 0.21304777\n",
            "Iteration 4099, loss = 0.21295830\n",
            "Iteration 4100, loss = 0.21296361\n",
            "Iteration 4101, loss = 0.21286485\n",
            "Iteration 4102, loss = 0.21277114\n",
            "Iteration 4103, loss = 0.21272851\n",
            "Iteration 4104, loss = 0.21267429\n",
            "Iteration 4105, loss = 0.21261197\n",
            "Iteration 4106, loss = 0.21255590\n",
            "Iteration 4107, loss = 0.21246232\n",
            "Iteration 4108, loss = 0.21242111\n",
            "Iteration 4109, loss = 0.21234658\n",
            "Iteration 4110, loss = 0.21230829\n",
            "Iteration 4111, loss = 0.21230579\n",
            "Iteration 4112, loss = 0.21225771\n",
            "Iteration 4113, loss = 0.21213761\n",
            "Iteration 4114, loss = 0.21205658\n",
            "Iteration 4115, loss = 0.21198044\n",
            "Iteration 4116, loss = 0.21190872\n",
            "Iteration 4117, loss = 0.21193126\n",
            "Iteration 4118, loss = 0.21184074\n",
            "Iteration 4119, loss = 0.21179543\n",
            "Iteration 4120, loss = 0.21170960\n",
            "Iteration 4121, loss = 0.21163422\n",
            "Iteration 4122, loss = 0.21160846\n",
            "Iteration 4123, loss = 0.21159109\n",
            "Iteration 4124, loss = 0.21153709\n",
            "Iteration 4125, loss = 0.21150431\n",
            "Iteration 4126, loss = 0.21142386\n",
            "Iteration 4127, loss = 0.21135777\n",
            "Iteration 4128, loss = 0.21127136\n",
            "Iteration 4129, loss = 0.21121186\n",
            "Iteration 4130, loss = 0.21116297\n",
            "Iteration 4131, loss = 0.21106238\n",
            "Iteration 4132, loss = 0.21099096\n",
            "Iteration 4133, loss = 0.21096532\n",
            "Iteration 4134, loss = 0.21088567\n",
            "Iteration 4135, loss = 0.21082683\n",
            "Iteration 4136, loss = 0.21075587\n",
            "Iteration 4137, loss = 0.21070423\n",
            "Iteration 4138, loss = 0.21064982\n",
            "Iteration 4139, loss = 0.21060381\n",
            "Iteration 4140, loss = 0.21051563\n",
            "Iteration 4141, loss = 0.21047184\n",
            "Iteration 4142, loss = 0.21042152\n",
            "Iteration 4143, loss = 0.21034216\n",
            "Iteration 4144, loss = 0.21028460\n",
            "Iteration 4145, loss = 0.21025897\n",
            "Iteration 4146, loss = 0.21018189\n",
            "Iteration 4147, loss = 0.21012550\n",
            "Iteration 4148, loss = 0.21006329\n",
            "Iteration 4149, loss = 0.21001109\n",
            "Iteration 4150, loss = 0.20992617\n",
            "Iteration 4151, loss = 0.20989269\n",
            "Iteration 4152, loss = 0.20982016\n",
            "Iteration 4153, loss = 0.20978081\n",
            "Iteration 4154, loss = 0.20975289\n",
            "Iteration 4155, loss = 0.20976552\n",
            "Iteration 4156, loss = 0.20964173\n",
            "Iteration 4157, loss = 0.20956595\n",
            "Iteration 4158, loss = 0.20950715\n",
            "Iteration 4159, loss = 0.20946911\n",
            "Iteration 4160, loss = 0.20937320\n",
            "Iteration 4161, loss = 0.20937638\n",
            "Iteration 4162, loss = 0.20925444\n",
            "Iteration 4163, loss = 0.20920693\n",
            "Iteration 4164, loss = 0.20912567\n",
            "Iteration 4165, loss = 0.20907909\n",
            "Iteration 4166, loss = 0.20903601\n",
            "Iteration 4167, loss = 0.20896861\n",
            "Iteration 4168, loss = 0.20892109\n",
            "Iteration 4169, loss = 0.20885173\n",
            "Iteration 4170, loss = 0.20879462\n",
            "Iteration 4171, loss = 0.20874009\n",
            "Iteration 4172, loss = 0.20868725\n",
            "Iteration 4173, loss = 0.20869627\n",
            "Iteration 4174, loss = 0.20858532\n",
            "Iteration 4175, loss = 0.20850960\n",
            "Iteration 4176, loss = 0.20846546\n",
            "Iteration 4177, loss = 0.20842052\n",
            "Iteration 4178, loss = 0.20834604\n",
            "Iteration 4179, loss = 0.20828155\n",
            "Iteration 4180, loss = 0.20821822\n",
            "Iteration 4181, loss = 0.20816041\n",
            "Iteration 4182, loss = 0.20811318\n",
            "Iteration 4183, loss = 0.20804669\n",
            "Iteration 4184, loss = 0.20799355\n",
            "Iteration 4185, loss = 0.20794493\n",
            "Iteration 4186, loss = 0.20789602\n",
            "Iteration 4187, loss = 0.20787750\n",
            "Iteration 4188, loss = 0.20777218\n",
            "Iteration 4189, loss = 0.20769669\n",
            "Iteration 4190, loss = 0.20763763\n",
            "Iteration 4191, loss = 0.20764695\n",
            "Iteration 4192, loss = 0.20756688\n",
            "Iteration 4193, loss = 0.20748878\n",
            "Iteration 4194, loss = 0.20742035\n",
            "Iteration 4195, loss = 0.20737533\n",
            "Iteration 4196, loss = 0.20732831\n",
            "Iteration 4197, loss = 0.20726612\n",
            "Iteration 4198, loss = 0.20720370\n",
            "Iteration 4199, loss = 0.20715464\n",
            "Iteration 4200, loss = 0.20710008\n",
            "Iteration 4201, loss = 0.20703727\n",
            "Iteration 4202, loss = 0.20697658\n",
            "Iteration 4203, loss = 0.20693651\n",
            "Iteration 4204, loss = 0.20691444\n",
            "Iteration 4205, loss = 0.20685311\n",
            "Iteration 4206, loss = 0.20676582\n",
            "Iteration 4207, loss = 0.20671738\n",
            "Iteration 4208, loss = 0.20663260\n",
            "Iteration 4209, loss = 0.20659288\n",
            "Iteration 4210, loss = 0.20656388\n",
            "Iteration 4211, loss = 0.20650327\n",
            "Iteration 4212, loss = 0.20641994\n",
            "Iteration 4213, loss = 0.20639450\n",
            "Iteration 4214, loss = 0.20631473\n",
            "Iteration 4215, loss = 0.20625293\n",
            "Iteration 4216, loss = 0.20619435\n",
            "Iteration 4217, loss = 0.20616692\n",
            "Iteration 4218, loss = 0.20608974\n",
            "Iteration 4219, loss = 0.20601954\n",
            "Iteration 4220, loss = 0.20594817\n",
            "Iteration 4221, loss = 0.20589575\n",
            "Iteration 4222, loss = 0.20585307\n",
            "Iteration 4223, loss = 0.20582102\n",
            "Iteration 4224, loss = 0.20576148\n",
            "Iteration 4225, loss = 0.20566555\n",
            "Iteration 4226, loss = 0.20563365\n",
            "Iteration 4227, loss = 0.20556770\n",
            "Iteration 4228, loss = 0.20556497\n",
            "Iteration 4229, loss = 0.20545001\n",
            "Iteration 4230, loss = 0.20541721\n",
            "Iteration 4231, loss = 0.20536764\n",
            "Iteration 4232, loss = 0.20530962\n",
            "Iteration 4233, loss = 0.20525294\n",
            "Iteration 4234, loss = 0.20517412\n",
            "Iteration 4235, loss = 0.20513192\n",
            "Iteration 4236, loss = 0.20505819\n",
            "Iteration 4237, loss = 0.20504291\n",
            "Iteration 4238, loss = 0.20497346\n",
            "Iteration 4239, loss = 0.20492411\n",
            "Iteration 4240, loss = 0.20490730\n",
            "Iteration 4241, loss = 0.20483795\n",
            "Iteration 4242, loss = 0.20476612\n",
            "Iteration 4243, loss = 0.20471195\n",
            "Iteration 4244, loss = 0.20466351\n",
            "Iteration 4245, loss = 0.20458964\n",
            "Iteration 4246, loss = 0.20455558\n",
            "Iteration 4247, loss = 0.20455482\n",
            "Iteration 4248, loss = 0.20447682\n",
            "Iteration 4249, loss = 0.20441961\n",
            "Iteration 4250, loss = 0.20434293\n",
            "Iteration 4251, loss = 0.20428763\n",
            "Iteration 4252, loss = 0.20424147\n",
            "Iteration 4253, loss = 0.20418483\n",
            "Iteration 4254, loss = 0.20415537\n",
            "Iteration 4255, loss = 0.20409911\n",
            "Iteration 4256, loss = 0.20401834\n",
            "Iteration 4257, loss = 0.20398374\n",
            "Iteration 4258, loss = 0.20389303\n",
            "Iteration 4259, loss = 0.20385470\n",
            "Iteration 4260, loss = 0.20377987\n",
            "Iteration 4261, loss = 0.20371772\n",
            "Iteration 4262, loss = 0.20363237\n",
            "Iteration 4263, loss = 0.20359419\n",
            "Iteration 4264, loss = 0.20353811\n",
            "Iteration 4265, loss = 0.20347764\n",
            "Iteration 4266, loss = 0.20341337\n",
            "Iteration 4267, loss = 0.20338282\n",
            "Iteration 4268, loss = 0.20331739\n",
            "Iteration 4269, loss = 0.20327536\n",
            "Iteration 4270, loss = 0.20324163\n",
            "Iteration 4271, loss = 0.20317329\n",
            "Iteration 4272, loss = 0.20309625\n",
            "Iteration 4273, loss = 0.20308084\n",
            "Iteration 4274, loss = 0.20298597\n",
            "Iteration 4275, loss = 0.20298425\n",
            "Iteration 4276, loss = 0.20287465\n",
            "Iteration 4277, loss = 0.20283189\n",
            "Iteration 4278, loss = 0.20277251\n",
            "Iteration 4279, loss = 0.20269963\n",
            "Iteration 4280, loss = 0.20267969\n",
            "Iteration 4281, loss = 0.20265584\n",
            "Iteration 4282, loss = 0.20265536\n",
            "Iteration 4283, loss = 0.20256940\n",
            "Iteration 4284, loss = 0.20254526\n",
            "Iteration 4285, loss = 0.20248290\n",
            "Iteration 4286, loss = 0.20240217\n",
            "Iteration 4287, loss = 0.20234064\n",
            "Iteration 4288, loss = 0.20228668\n",
            "Iteration 4289, loss = 0.20225619\n",
            "Iteration 4290, loss = 0.20216315\n",
            "Iteration 4291, loss = 0.20210006\n",
            "Iteration 4292, loss = 0.20204910\n",
            "Iteration 4293, loss = 0.20202788\n",
            "Iteration 4294, loss = 0.20193644\n",
            "Iteration 4295, loss = 0.20188653\n",
            "Iteration 4296, loss = 0.20182290\n",
            "Iteration 4297, loss = 0.20178125\n",
            "Iteration 4298, loss = 0.20172368\n",
            "Iteration 4299, loss = 0.20169413\n",
            "Iteration 4300, loss = 0.20165002\n",
            "Iteration 4301, loss = 0.20157504\n",
            "Iteration 4302, loss = 0.20159730\n",
            "Iteration 4303, loss = 0.20147418\n",
            "Iteration 4304, loss = 0.20143483\n",
            "Iteration 4305, loss = 0.20138145\n",
            "Iteration 4306, loss = 0.20130774\n",
            "Iteration 4307, loss = 0.20128404\n",
            "Iteration 4308, loss = 0.20123547\n",
            "Iteration 4309, loss = 0.20113572\n",
            "Iteration 4310, loss = 0.20111935\n",
            "Iteration 4311, loss = 0.20114508\n",
            "Iteration 4312, loss = 0.20110589\n",
            "Iteration 4313, loss = 0.20104810\n",
            "Iteration 4314, loss = 0.20095827\n",
            "Iteration 4315, loss = 0.20083105\n",
            "Iteration 4316, loss = 0.20076981\n",
            "Iteration 4317, loss = 0.20068568\n",
            "Iteration 4318, loss = 0.20069040\n",
            "Iteration 4319, loss = 0.20066305\n",
            "Iteration 4320, loss = 0.20061724\n",
            "Iteration 4321, loss = 0.20055679\n",
            "Iteration 4322, loss = 0.20052422\n",
            "Iteration 4323, loss = 0.20047325\n",
            "Iteration 4324, loss = 0.20041533\n",
            "Iteration 4325, loss = 0.20033101\n",
            "Iteration 4326, loss = 0.20025644\n",
            "Iteration 4327, loss = 0.20019635\n",
            "Iteration 4328, loss = 0.20016268\n",
            "Iteration 4329, loss = 0.20013789\n",
            "Iteration 4330, loss = 0.20005088\n",
            "Iteration 4331, loss = 0.20000944\n",
            "Iteration 4332, loss = 0.19997581\n",
            "Iteration 4333, loss = 0.19991791\n",
            "Iteration 4334, loss = 0.19987199\n",
            "Iteration 4335, loss = 0.19978957\n",
            "Iteration 4336, loss = 0.19973660\n",
            "Iteration 4337, loss = 0.19967985\n",
            "Iteration 4338, loss = 0.19962257\n",
            "Iteration 4339, loss = 0.19957377\n",
            "Iteration 4340, loss = 0.19957037\n",
            "Iteration 4341, loss = 0.19947548\n",
            "Iteration 4342, loss = 0.19943017\n",
            "Iteration 4343, loss = 0.19940265\n",
            "Iteration 4344, loss = 0.19933974\n",
            "Iteration 4345, loss = 0.19932321\n",
            "Iteration 4346, loss = 0.19927881\n",
            "Iteration 4347, loss = 0.19920715\n",
            "Iteration 4348, loss = 0.19914980\n",
            "Iteration 4349, loss = 0.19908922\n",
            "Iteration 4350, loss = 0.19907266\n",
            "Iteration 4351, loss = 0.19901836\n",
            "Iteration 4352, loss = 0.19899532\n",
            "Iteration 4353, loss = 0.19893747\n",
            "Iteration 4354, loss = 0.19889864\n",
            "Iteration 4355, loss = 0.19884962\n",
            "Iteration 4356, loss = 0.19878993\n",
            "Iteration 4357, loss = 0.19872384\n",
            "Iteration 4358, loss = 0.19868100\n",
            "Iteration 4359, loss = 0.19862271\n",
            "Iteration 4360, loss = 0.19858253\n",
            "Iteration 4361, loss = 0.19851578\n",
            "Iteration 4362, loss = 0.19845305\n",
            "Iteration 4363, loss = 0.19838976\n",
            "Iteration 4364, loss = 0.19832926\n",
            "Iteration 4365, loss = 0.19826024\n",
            "Iteration 4366, loss = 0.19821276\n",
            "Iteration 4367, loss = 0.19825391\n",
            "Iteration 4368, loss = 0.19820732\n",
            "Iteration 4369, loss = 0.19807941\n",
            "Iteration 4370, loss = 0.19804041\n",
            "Iteration 4371, loss = 0.19797097\n",
            "Iteration 4372, loss = 0.19792575\n",
            "Iteration 4373, loss = 0.19787863\n",
            "Iteration 4374, loss = 0.19780797\n",
            "Iteration 4375, loss = 0.19775749\n",
            "Iteration 4376, loss = 0.19774597\n",
            "Iteration 4377, loss = 0.19769028\n",
            "Iteration 4378, loss = 0.19761134\n",
            "Iteration 4379, loss = 0.19754335\n",
            "Iteration 4380, loss = 0.19752313\n",
            "Iteration 4381, loss = 0.19744219\n",
            "Iteration 4382, loss = 0.19740251\n",
            "Iteration 4383, loss = 0.19736057\n",
            "Iteration 4384, loss = 0.19728287\n",
            "Iteration 4385, loss = 0.19723339\n",
            "Iteration 4386, loss = 0.19717120\n",
            "Iteration 4387, loss = 0.19713305\n",
            "Iteration 4388, loss = 0.19707795\n",
            "Iteration 4389, loss = 0.19702431\n",
            "Iteration 4390, loss = 0.19696751\n",
            "Iteration 4391, loss = 0.19693011\n",
            "Iteration 4392, loss = 0.19688667\n",
            "Iteration 4393, loss = 0.19683417\n",
            "Iteration 4394, loss = 0.19675091\n",
            "Iteration 4395, loss = 0.19674254\n",
            "Iteration 4396, loss = 0.19666833\n",
            "Iteration 4397, loss = 0.19661982\n",
            "Iteration 4398, loss = 0.19658765\n",
            "Iteration 4399, loss = 0.19653859\n",
            "Iteration 4400, loss = 0.19651057\n",
            "Iteration 4401, loss = 0.19645660\n",
            "Iteration 4402, loss = 0.19639498\n",
            "Iteration 4403, loss = 0.19633941\n",
            "Iteration 4404, loss = 0.19630021\n",
            "Iteration 4405, loss = 0.19630918\n",
            "Iteration 4406, loss = 0.19621291\n",
            "Iteration 4407, loss = 0.19616395\n",
            "Iteration 4408, loss = 0.19610966\n",
            "Iteration 4409, loss = 0.19608728\n",
            "Iteration 4410, loss = 0.19600477\n",
            "Iteration 4411, loss = 0.19593322\n",
            "Iteration 4412, loss = 0.19589002\n",
            "Iteration 4413, loss = 0.19583939\n",
            "Iteration 4414, loss = 0.19579305\n",
            "Iteration 4415, loss = 0.19573519\n",
            "Iteration 4416, loss = 0.19568419\n",
            "Iteration 4417, loss = 0.19563446\n",
            "Iteration 4418, loss = 0.19558577\n",
            "Iteration 4419, loss = 0.19553316\n",
            "Iteration 4420, loss = 0.19550630\n",
            "Iteration 4421, loss = 0.19544115\n",
            "Iteration 4422, loss = 0.19535375\n",
            "Iteration 4423, loss = 0.19534408\n",
            "Iteration 4424, loss = 0.19533126\n",
            "Iteration 4425, loss = 0.19529388\n",
            "Iteration 4426, loss = 0.19525695\n",
            "Iteration 4427, loss = 0.19518065\n",
            "Iteration 4428, loss = 0.19510418\n",
            "Iteration 4429, loss = 0.19505012\n",
            "Iteration 4430, loss = 0.19501810\n",
            "Iteration 4431, loss = 0.19496311\n",
            "Iteration 4432, loss = 0.19496427\n",
            "Iteration 4433, loss = 0.19490202\n",
            "Iteration 4434, loss = 0.19480768\n",
            "Iteration 4435, loss = 0.19476350\n",
            "Iteration 4436, loss = 0.19472457\n",
            "Iteration 4437, loss = 0.19466096\n",
            "Iteration 4438, loss = 0.19459944\n",
            "Iteration 4439, loss = 0.19457819\n",
            "Iteration 4440, loss = 0.19449022\n",
            "Iteration 4441, loss = 0.19446461\n",
            "Iteration 4442, loss = 0.19439957\n",
            "Iteration 4443, loss = 0.19436617\n",
            "Iteration 4444, loss = 0.19433797\n",
            "Iteration 4445, loss = 0.19429331\n",
            "Iteration 4446, loss = 0.19423751\n",
            "Iteration 4447, loss = 0.19420952\n",
            "Iteration 4448, loss = 0.19414187\n",
            "Iteration 4449, loss = 0.19410392\n",
            "Iteration 4450, loss = 0.19403784\n",
            "Iteration 4451, loss = 0.19396672\n",
            "Iteration 4452, loss = 0.19393164\n",
            "Iteration 4453, loss = 0.19388077\n",
            "Iteration 4454, loss = 0.19382189\n",
            "Iteration 4455, loss = 0.19379614\n",
            "Iteration 4456, loss = 0.19377256\n",
            "Iteration 4457, loss = 0.19377103\n",
            "Iteration 4458, loss = 0.19364141\n",
            "Iteration 4459, loss = 0.19359546\n",
            "Iteration 4460, loss = 0.19354852\n",
            "Iteration 4461, loss = 0.19351615\n",
            "Iteration 4462, loss = 0.19346318\n",
            "Iteration 4463, loss = 0.19342279\n",
            "Iteration 4464, loss = 0.19338239\n",
            "Iteration 4465, loss = 0.19330565\n",
            "Iteration 4466, loss = 0.19325754\n",
            "Iteration 4467, loss = 0.19323577\n",
            "Iteration 4468, loss = 0.19316950\n",
            "Iteration 4469, loss = 0.19314485\n",
            "Iteration 4470, loss = 0.19310629\n",
            "Iteration 4471, loss = 0.19308777\n",
            "Iteration 4472, loss = 0.19299138\n",
            "Iteration 4473, loss = 0.19293025\n",
            "Iteration 4474, loss = 0.19291561\n",
            "Iteration 4475, loss = 0.19282385\n",
            "Iteration 4476, loss = 0.19278828\n",
            "Iteration 4477, loss = 0.19273432\n",
            "Iteration 4478, loss = 0.19269691\n",
            "Iteration 4479, loss = 0.19266728\n",
            "Iteration 4480, loss = 0.19262967\n",
            "Iteration 4481, loss = 0.19259055\n",
            "Iteration 4482, loss = 0.19253323\n",
            "Iteration 4483, loss = 0.19248622\n",
            "Iteration 4484, loss = 0.19244998\n",
            "Iteration 4485, loss = 0.19239537\n",
            "Iteration 4486, loss = 0.19235566\n",
            "Iteration 4487, loss = 0.19236699\n",
            "Iteration 4488, loss = 0.19228952\n",
            "Iteration 4489, loss = 0.19226144\n",
            "Iteration 4490, loss = 0.19216760\n",
            "Iteration 4491, loss = 0.19212072\n",
            "Iteration 4492, loss = 0.19209138\n",
            "Iteration 4493, loss = 0.19205155\n",
            "Iteration 4494, loss = 0.19197517\n",
            "Iteration 4495, loss = 0.19191068\n",
            "Iteration 4496, loss = 0.19188815\n",
            "Iteration 4497, loss = 0.19180662\n",
            "Iteration 4498, loss = 0.19177817\n",
            "Iteration 4499, loss = 0.19176152\n",
            "Iteration 4500, loss = 0.19170557\n",
            "Iteration 4501, loss = 0.19165852\n",
            "Iteration 4502, loss = 0.19161291\n",
            "Iteration 4503, loss = 0.19154624\n",
            "Iteration 4504, loss = 0.19153616\n",
            "Iteration 4505, loss = 0.19143291\n",
            "Iteration 4506, loss = 0.19140074\n",
            "Iteration 4507, loss = 0.19135469\n",
            "Iteration 4508, loss = 0.19128959\n",
            "Iteration 4509, loss = 0.19127239\n",
            "Iteration 4510, loss = 0.19121573\n",
            "Iteration 4511, loss = 0.19118422\n",
            "Iteration 4512, loss = 0.19110940\n",
            "Iteration 4513, loss = 0.19105461\n",
            "Iteration 4514, loss = 0.19101870\n",
            "Iteration 4515, loss = 0.19096985\n",
            "Iteration 4516, loss = 0.19095144\n",
            "Iteration 4517, loss = 0.19088496\n",
            "Iteration 4518, loss = 0.19080278\n",
            "Iteration 4519, loss = 0.19081273\n",
            "Iteration 4520, loss = 0.19076193\n",
            "Iteration 4521, loss = 0.19070638\n",
            "Iteration 4522, loss = 0.19061818\n",
            "Iteration 4523, loss = 0.19058171\n",
            "Iteration 4524, loss = 0.19056691\n",
            "Iteration 4525, loss = 0.19047128\n",
            "Iteration 4526, loss = 0.19043604\n",
            "Iteration 4527, loss = 0.19040355\n",
            "Iteration 4528, loss = 0.19036673\n",
            "Iteration 4529, loss = 0.19031150\n",
            "Iteration 4530, loss = 0.19027497\n",
            "Iteration 4531, loss = 0.19021532\n",
            "Iteration 4532, loss = 0.19016869\n",
            "Iteration 4533, loss = 0.19012350\n",
            "Iteration 4534, loss = 0.19006568\n",
            "Iteration 4535, loss = 0.19001750\n",
            "Iteration 4536, loss = 0.18996129\n",
            "Iteration 4537, loss = 0.18992046\n",
            "Iteration 4538, loss = 0.18987882\n",
            "Iteration 4539, loss = 0.18989416\n",
            "Iteration 4540, loss = 0.18980220\n",
            "Iteration 4541, loss = 0.18974621\n",
            "Iteration 4542, loss = 0.18967841\n",
            "Iteration 4543, loss = 0.18967045\n",
            "Iteration 4544, loss = 0.18961416\n",
            "Iteration 4545, loss = 0.18952860\n",
            "Iteration 4546, loss = 0.18955001\n",
            "Iteration 4547, loss = 0.18951431\n",
            "Iteration 4548, loss = 0.18944789\n",
            "Iteration 4549, loss = 0.18940516\n",
            "Iteration 4550, loss = 0.18938398\n",
            "Iteration 4551, loss = 0.18932966\n",
            "Iteration 4552, loss = 0.18928785\n",
            "Iteration 4553, loss = 0.18918041\n",
            "Iteration 4554, loss = 0.18925810\n",
            "Iteration 4555, loss = 0.18911543\n",
            "Iteration 4556, loss = 0.18907289\n",
            "Iteration 4557, loss = 0.18903420\n",
            "Iteration 4558, loss = 0.18896621\n",
            "Iteration 4559, loss = 0.18891880\n",
            "Iteration 4560, loss = 0.18887327\n",
            "Iteration 4561, loss = 0.18881668\n",
            "Iteration 4562, loss = 0.18879061\n",
            "Iteration 4563, loss = 0.18877506\n",
            "Iteration 4564, loss = 0.18872257\n",
            "Iteration 4565, loss = 0.18866133\n",
            "Iteration 4566, loss = 0.18860457\n",
            "Iteration 4567, loss = 0.18855853\n",
            "Iteration 4568, loss = 0.18861174\n",
            "Iteration 4569, loss = 0.18847931\n",
            "Iteration 4570, loss = 0.18847607\n",
            "Iteration 4571, loss = 0.18840428\n",
            "Iteration 4572, loss = 0.18833622\n",
            "Iteration 4573, loss = 0.18828177\n",
            "Iteration 4574, loss = 0.18823314\n",
            "Iteration 4575, loss = 0.18825537\n",
            "Iteration 4576, loss = 0.18821063\n",
            "Iteration 4577, loss = 0.18818042\n",
            "Iteration 4578, loss = 0.18813132\n",
            "Iteration 4579, loss = 0.18805753\n",
            "Iteration 4580, loss = 0.18801493\n",
            "Iteration 4581, loss = 0.18801320\n",
            "Iteration 4582, loss = 0.18790485\n",
            "Iteration 4583, loss = 0.18786067\n",
            "Iteration 4584, loss = 0.18782038\n",
            "Iteration 4585, loss = 0.18777743\n",
            "Iteration 4586, loss = 0.18774680\n",
            "Iteration 4587, loss = 0.18769008\n",
            "Iteration 4588, loss = 0.18765525\n",
            "Iteration 4589, loss = 0.18762167\n",
            "Iteration 4590, loss = 0.18754294\n",
            "Iteration 4591, loss = 0.18751489\n",
            "Iteration 4592, loss = 0.18754101\n",
            "Iteration 4593, loss = 0.18742320\n",
            "Iteration 4594, loss = 0.18739215\n",
            "Iteration 4595, loss = 0.18733310\n",
            "Iteration 4596, loss = 0.18727488\n",
            "Iteration 4597, loss = 0.18725231\n",
            "Iteration 4598, loss = 0.18719532\n",
            "Iteration 4599, loss = 0.18715192\n",
            "Iteration 4600, loss = 0.18718628\n",
            "Iteration 4601, loss = 0.18712771\n",
            "Iteration 4602, loss = 0.18705910\n",
            "Iteration 4603, loss = 0.18701963\n",
            "Iteration 4604, loss = 0.18701964\n",
            "Iteration 4605, loss = 0.18692996\n",
            "Iteration 4606, loss = 0.18687145\n",
            "Iteration 4607, loss = 0.18684028\n",
            "Iteration 4608, loss = 0.18680896\n",
            "Iteration 4609, loss = 0.18674084\n",
            "Iteration 4610, loss = 0.18667917\n",
            "Iteration 4611, loss = 0.18664128\n",
            "Iteration 4612, loss = 0.18657740\n",
            "Iteration 4613, loss = 0.18652778\n",
            "Iteration 4614, loss = 0.18647855\n",
            "Iteration 4615, loss = 0.18646488\n",
            "Iteration 4616, loss = 0.18639043\n",
            "Iteration 4617, loss = 0.18633772\n",
            "Iteration 4618, loss = 0.18630354\n",
            "Iteration 4619, loss = 0.18626856\n",
            "Iteration 4620, loss = 0.18622941\n",
            "Iteration 4621, loss = 0.18618909\n",
            "Iteration 4622, loss = 0.18613929\n",
            "Iteration 4623, loss = 0.18608608\n",
            "Iteration 4624, loss = 0.18602811\n",
            "Iteration 4625, loss = 0.18600739\n",
            "Iteration 4626, loss = 0.18592790\n",
            "Iteration 4627, loss = 0.18589916\n",
            "Iteration 4628, loss = 0.18585830\n",
            "Iteration 4629, loss = 0.18580101\n",
            "Iteration 4630, loss = 0.18577985\n",
            "Iteration 4631, loss = 0.18573533\n",
            "Iteration 4632, loss = 0.18567292\n",
            "Iteration 4633, loss = 0.18563746\n",
            "Iteration 4634, loss = 0.18558758\n",
            "Iteration 4635, loss = 0.18555914\n",
            "Iteration 4636, loss = 0.18549796\n",
            "Iteration 4637, loss = 0.18545123\n",
            "Iteration 4638, loss = 0.18540954\n",
            "Iteration 4639, loss = 0.18540807\n",
            "Iteration 4640, loss = 0.18534702\n",
            "Iteration 4641, loss = 0.18532137\n",
            "Iteration 4642, loss = 0.18524419\n",
            "Iteration 4643, loss = 0.18521679\n",
            "Iteration 4644, loss = 0.18515084\n",
            "Iteration 4645, loss = 0.18514344\n",
            "Iteration 4646, loss = 0.18510247\n",
            "Iteration 4647, loss = 0.18507846\n",
            "Iteration 4648, loss = 0.18499828\n",
            "Iteration 4649, loss = 0.18496310\n",
            "Iteration 4650, loss = 0.18490099\n",
            "Iteration 4651, loss = 0.18485571\n",
            "Iteration 4652, loss = 0.18480811\n",
            "Iteration 4653, loss = 0.18477615\n",
            "Iteration 4654, loss = 0.18481640\n",
            "Iteration 4655, loss = 0.18483829\n",
            "Iteration 4656, loss = 0.18479605\n",
            "Iteration 4657, loss = 0.18473438\n",
            "Iteration 4658, loss = 0.18469828\n",
            "Iteration 4659, loss = 0.18460028\n",
            "Iteration 4660, loss = 0.18460389\n",
            "Iteration 4661, loss = 0.18449789\n",
            "Iteration 4662, loss = 0.18443275\n",
            "Iteration 4663, loss = 0.18436907\n",
            "Iteration 4664, loss = 0.18433972\n",
            "Iteration 4665, loss = 0.18428146\n",
            "Iteration 4666, loss = 0.18426770\n",
            "Iteration 4667, loss = 0.18419927\n",
            "Iteration 4668, loss = 0.18415806\n",
            "Iteration 4669, loss = 0.18415446\n",
            "Iteration 4670, loss = 0.18408338\n",
            "Iteration 4671, loss = 0.18408937\n",
            "Iteration 4672, loss = 0.18405007\n",
            "Iteration 4673, loss = 0.18398343\n",
            "Iteration 4674, loss = 0.18392770\n",
            "Iteration 4675, loss = 0.18387734\n",
            "Iteration 4676, loss = 0.18383558\n",
            "Iteration 4677, loss = 0.18378989\n",
            "Iteration 4678, loss = 0.18372641\n",
            "Iteration 4679, loss = 0.18368677\n",
            "Iteration 4680, loss = 0.18363799\n",
            "Iteration 4681, loss = 0.18359521\n",
            "Iteration 4682, loss = 0.18355815\n",
            "Iteration 4683, loss = 0.18352262\n",
            "Iteration 4684, loss = 0.18347422\n",
            "Iteration 4685, loss = 0.18343298\n",
            "Iteration 4686, loss = 0.18341428\n",
            "Iteration 4687, loss = 0.18335950\n",
            "Iteration 4688, loss = 0.18329857\n",
            "Iteration 4689, loss = 0.18326392\n",
            "Iteration 4690, loss = 0.18324479\n",
            "Iteration 4691, loss = 0.18321536\n",
            "Iteration 4692, loss = 0.18313611\n",
            "Iteration 4693, loss = 0.18309585\n",
            "Iteration 4694, loss = 0.18308888\n",
            "Iteration 4695, loss = 0.18301943\n",
            "Iteration 4696, loss = 0.18295452\n",
            "Iteration 4697, loss = 0.18293981\n",
            "Iteration 4698, loss = 0.18286257\n",
            "Iteration 4699, loss = 0.18281886\n",
            "Iteration 4700, loss = 0.18275095\n",
            "Iteration 4701, loss = 0.18274314\n",
            "Iteration 4702, loss = 0.18270669\n",
            "Iteration 4703, loss = 0.18266843\n",
            "Iteration 4704, loss = 0.18262439\n",
            "Iteration 4705, loss = 0.18255944\n",
            "Iteration 4706, loss = 0.18250698\n",
            "Iteration 4707, loss = 0.18245757\n",
            "Iteration 4708, loss = 0.18243002\n",
            "Iteration 4709, loss = 0.18238780\n",
            "Iteration 4710, loss = 0.18242054\n",
            "Iteration 4711, loss = 0.18229418\n",
            "Iteration 4712, loss = 0.18224768\n",
            "Iteration 4713, loss = 0.18222019\n",
            "Iteration 4714, loss = 0.18216575\n",
            "Iteration 4715, loss = 0.18214066\n",
            "Iteration 4716, loss = 0.18210882\n",
            "Iteration 4717, loss = 0.18204722\n",
            "Iteration 4718, loss = 0.18202198\n",
            "Iteration 4719, loss = 0.18194727\n",
            "Iteration 4720, loss = 0.18191169\n",
            "Iteration 4721, loss = 0.18186700\n",
            "Iteration 4722, loss = 0.18186008\n",
            "Iteration 4723, loss = 0.18179796\n",
            "Iteration 4724, loss = 0.18176211\n",
            "Iteration 4725, loss = 0.18174833\n",
            "Iteration 4726, loss = 0.18168520\n",
            "Iteration 4727, loss = 0.18163466\n",
            "Iteration 4728, loss = 0.18161936\n",
            "Iteration 4729, loss = 0.18155920\n",
            "Iteration 4730, loss = 0.18152664\n",
            "Iteration 4731, loss = 0.18146599\n",
            "Iteration 4732, loss = 0.18149090\n",
            "Iteration 4733, loss = 0.18139683\n",
            "Iteration 4734, loss = 0.18134954\n",
            "Iteration 4735, loss = 0.18129561\n",
            "Iteration 4736, loss = 0.18124231\n",
            "Iteration 4737, loss = 0.18124184\n",
            "Iteration 4738, loss = 0.18116417\n",
            "Iteration 4739, loss = 0.18112300\n",
            "Iteration 4740, loss = 0.18109388\n",
            "Iteration 4741, loss = 0.18104047\n",
            "Iteration 4742, loss = 0.18101255\n",
            "Iteration 4743, loss = 0.18097786\n",
            "Iteration 4744, loss = 0.18092860\n",
            "Iteration 4745, loss = 0.18090043\n",
            "Iteration 4746, loss = 0.18087044\n",
            "Iteration 4747, loss = 0.18082140\n",
            "Iteration 4748, loss = 0.18075628\n",
            "Iteration 4749, loss = 0.18068848\n",
            "Iteration 4750, loss = 0.18064250\n",
            "Iteration 4751, loss = 0.18062542\n",
            "Iteration 4752, loss = 0.18056405\n",
            "Iteration 4753, loss = 0.18051238\n",
            "Iteration 4754, loss = 0.18049327\n",
            "Iteration 4755, loss = 0.18044641\n",
            "Iteration 4756, loss = 0.18038411\n",
            "Iteration 4757, loss = 0.18037367\n",
            "Iteration 4758, loss = 0.18033371\n",
            "Iteration 4759, loss = 0.18026850\n",
            "Iteration 4760, loss = 0.18022323\n",
            "Iteration 4761, loss = 0.18017985\n",
            "Iteration 4762, loss = 0.18015363\n",
            "Iteration 4763, loss = 0.18011248\n",
            "Iteration 4764, loss = 0.18006122\n",
            "Iteration 4765, loss = 0.17998259\n",
            "Iteration 4766, loss = 0.18005737\n",
            "Iteration 4767, loss = 0.17997477\n",
            "Iteration 4768, loss = 0.17997285\n",
            "Iteration 4769, loss = 0.17992840\n",
            "Iteration 4770, loss = 0.17988435\n",
            "Iteration 4771, loss = 0.17982339\n",
            "Iteration 4772, loss = 0.17975404\n",
            "Iteration 4773, loss = 0.17969190\n",
            "Iteration 4774, loss = 0.17968522\n",
            "Iteration 4775, loss = 0.17967380\n",
            "Iteration 4776, loss = 0.17960675\n",
            "Iteration 4777, loss = 0.17958185\n",
            "Iteration 4778, loss = 0.17957274\n",
            "Iteration 4779, loss = 0.17950515\n",
            "Iteration 4780, loss = 0.17946635\n",
            "Iteration 4781, loss = 0.17940442\n",
            "Iteration 4782, loss = 0.17934595\n",
            "Iteration 4783, loss = 0.17931959\n",
            "Iteration 4784, loss = 0.17928585\n",
            "Iteration 4785, loss = 0.17925799\n",
            "Iteration 4786, loss = 0.17918488\n",
            "Iteration 4787, loss = 0.17916476\n",
            "Iteration 4788, loss = 0.17910698\n",
            "Iteration 4789, loss = 0.17908311\n",
            "Iteration 4790, loss = 0.17902283\n",
            "Iteration 4791, loss = 0.17896820\n",
            "Iteration 4792, loss = 0.17894539\n",
            "Iteration 4793, loss = 0.17889851\n",
            "Iteration 4794, loss = 0.17885699\n",
            "Iteration 4795, loss = 0.17881432\n",
            "Iteration 4796, loss = 0.17877867\n",
            "Iteration 4797, loss = 0.17872946\n",
            "Iteration 4798, loss = 0.17869885\n",
            "Iteration 4799, loss = 0.17866015\n",
            "Iteration 4800, loss = 0.17861190\n",
            "Iteration 4801, loss = 0.17860091\n",
            "Iteration 4802, loss = 0.17855727\n",
            "Iteration 4803, loss = 0.17853288\n",
            "Iteration 4804, loss = 0.17848223\n",
            "Iteration 4805, loss = 0.17842162\n",
            "Iteration 4806, loss = 0.17838275\n",
            "Iteration 4807, loss = 0.17832774\n",
            "Iteration 4808, loss = 0.17828191\n",
            "Iteration 4809, loss = 0.17826777\n",
            "Iteration 4810, loss = 0.17819316\n",
            "Iteration 4811, loss = 0.17819519\n",
            "Iteration 4812, loss = 0.17813137\n",
            "Iteration 4813, loss = 0.17810310\n",
            "Iteration 4814, loss = 0.17804648\n",
            "Iteration 4815, loss = 0.17802436\n",
            "Iteration 4816, loss = 0.17797019\n",
            "Iteration 4817, loss = 0.17795222\n",
            "Iteration 4818, loss = 0.17790828\n",
            "Iteration 4819, loss = 0.17787611\n",
            "Iteration 4820, loss = 0.17785176\n",
            "Iteration 4821, loss = 0.17780354\n",
            "Iteration 4822, loss = 0.17774853\n",
            "Iteration 4823, loss = 0.17772924\n",
            "Iteration 4824, loss = 0.17768725\n",
            "Iteration 4825, loss = 0.17774782\n",
            "Iteration 4826, loss = 0.17759755\n",
            "Iteration 4827, loss = 0.17757590\n",
            "Iteration 4828, loss = 0.17759127\n",
            "Iteration 4829, loss = 0.17750696\n",
            "Iteration 4830, loss = 0.17746058\n",
            "Iteration 4831, loss = 0.17741422\n",
            "Iteration 4832, loss = 0.17737514\n",
            "Iteration 4833, loss = 0.17734666\n",
            "Iteration 4834, loss = 0.17730312\n",
            "Iteration 4835, loss = 0.17729280\n",
            "Iteration 4836, loss = 0.17726207\n",
            "Iteration 4837, loss = 0.17719484\n",
            "Iteration 4838, loss = 0.17717097\n",
            "Iteration 4839, loss = 0.17711023\n",
            "Iteration 4840, loss = 0.17708907\n",
            "Iteration 4841, loss = 0.17703001\n",
            "Iteration 4842, loss = 0.17701790\n",
            "Iteration 4843, loss = 0.17695880\n",
            "Iteration 4844, loss = 0.17692292\n",
            "Iteration 4845, loss = 0.17687440\n",
            "Iteration 4846, loss = 0.17682141\n",
            "Iteration 4847, loss = 0.17678728\n",
            "Iteration 4848, loss = 0.17681779\n",
            "Iteration 4849, loss = 0.17671490\n",
            "Iteration 4850, loss = 0.17668003\n",
            "Iteration 4851, loss = 0.17662718\n",
            "Iteration 4852, loss = 0.17657714\n",
            "Iteration 4853, loss = 0.17657278\n",
            "Iteration 4854, loss = 0.17651862\n",
            "Iteration 4855, loss = 0.17648864\n",
            "Iteration 4856, loss = 0.17649550\n",
            "Iteration 4857, loss = 0.17637981\n",
            "Iteration 4858, loss = 0.17638280\n",
            "Iteration 4859, loss = 0.17633298\n",
            "Iteration 4860, loss = 0.17629643\n",
            "Iteration 4861, loss = 0.17625074\n",
            "Iteration 4862, loss = 0.17622627\n",
            "Iteration 4863, loss = 0.17619066\n",
            "Iteration 4864, loss = 0.17615935\n",
            "Iteration 4865, loss = 0.17610027\n",
            "Iteration 4866, loss = 0.17603979\n",
            "Iteration 4867, loss = 0.17603543\n",
            "Iteration 4868, loss = 0.17597846\n",
            "Iteration 4869, loss = 0.17593273\n",
            "Iteration 4870, loss = 0.17588767\n",
            "Iteration 4871, loss = 0.17585127\n",
            "Iteration 4872, loss = 0.17584827\n",
            "Iteration 4873, loss = 0.17577895\n",
            "Iteration 4874, loss = 0.17577787\n",
            "Iteration 4875, loss = 0.17570508\n",
            "Iteration 4876, loss = 0.17571011\n",
            "Iteration 4877, loss = 0.17563419\n",
            "Iteration 4878, loss = 0.17562191\n",
            "Iteration 4879, loss = 0.17557601\n",
            "Iteration 4880, loss = 0.17555773\n",
            "Iteration 4881, loss = 0.17549241\n",
            "Iteration 4882, loss = 0.17545764\n",
            "Iteration 4883, loss = 0.17540763\n",
            "Iteration 4884, loss = 0.17540273\n",
            "Iteration 4885, loss = 0.17534266\n",
            "Iteration 4886, loss = 0.17529466\n",
            "Iteration 4887, loss = 0.17527754\n",
            "Iteration 4888, loss = 0.17521931\n",
            "Iteration 4889, loss = 0.17519148\n",
            "Iteration 4890, loss = 0.17512729\n",
            "Iteration 4891, loss = 0.17515174\n",
            "Iteration 4892, loss = 0.17510915\n",
            "Iteration 4893, loss = 0.17508317\n",
            "Iteration 4894, loss = 0.17501557\n",
            "Iteration 4895, loss = 0.17500814\n",
            "Iteration 4896, loss = 0.17493942\n",
            "Iteration 4897, loss = 0.17494199\n",
            "Iteration 4898, loss = 0.17483974\n",
            "Iteration 4899, loss = 0.17484118\n",
            "Iteration 4900, loss = 0.17482526\n",
            "Iteration 4901, loss = 0.17479237\n",
            "Iteration 4902, loss = 0.17474497\n",
            "Iteration 4903, loss = 0.17474895\n",
            "Iteration 4904, loss = 0.17462598\n",
            "Iteration 4905, loss = 0.17462122\n",
            "Iteration 4906, loss = 0.17458728\n",
            "Iteration 4907, loss = 0.17453592\n",
            "Iteration 4908, loss = 0.17449788\n",
            "Iteration 4909, loss = 0.17446868\n",
            "Iteration 4910, loss = 0.17442554\n",
            "Iteration 4911, loss = 0.17439202\n",
            "Iteration 4912, loss = 0.17437733\n",
            "Iteration 4913, loss = 0.17437456\n",
            "Iteration 4914, loss = 0.17428368\n",
            "Iteration 4915, loss = 0.17426696\n",
            "Iteration 4916, loss = 0.17423125\n",
            "Iteration 4917, loss = 0.17421180\n",
            "Iteration 4918, loss = 0.17422821\n",
            "Iteration 4919, loss = 0.17411385\n",
            "Iteration 4920, loss = 0.17407409\n",
            "Iteration 4921, loss = 0.17405458\n",
            "Iteration 4922, loss = 0.17399693\n",
            "Iteration 4923, loss = 0.17394038\n",
            "Iteration 4924, loss = 0.17390519\n",
            "Iteration 4925, loss = 0.17388388\n",
            "Iteration 4926, loss = 0.17384709\n",
            "Iteration 4927, loss = 0.17381232\n",
            "Iteration 4928, loss = 0.17376083\n",
            "Iteration 4929, loss = 0.17372339\n",
            "Iteration 4930, loss = 0.17371168\n",
            "Iteration 4931, loss = 0.17363274\n",
            "Iteration 4932, loss = 0.17360057\n",
            "Iteration 4933, loss = 0.17356706\n",
            "Iteration 4934, loss = 0.17352499\n",
            "Iteration 4935, loss = 0.17353086\n",
            "Iteration 4936, loss = 0.17347106\n",
            "Iteration 4937, loss = 0.17343922\n",
            "Iteration 4938, loss = 0.17338546\n",
            "Iteration 4939, loss = 0.17337819\n",
            "Iteration 4940, loss = 0.17334175\n",
            "Iteration 4941, loss = 0.17331570\n",
            "Iteration 4942, loss = 0.17325296\n",
            "Iteration 4943, loss = 0.17324351\n",
            "Iteration 4944, loss = 0.17315586\n",
            "Iteration 4945, loss = 0.17324097\n",
            "Iteration 4946, loss = 0.17312004\n",
            "Iteration 4947, loss = 0.17308806\n",
            "Iteration 4948, loss = 0.17307004\n",
            "Iteration 4949, loss = 0.17302920\n",
            "Iteration 4950, loss = 0.17298393\n",
            "Iteration 4951, loss = 0.17293788\n",
            "Iteration 4952, loss = 0.17288702\n",
            "Iteration 4953, loss = 0.17285209\n",
            "Iteration 4954, loss = 0.17283353\n",
            "Iteration 4955, loss = 0.17283501\n",
            "Iteration 4956, loss = 0.17275937\n",
            "Iteration 4957, loss = 0.17274434\n",
            "Iteration 4958, loss = 0.17268608\n",
            "Iteration 4959, loss = 0.17268805\n",
            "Iteration 4960, loss = 0.17260880\n",
            "Iteration 4961, loss = 0.17257842\n",
            "Iteration 4962, loss = 0.17260055\n",
            "Iteration 4963, loss = 0.17252103\n",
            "Iteration 4964, loss = 0.17255702\n",
            "Iteration 4965, loss = 0.17246055\n",
            "Iteration 4966, loss = 0.17239050\n",
            "Iteration 4967, loss = 0.17235469\n",
            "Iteration 4968, loss = 0.17236912\n",
            "Iteration 4969, loss = 0.17229484\n",
            "Iteration 4970, loss = 0.17226149\n",
            "Iteration 4971, loss = 0.17220317\n",
            "Iteration 4972, loss = 0.17222925\n",
            "Iteration 4973, loss = 0.17218570\n",
            "Iteration 4974, loss = 0.17214660\n",
            "Iteration 4975, loss = 0.17208769\n",
            "Iteration 4976, loss = 0.17206986\n",
            "Iteration 4977, loss = 0.17201464\n",
            "Iteration 4978, loss = 0.17197071\n",
            "Iteration 4979, loss = 0.17191156\n",
            "Iteration 4980, loss = 0.17199415\n",
            "Iteration 4981, loss = 0.17189014\n",
            "Iteration 4982, loss = 0.17186443\n",
            "Iteration 4983, loss = 0.17180446\n",
            "Iteration 4984, loss = 0.17179498\n",
            "Iteration 4985, loss = 0.17172333\n",
            "Iteration 4986, loss = 0.17171685\n",
            "Iteration 4987, loss = 0.17165904\n",
            "Iteration 4988, loss = 0.17161829\n",
            "Iteration 4989, loss = 0.17158771\n",
            "Iteration 4990, loss = 0.17154426\n",
            "Iteration 4991, loss = 0.17150045\n",
            "Iteration 4992, loss = 0.17147676\n",
            "Iteration 4993, loss = 0.17143263\n",
            "Iteration 4994, loss = 0.17140314\n",
            "Iteration 4995, loss = 0.17137567\n",
            "Iteration 4996, loss = 0.17136726\n",
            "Iteration 4997, loss = 0.17132665\n",
            "Iteration 4998, loss = 0.17130167\n",
            "Iteration 4999, loss = 0.17125389\n",
            "Iteration 5000, loss = 0.17122486\n",
            "Iteration 5001, loss = 0.17115509\n",
            "Iteration 5002, loss = 0.17112630\n",
            "Iteration 5003, loss = 0.17110508\n",
            "Iteration 5004, loss = 0.17118946\n",
            "Iteration 5005, loss = 0.17104126\n",
            "Iteration 5006, loss = 0.17100512\n",
            "Iteration 5007, loss = 0.17100974\n",
            "Iteration 5008, loss = 0.17095376\n",
            "Iteration 5009, loss = 0.17090503\n",
            "Iteration 5010, loss = 0.17087155\n",
            "Iteration 5011, loss = 0.17084791\n",
            "Iteration 5012, loss = 0.17078192\n",
            "Iteration 5013, loss = 0.17073083\n",
            "Iteration 5014, loss = 0.17071441\n",
            "Iteration 5015, loss = 0.17071063\n",
            "Iteration 5016, loss = 0.17065012\n",
            "Iteration 5017, loss = 0.17062793\n",
            "Iteration 5018, loss = 0.17061984\n",
            "Iteration 5019, loss = 0.17057218\n",
            "Iteration 5020, loss = 0.17054875\n",
            "Iteration 5021, loss = 0.17050465\n",
            "Iteration 5022, loss = 0.17048238\n",
            "Iteration 5023, loss = 0.17044688\n",
            "Iteration 5024, loss = 0.17042807\n",
            "Iteration 5025, loss = 0.17040455\n",
            "Iteration 5026, loss = 0.17032814\n",
            "Iteration 5027, loss = 0.17027592\n",
            "Iteration 5028, loss = 0.17024591\n",
            "Iteration 5029, loss = 0.17022816\n",
            "Iteration 5030, loss = 0.17016669\n",
            "Iteration 5031, loss = 0.17011794\n",
            "Iteration 5032, loss = 0.17011600\n",
            "Iteration 5033, loss = 0.17007510\n",
            "Iteration 5034, loss = 0.17003436\n",
            "Iteration 5035, loss = 0.17000153\n",
            "Iteration 5036, loss = 0.16994782\n",
            "Iteration 5037, loss = 0.16991479\n",
            "Iteration 5038, loss = 0.16986705\n",
            "Iteration 5039, loss = 0.16984500\n",
            "Iteration 5040, loss = 0.16981908\n",
            "Iteration 5041, loss = 0.16978146\n",
            "Iteration 5042, loss = 0.16973177\n",
            "Iteration 5043, loss = 0.16970575\n",
            "Iteration 5044, loss = 0.16967033\n",
            "Iteration 5045, loss = 0.16967073\n",
            "Iteration 5046, loss = 0.16960679\n",
            "Iteration 5047, loss = 0.16958256\n",
            "Iteration 5048, loss = 0.16951716\n",
            "Iteration 5049, loss = 0.16950318\n",
            "Iteration 5050, loss = 0.16951339\n",
            "Iteration 5051, loss = 0.16949304\n",
            "Iteration 5052, loss = 0.16944011\n",
            "Iteration 5053, loss = 0.16939008\n",
            "Iteration 5054, loss = 0.16935572\n",
            "Iteration 5055, loss = 0.16934708\n",
            "Iteration 5056, loss = 0.16929271\n",
            "Iteration 5057, loss = 0.16925032\n",
            "Iteration 5058, loss = 0.16920798\n",
            "Iteration 5059, loss = 0.16917874\n",
            "Iteration 5060, loss = 0.16916411\n",
            "Iteration 5061, loss = 0.16913821\n",
            "Iteration 5062, loss = 0.16909261\n",
            "Iteration 5063, loss = 0.16903069\n",
            "Iteration 5064, loss = 0.16895635\n",
            "Iteration 5065, loss = 0.16894483\n",
            "Iteration 5066, loss = 0.16896960\n",
            "Iteration 5067, loss = 0.16894564\n",
            "Iteration 5068, loss = 0.16899709\n",
            "Iteration 5069, loss = 0.16890404\n",
            "Iteration 5070, loss = 0.16886819\n",
            "Iteration 5071, loss = 0.16884840\n",
            "Iteration 5072, loss = 0.16879076\n",
            "Iteration 5073, loss = 0.16871307\n",
            "Iteration 5074, loss = 0.16867074\n",
            "Iteration 5075, loss = 0.16867173\n",
            "Iteration 5076, loss = 0.16861441\n",
            "Iteration 5077, loss = 0.16856730\n",
            "Iteration 5078, loss = 0.16853846\n",
            "Iteration 5079, loss = 0.16855010\n",
            "Iteration 5080, loss = 0.16849378\n",
            "Iteration 5081, loss = 0.16844182\n",
            "Iteration 5082, loss = 0.16841802\n",
            "Iteration 5083, loss = 0.16842259\n",
            "Iteration 5084, loss = 0.16836797\n",
            "Iteration 5085, loss = 0.16833765\n",
            "Iteration 5086, loss = 0.16830694\n",
            "Iteration 5087, loss = 0.16828750\n",
            "Iteration 5088, loss = 0.16826132\n",
            "Iteration 5089, loss = 0.16824737\n",
            "Iteration 5090, loss = 0.16820460\n",
            "Iteration 5091, loss = 0.16817291\n",
            "Iteration 5092, loss = 0.16812204\n",
            "Iteration 5093, loss = 0.16810116\n",
            "Iteration 5094, loss = 0.16808523\n",
            "Iteration 5095, loss = 0.16801664\n",
            "Iteration 5096, loss = 0.16801788\n",
            "Iteration 5097, loss = 0.16794857\n",
            "Iteration 5098, loss = 0.16792742\n",
            "Iteration 5099, loss = 0.16787692\n",
            "Iteration 5100, loss = 0.16783555\n",
            "Iteration 5101, loss = 0.16781164\n",
            "Iteration 5102, loss = 0.16777441\n",
            "Iteration 5103, loss = 0.16778434\n",
            "Iteration 5104, loss = 0.16775155\n",
            "Iteration 5105, loss = 0.16773921\n",
            "Iteration 5106, loss = 0.16769319\n",
            "Iteration 5107, loss = 0.16762622\n",
            "Iteration 5108, loss = 0.16762355\n",
            "Iteration 5109, loss = 0.16755257\n",
            "Iteration 5110, loss = 0.16751148\n",
            "Iteration 5111, loss = 0.16747128\n",
            "Iteration 5112, loss = 0.16745257\n",
            "Iteration 5113, loss = 0.16739514\n",
            "Iteration 5114, loss = 0.16735446\n",
            "Iteration 5115, loss = 0.16737911\n",
            "Iteration 5116, loss = 0.16730911\n",
            "Iteration 5117, loss = 0.16730459\n",
            "Iteration 5118, loss = 0.16727748\n",
            "Iteration 5119, loss = 0.16725130\n",
            "Iteration 5120, loss = 0.16719197\n",
            "Iteration 5121, loss = 0.16716369\n",
            "Iteration 5122, loss = 0.16714244\n",
            "Iteration 5123, loss = 0.16710096\n",
            "Iteration 5124, loss = 0.16704972\n",
            "Iteration 5125, loss = 0.16706942\n",
            "Iteration 5126, loss = 0.16698556\n",
            "Iteration 5127, loss = 0.16694802\n",
            "Iteration 5128, loss = 0.16694113\n",
            "Iteration 5129, loss = 0.16689697\n",
            "Iteration 5130, loss = 0.16684589\n",
            "Iteration 5131, loss = 0.16682681\n",
            "Iteration 5132, loss = 0.16680254\n",
            "Iteration 5133, loss = 0.16675555\n",
            "Iteration 5134, loss = 0.16672061\n",
            "Iteration 5135, loss = 0.16675827\n",
            "Iteration 5136, loss = 0.16664725\n",
            "Iteration 5137, loss = 0.16661259\n",
            "Iteration 5138, loss = 0.16660650\n",
            "Iteration 5139, loss = 0.16658870\n",
            "Iteration 5140, loss = 0.16653414\n",
            "Iteration 5141, loss = 0.16649311\n",
            "Iteration 5142, loss = 0.16646387\n",
            "Iteration 5143, loss = 0.16642524\n",
            "Iteration 5144, loss = 0.16640089\n",
            "Iteration 5145, loss = 0.16635724\n",
            "Iteration 5146, loss = 0.16634210\n",
            "Iteration 5147, loss = 0.16633921\n",
            "Iteration 5148, loss = 0.16626138\n",
            "Iteration 5149, loss = 0.16625426\n",
            "Iteration 5150, loss = 0.16620399\n",
            "Iteration 5151, loss = 0.16616539\n",
            "Iteration 5152, loss = 0.16612768\n",
            "Iteration 5153, loss = 0.16610998\n",
            "Iteration 5154, loss = 0.16608640\n",
            "Iteration 5155, loss = 0.16602104\n",
            "Iteration 5156, loss = 0.16599579\n",
            "Iteration 5157, loss = 0.16597635\n",
            "Iteration 5158, loss = 0.16593948\n",
            "Iteration 5159, loss = 0.16591379\n",
            "Iteration 5160, loss = 0.16589249\n",
            "Iteration 5161, loss = 0.16585345\n",
            "Iteration 5162, loss = 0.16582098\n",
            "Iteration 5163, loss = 0.16576779\n",
            "Iteration 5164, loss = 0.16577130\n",
            "Iteration 5165, loss = 0.16574481\n",
            "Iteration 5166, loss = 0.16570020\n",
            "Iteration 5167, loss = 0.16567792\n",
            "Iteration 5168, loss = 0.16567051\n",
            "Iteration 5169, loss = 0.16559622\n",
            "Iteration 5170, loss = 0.16555896\n",
            "Iteration 5171, loss = 0.16555557\n",
            "Iteration 5172, loss = 0.16550954\n",
            "Iteration 5173, loss = 0.16546890\n",
            "Iteration 5174, loss = 0.16543518\n",
            "Iteration 5175, loss = 0.16539416\n",
            "Iteration 5176, loss = 0.16538316\n",
            "Iteration 5177, loss = 0.16533382\n",
            "Iteration 5178, loss = 0.16535205\n",
            "Iteration 5179, loss = 0.16529678\n",
            "Iteration 5180, loss = 0.16529231\n",
            "Iteration 5181, loss = 0.16526038\n",
            "Iteration 5182, loss = 0.16518105\n",
            "Iteration 5183, loss = 0.16513835\n",
            "Iteration 5184, loss = 0.16514076\n",
            "Iteration 5185, loss = 0.16508791\n",
            "Iteration 5186, loss = 0.16509044\n",
            "Iteration 5187, loss = 0.16505057\n",
            "Iteration 5188, loss = 0.16499479\n",
            "Iteration 5189, loss = 0.16493009\n",
            "Iteration 5190, loss = 0.16491404\n",
            "Iteration 5191, loss = 0.16490119\n",
            "Iteration 5192, loss = 0.16487952\n",
            "Iteration 5193, loss = 0.16483577\n",
            "Iteration 5194, loss = 0.16480527\n",
            "Iteration 5195, loss = 0.16479163\n",
            "Iteration 5196, loss = 0.16474362\n",
            "Iteration 5197, loss = 0.16474602\n",
            "Iteration 5198, loss = 0.16467479\n",
            "Iteration 5199, loss = 0.16465637\n",
            "Iteration 5200, loss = 0.16461350\n",
            "Iteration 5201, loss = 0.16458571\n",
            "Iteration 5202, loss = 0.16453479\n",
            "Iteration 5203, loss = 0.16451161\n",
            "Iteration 5204, loss = 0.16449802\n",
            "Iteration 5205, loss = 0.16443410\n",
            "Iteration 5206, loss = 0.16440834\n",
            "Iteration 5207, loss = 0.16437743\n",
            "Iteration 5208, loss = 0.16434881\n",
            "Iteration 5209, loss = 0.16430351\n",
            "Iteration 5210, loss = 0.16428082\n",
            "Iteration 5211, loss = 0.16427367\n",
            "Iteration 5212, loss = 0.16419922\n",
            "Iteration 5213, loss = 0.16422051\n",
            "Iteration 5214, loss = 0.16415787\n",
            "Iteration 5215, loss = 0.16412635\n",
            "Iteration 5216, loss = 0.16410917\n",
            "Iteration 5217, loss = 0.16405270\n",
            "Iteration 5218, loss = 0.16403306\n",
            "Iteration 5219, loss = 0.16398071\n",
            "Iteration 5220, loss = 0.16396258\n",
            "Iteration 5221, loss = 0.16395146\n",
            "Iteration 5222, loss = 0.16389180\n",
            "Iteration 5223, loss = 0.16385512\n",
            "Iteration 5224, loss = 0.16382069\n",
            "Iteration 5225, loss = 0.16382928\n",
            "Iteration 5226, loss = 0.16381302\n",
            "Iteration 5227, loss = 0.16376935\n",
            "Iteration 5228, loss = 0.16377528\n",
            "Iteration 5229, loss = 0.16369313\n",
            "Iteration 5230, loss = 0.16373153\n",
            "Iteration 5231, loss = 0.16361680\n",
            "Iteration 5232, loss = 0.16359330\n",
            "Iteration 5233, loss = 0.16357669\n",
            "Iteration 5234, loss = 0.16359444\n",
            "Iteration 5235, loss = 0.16352330\n",
            "Iteration 5236, loss = 0.16348409\n",
            "Iteration 5237, loss = 0.16343237\n",
            "Iteration 5238, loss = 0.16342176\n",
            "Iteration 5239, loss = 0.16341600\n",
            "Iteration 5240, loss = 0.16338752\n",
            "Iteration 5241, loss = 0.16338819\n",
            "Iteration 5242, loss = 0.16330722\n",
            "Iteration 5243, loss = 0.16329629\n",
            "Iteration 5244, loss = 0.16327867\n",
            "Iteration 5245, loss = 0.16323281\n",
            "Iteration 5246, loss = 0.16322047\n",
            "Iteration 5247, loss = 0.16316593\n",
            "Iteration 5248, loss = 0.16313895\n",
            "Iteration 5249, loss = 0.16309293\n",
            "Iteration 5250, loss = 0.16308783\n",
            "Iteration 5251, loss = 0.16303286\n",
            "Iteration 5252, loss = 0.16301104\n",
            "Iteration 5253, loss = 0.16298873\n",
            "Iteration 5254, loss = 0.16293907\n",
            "Iteration 5255, loss = 0.16293893\n",
            "Iteration 5256, loss = 0.16288200\n",
            "Iteration 5257, loss = 0.16284380\n",
            "Iteration 5258, loss = 0.16281078\n",
            "Iteration 5259, loss = 0.16278394\n",
            "Iteration 5260, loss = 0.16277356\n",
            "Iteration 5261, loss = 0.16278755\n",
            "Iteration 5262, loss = 0.16271043\n",
            "Iteration 5263, loss = 0.16270322\n",
            "Iteration 5264, loss = 0.16263475\n",
            "Iteration 5265, loss = 0.16261893\n",
            "Iteration 5266, loss = 0.16257048\n",
            "Iteration 5267, loss = 0.16253489\n",
            "Iteration 5268, loss = 0.16252824\n",
            "Iteration 5269, loss = 0.16248328\n",
            "Iteration 5270, loss = 0.16245948\n",
            "Iteration 5271, loss = 0.16238847\n",
            "Iteration 5272, loss = 0.16237685\n",
            "Iteration 5273, loss = 0.16233928\n",
            "Iteration 5274, loss = 0.16242291\n",
            "Iteration 5275, loss = 0.16237613\n",
            "Iteration 5276, loss = 0.16238498\n",
            "Iteration 5277, loss = 0.16237658\n",
            "Iteration 5278, loss = 0.16230874\n",
            "Iteration 5279, loss = 0.16230067\n",
            "Iteration 5280, loss = 0.16227097\n",
            "Iteration 5281, loss = 0.16220134\n",
            "Iteration 5282, loss = 0.16214232\n",
            "Iteration 5283, loss = 0.16213444\n",
            "Iteration 5284, loss = 0.16209702\n",
            "Iteration 5285, loss = 0.16204876\n",
            "Iteration 5286, loss = 0.16198444\n",
            "Iteration 5287, loss = 0.16196424\n",
            "Iteration 5288, loss = 0.16191227\n",
            "Iteration 5289, loss = 0.16189251\n",
            "Iteration 5290, loss = 0.16190675\n",
            "Iteration 5291, loss = 0.16187497\n",
            "Iteration 5292, loss = 0.16186109\n",
            "Iteration 5293, loss = 0.16185517\n",
            "Iteration 5294, loss = 0.16183433\n",
            "Iteration 5295, loss = 0.16184204\n",
            "Iteration 5296, loss = 0.16176360\n",
            "Iteration 5297, loss = 0.16173060\n",
            "Iteration 5298, loss = 0.16169194\n",
            "Iteration 5299, loss = 0.16162632\n",
            "Iteration 5300, loss = 0.16162031\n",
            "Iteration 5301, loss = 0.16156183\n",
            "Iteration 5302, loss = 0.16153513\n",
            "Iteration 5303, loss = 0.16150457\n",
            "Iteration 5304, loss = 0.16146285\n",
            "Iteration 5305, loss = 0.16142980\n",
            "Iteration 5306, loss = 0.16139992\n",
            "Iteration 5307, loss = 0.16136101\n",
            "Iteration 5308, loss = 0.16134420\n",
            "Iteration 5309, loss = 0.16137910\n",
            "Iteration 5310, loss = 0.16129407\n",
            "Iteration 5311, loss = 0.16125933\n",
            "Iteration 5312, loss = 0.16123334\n",
            "Iteration 5313, loss = 0.16120611\n",
            "Iteration 5314, loss = 0.16116516\n",
            "Iteration 5315, loss = 0.16119807\n",
            "Iteration 5316, loss = 0.16107914\n",
            "Iteration 5317, loss = 0.16110934\n",
            "Iteration 5318, loss = 0.16106345\n",
            "Iteration 5319, loss = 0.16111991\n",
            "Iteration 5320, loss = 0.16101619\n",
            "Iteration 5321, loss = 0.16097523\n",
            "Iteration 5322, loss = 0.16096286\n",
            "Iteration 5323, loss = 0.16090238\n",
            "Iteration 5324, loss = 0.16091691\n",
            "Iteration 5325, loss = 0.16086127\n",
            "Iteration 5326, loss = 0.16084115\n",
            "Iteration 5327, loss = 0.16084309\n",
            "Iteration 5328, loss = 0.16079772\n",
            "Iteration 5329, loss = 0.16075648\n",
            "Iteration 5330, loss = 0.16077893\n",
            "Iteration 5331, loss = 0.16072459\n",
            "Iteration 5332, loss = 0.16068246\n",
            "Iteration 5333, loss = 0.16065229\n",
            "Iteration 5334, loss = 0.16061310\n",
            "Iteration 5335, loss = 0.16060771\n",
            "Iteration 5336, loss = 0.16056669\n",
            "Iteration 5337, loss = 0.16051160\n",
            "Iteration 5338, loss = 0.16047802\n",
            "Iteration 5339, loss = 0.16045154\n",
            "Iteration 5340, loss = 0.16044046\n",
            "Iteration 5341, loss = 0.16041065\n",
            "Iteration 5342, loss = 0.16042926\n",
            "Iteration 5343, loss = 0.16036772\n",
            "Iteration 5344, loss = 0.16030540\n",
            "Iteration 5345, loss = 0.16028640\n",
            "Iteration 5346, loss = 0.16029363\n",
            "Iteration 5347, loss = 0.16025365\n",
            "Iteration 5348, loss = 0.16026047\n",
            "Iteration 5349, loss = 0.16024563\n",
            "Iteration 5350, loss = 0.16018501\n",
            "Iteration 5351, loss = 0.16010759\n",
            "Iteration 5352, loss = 0.16009129\n",
            "Iteration 5353, loss = 0.16004307\n",
            "Iteration 5354, loss = 0.16003386\n",
            "Iteration 5355, loss = 0.16003777\n",
            "Iteration 5356, loss = 0.16000506\n",
            "Iteration 5357, loss = 0.15996796\n",
            "Iteration 5358, loss = 0.15994338\n",
            "Iteration 5359, loss = 0.15992317\n",
            "Iteration 5360, loss = 0.15987716\n",
            "Iteration 5361, loss = 0.15983981\n",
            "Iteration 5362, loss = 0.15977598\n",
            "Iteration 5363, loss = 0.15977158\n",
            "Iteration 5364, loss = 0.15972066\n",
            "Iteration 5365, loss = 0.15968355\n",
            "Iteration 5366, loss = 0.15973242\n",
            "Iteration 5367, loss = 0.15972867\n",
            "Iteration 5368, loss = 0.15974725\n",
            "Iteration 5369, loss = 0.15970070\n",
            "Iteration 5370, loss = 0.15964922\n",
            "Iteration 5371, loss = 0.15964929\n",
            "Iteration 5372, loss = 0.15957844\n",
            "Iteration 5373, loss = 0.15954076\n",
            "Iteration 5374, loss = 0.15952103\n",
            "Iteration 5375, loss = 0.15951966\n",
            "Iteration 5376, loss = 0.15944017\n",
            "Iteration 5377, loss = 0.15940673\n",
            "Iteration 5378, loss = 0.15940758\n",
            "Iteration 5379, loss = 0.15935851\n",
            "Iteration 5380, loss = 0.15931333\n",
            "Iteration 5381, loss = 0.15928473\n",
            "Iteration 5382, loss = 0.15925733\n",
            "Iteration 5383, loss = 0.15922850\n",
            "Iteration 5384, loss = 0.15920603\n",
            "Iteration 5385, loss = 0.15917533\n",
            "Iteration 5386, loss = 0.15918239\n",
            "Iteration 5387, loss = 0.15912982\n",
            "Iteration 5388, loss = 0.15908814\n",
            "Iteration 5389, loss = 0.15906328\n",
            "Iteration 5390, loss = 0.15906665\n",
            "Iteration 5391, loss = 0.15901251\n",
            "Iteration 5392, loss = 0.15897166\n",
            "Iteration 5393, loss = 0.15894015\n",
            "Iteration 5394, loss = 0.15894740\n",
            "Iteration 5395, loss = 0.15889515\n",
            "Iteration 5396, loss = 0.15892864\n",
            "Iteration 5397, loss = 0.15888866\n",
            "Iteration 5398, loss = 0.15880606\n",
            "Iteration 5399, loss = 0.15880568\n",
            "Iteration 5400, loss = 0.15876053\n",
            "Iteration 5401, loss = 0.15872097\n",
            "Iteration 5402, loss = 0.15869707\n",
            "Iteration 5403, loss = 0.15869974\n",
            "Iteration 5404, loss = 0.15867852\n",
            "Iteration 5405, loss = 0.15863899\n",
            "Iteration 5406, loss = 0.15861209\n",
            "Iteration 5407, loss = 0.15855989\n",
            "Iteration 5408, loss = 0.15855116\n",
            "Iteration 5409, loss = 0.15854617\n",
            "Iteration 5410, loss = 0.15852390\n",
            "Iteration 5411, loss = 0.15849466\n",
            "Iteration 5412, loss = 0.15846175\n",
            "Iteration 5413, loss = 0.15845280\n",
            "Iteration 5414, loss = 0.15842874\n",
            "Iteration 5415, loss = 0.15839673\n",
            "Iteration 5416, loss = 0.15835971\n",
            "Iteration 5417, loss = 0.15834175\n",
            "Iteration 5418, loss = 0.15830252\n",
            "Iteration 5419, loss = 0.15828350\n",
            "Iteration 5420, loss = 0.15828167\n",
            "Iteration 5421, loss = 0.15828335\n",
            "Iteration 5422, loss = 0.15826099\n",
            "Iteration 5423, loss = 0.15825583\n",
            "Iteration 5424, loss = 0.15821325\n",
            "Iteration 5425, loss = 0.15818903\n",
            "Iteration 5426, loss = 0.15814473\n",
            "Iteration 5427, loss = 0.15811633\n",
            "Iteration 5428, loss = 0.15816175\n",
            "Iteration 5429, loss = 0.15809375\n",
            "Iteration 5430, loss = 0.15810029\n",
            "Iteration 5431, loss = 0.15801534\n",
            "Iteration 5432, loss = 0.15800391\n",
            "Iteration 5433, loss = 0.15795155\n",
            "Iteration 5434, loss = 0.15790947\n",
            "Iteration 5435, loss = 0.15786979\n",
            "Iteration 5436, loss = 0.15785908\n",
            "Iteration 5437, loss = 0.15785675\n",
            "Iteration 5438, loss = 0.15780990\n",
            "Iteration 5439, loss = 0.15779096\n",
            "Iteration 5440, loss = 0.15775047\n",
            "Iteration 5441, loss = 0.15768935\n",
            "Iteration 5442, loss = 0.15770765\n",
            "Iteration 5443, loss = 0.15762984\n",
            "Iteration 5444, loss = 0.15771085\n",
            "Iteration 5445, loss = 0.15761737\n",
            "Iteration 5446, loss = 0.15757386\n",
            "Iteration 5447, loss = 0.15754795\n",
            "Iteration 5448, loss = 0.15753929\n",
            "Iteration 5449, loss = 0.15759347\n",
            "Iteration 5450, loss = 0.15752159\n",
            "Iteration 5451, loss = 0.15749992\n",
            "Iteration 5452, loss = 0.15746157\n",
            "Iteration 5453, loss = 0.15743351\n",
            "Iteration 5454, loss = 0.15738977\n",
            "Iteration 5455, loss = 0.15734841\n",
            "Iteration 5456, loss = 0.15740777\n",
            "Iteration 5457, loss = 0.15732467\n",
            "Iteration 5458, loss = 0.15728637\n",
            "Iteration 5459, loss = 0.15725394\n",
            "Iteration 5460, loss = 0.15720508\n",
            "Iteration 5461, loss = 0.15722147\n",
            "Iteration 5462, loss = 0.15714036\n",
            "Iteration 5463, loss = 0.15712545\n",
            "Iteration 5464, loss = 0.15710228\n",
            "Iteration 5465, loss = 0.15709138\n",
            "Iteration 5466, loss = 0.15705882\n",
            "Iteration 5467, loss = 0.15704314\n",
            "Iteration 5468, loss = 0.15703331\n",
            "Iteration 5469, loss = 0.15698426\n",
            "Iteration 5470, loss = 0.15696081\n",
            "Iteration 5471, loss = 0.15691708\n",
            "Iteration 5472, loss = 0.15693319\n",
            "Iteration 5473, loss = 0.15687714\n",
            "Iteration 5474, loss = 0.15685679\n",
            "Iteration 5475, loss = 0.15684903\n",
            "Iteration 5476, loss = 0.15680225\n",
            "Iteration 5477, loss = 0.15680330\n",
            "Iteration 5478, loss = 0.15678227\n",
            "Iteration 5479, loss = 0.15676079\n",
            "Iteration 5480, loss = 0.15674081\n",
            "Iteration 5481, loss = 0.15670200\n",
            "Iteration 5482, loss = 0.15667120\n",
            "Iteration 5483, loss = 0.15662279\n",
            "Iteration 5484, loss = 0.15660226\n",
            "Iteration 5485, loss = 0.15655548\n",
            "Iteration 5486, loss = 0.15652093\n",
            "Iteration 5487, loss = 0.15659937\n",
            "Iteration 5488, loss = 0.15652643\n",
            "Iteration 5489, loss = 0.15645786\n",
            "Iteration 5490, loss = 0.15644165\n",
            "Iteration 5491, loss = 0.15640256\n",
            "Iteration 5492, loss = 0.15637494\n",
            "Iteration 5493, loss = 0.15633106\n",
            "Iteration 5494, loss = 0.15628658\n",
            "Iteration 5495, loss = 0.15627430\n",
            "Iteration 5496, loss = 0.15633722\n",
            "Iteration 5497, loss = 0.15627744\n",
            "Iteration 5498, loss = 0.15626053\n",
            "Iteration 5499, loss = 0.15626613\n",
            "Iteration 5500, loss = 0.15616858\n",
            "Iteration 5501, loss = 0.15618041\n",
            "Iteration 5502, loss = 0.15609933\n",
            "Iteration 5503, loss = 0.15608416\n",
            "Iteration 5504, loss = 0.15604294\n",
            "Iteration 5505, loss = 0.15602117\n",
            "Iteration 5506, loss = 0.15601726\n",
            "Iteration 5507, loss = 0.15600728\n",
            "Iteration 5508, loss = 0.15593411\n",
            "Iteration 5509, loss = 0.15589983\n",
            "Iteration 5510, loss = 0.15587820\n",
            "Iteration 5511, loss = 0.15587043\n",
            "Iteration 5512, loss = 0.15586872\n",
            "Iteration 5513, loss = 0.15581383\n",
            "Iteration 5514, loss = 0.15578413\n",
            "Iteration 5515, loss = 0.15579030\n",
            "Iteration 5516, loss = 0.15578171\n",
            "Iteration 5517, loss = 0.15576529\n",
            "Iteration 5518, loss = 0.15570160\n",
            "Iteration 5519, loss = 0.15568280\n",
            "Iteration 5520, loss = 0.15564859\n",
            "Iteration 5521, loss = 0.15560517\n",
            "Iteration 5522, loss = 0.15558773\n",
            "Iteration 5523, loss = 0.15556037\n",
            "Iteration 5524, loss = 0.15556000\n",
            "Iteration 5525, loss = 0.15551253\n",
            "Iteration 5526, loss = 0.15548771\n",
            "Iteration 5527, loss = 0.15545395\n",
            "Iteration 5528, loss = 0.15542314\n",
            "Iteration 5529, loss = 0.15539119\n",
            "Iteration 5530, loss = 0.15539403\n",
            "Iteration 5531, loss = 0.15532502\n",
            "Iteration 5532, loss = 0.15534944\n",
            "Iteration 5533, loss = 0.15533751\n",
            "Iteration 5534, loss = 0.15530745\n",
            "Iteration 5535, loss = 0.15526304\n",
            "Iteration 5536, loss = 0.15520111\n",
            "Iteration 5537, loss = 0.15519808\n",
            "Iteration 5538, loss = 0.15514711\n",
            "Iteration 5539, loss = 0.15521023\n",
            "Iteration 5540, loss = 0.15511996\n",
            "Iteration 5541, loss = 0.15511036\n",
            "Iteration 5542, loss = 0.15509152\n",
            "Iteration 5543, loss = 0.15507571\n",
            "Iteration 5544, loss = 0.15502126\n",
            "Iteration 5545, loss = 0.15497283\n",
            "Iteration 5546, loss = 0.15495950\n",
            "Iteration 5547, loss = 0.15493546\n",
            "Iteration 5548, loss = 0.15494120\n",
            "Iteration 5549, loss = 0.15491691\n",
            "Iteration 5550, loss = 0.15489640\n",
            "Iteration 5551, loss = 0.15490527\n",
            "Iteration 5552, loss = 0.15485795\n",
            "Iteration 5553, loss = 0.15485843\n",
            "Iteration 5554, loss = 0.15482500\n",
            "Iteration 5555, loss = 0.15477357\n",
            "Iteration 5556, loss = 0.15477285\n",
            "Iteration 5557, loss = 0.15472151\n",
            "Iteration 5558, loss = 0.15472122\n",
            "Iteration 5559, loss = 0.15468877\n",
            "Iteration 5560, loss = 0.15465472\n",
            "Iteration 5561, loss = 0.15463575\n",
            "Iteration 5562, loss = 0.15459765\n",
            "Iteration 5563, loss = 0.15457927\n",
            "Iteration 5564, loss = 0.15454937\n",
            "Iteration 5565, loss = 0.15450135\n",
            "Iteration 5566, loss = 0.15447630\n",
            "Iteration 5567, loss = 0.15448157\n",
            "Iteration 5568, loss = 0.15446442\n",
            "Iteration 5569, loss = 0.15443738\n",
            "Iteration 5570, loss = 0.15441003\n",
            "Iteration 5571, loss = 0.15436644\n",
            "Iteration 5572, loss = 0.15433954\n",
            "Iteration 5573, loss = 0.15432553\n",
            "Iteration 5574, loss = 0.15430652\n",
            "Iteration 5575, loss = 0.15427070\n",
            "Iteration 5576, loss = 0.15428320\n",
            "Iteration 5577, loss = 0.15423253\n",
            "Iteration 5578, loss = 0.15417702\n",
            "Iteration 5579, loss = 0.15416600\n",
            "Iteration 5580, loss = 0.15415435\n",
            "Iteration 5581, loss = 0.15409211\n",
            "Iteration 5582, loss = 0.15408911\n",
            "Iteration 5583, loss = 0.15411633\n",
            "Iteration 5584, loss = 0.15406608\n",
            "Iteration 5585, loss = 0.15404920\n",
            "Iteration 5586, loss = 0.15402637\n",
            "Iteration 5587, loss = 0.15398009\n",
            "Iteration 5588, loss = 0.15393222\n",
            "Iteration 5589, loss = 0.15393828\n",
            "Iteration 5590, loss = 0.15393433\n",
            "Iteration 5591, loss = 0.15386842\n",
            "Iteration 5592, loss = 0.15382040\n",
            "Iteration 5593, loss = 0.15380119\n",
            "Iteration 5594, loss = 0.15377305\n",
            "Iteration 5595, loss = 0.15378153\n",
            "Iteration 5596, loss = 0.15371382\n",
            "Iteration 5597, loss = 0.15369805\n",
            "Iteration 5598, loss = 0.15367261\n",
            "Iteration 5599, loss = 0.15364326\n",
            "Iteration 5600, loss = 0.15360751\n",
            "Iteration 5601, loss = 0.15361061\n",
            "Iteration 5602, loss = 0.15358325\n",
            "Iteration 5603, loss = 0.15356069\n",
            "Iteration 5604, loss = 0.15354875\n",
            "Iteration 5605, loss = 0.15353094\n",
            "Iteration 5606, loss = 0.15353134\n",
            "Iteration 5607, loss = 0.15345908\n",
            "Iteration 5608, loss = 0.15344389\n",
            "Iteration 5609, loss = 0.15340726\n",
            "Iteration 5610, loss = 0.15337942\n",
            "Iteration 5611, loss = 0.15340348\n",
            "Iteration 5612, loss = 0.15335434\n",
            "Iteration 5613, loss = 0.15335501\n",
            "Iteration 5614, loss = 0.15335499\n",
            "Iteration 5615, loss = 0.15333953\n",
            "Iteration 5616, loss = 0.15331132\n",
            "Iteration 5617, loss = 0.15329557\n",
            "Iteration 5618, loss = 0.15323106\n",
            "Iteration 5619, loss = 0.15317918\n",
            "Iteration 5620, loss = 0.15317600\n",
            "Iteration 5621, loss = 0.15311334\n",
            "Iteration 5622, loss = 0.15312483\n",
            "Iteration 5623, loss = 0.15309276\n",
            "Iteration 5624, loss = 0.15304082\n",
            "Iteration 5625, loss = 0.15306519\n",
            "Iteration 5626, loss = 0.15304772\n",
            "Iteration 5627, loss = 0.15297489\n",
            "Iteration 5628, loss = 0.15298042\n",
            "Iteration 5629, loss = 0.15295593\n",
            "Iteration 5630, loss = 0.15298497\n",
            "Iteration 5631, loss = 0.15288779\n",
            "Iteration 5632, loss = 0.15286189\n",
            "Iteration 5633, loss = 0.15281938\n",
            "Iteration 5634, loss = 0.15280824\n",
            "Iteration 5635, loss = 0.15277686\n",
            "Iteration 5636, loss = 0.15272814\n",
            "Iteration 5637, loss = 0.15273087\n",
            "Iteration 5638, loss = 0.15276672\n",
            "Iteration 5639, loss = 0.15272590\n",
            "Iteration 5640, loss = 0.15270343\n",
            "Iteration 5641, loss = 0.15271560\n",
            "Iteration 5642, loss = 0.15273019\n",
            "Iteration 5643, loss = 0.15270190\n",
            "Iteration 5644, loss = 0.15264799\n",
            "Iteration 5645, loss = 0.15263311\n",
            "Iteration 5646, loss = 0.15262163\n",
            "Iteration 5647, loss = 0.15253912\n",
            "Iteration 5648, loss = 0.15252664\n",
            "Iteration 5649, loss = 0.15250868\n",
            "Iteration 5650, loss = 0.15244588\n",
            "Iteration 5651, loss = 0.15240087\n",
            "Iteration 5652, loss = 0.15237904\n",
            "Iteration 5653, loss = 0.15233954\n",
            "Iteration 5654, loss = 0.15249182\n",
            "Iteration 5655, loss = 0.15237714\n",
            "Iteration 5656, loss = 0.15232402\n",
            "Iteration 5657, loss = 0.15228255\n",
            "Iteration 5658, loss = 0.15226556\n",
            "Iteration 5659, loss = 0.15219947\n",
            "Iteration 5660, loss = 0.15216427\n",
            "Iteration 5661, loss = 0.15218248\n",
            "Iteration 5662, loss = 0.15213231\n",
            "Iteration 5663, loss = 0.15220635\n",
            "Iteration 5664, loss = 0.15214731\n",
            "Iteration 5665, loss = 0.15211017\n",
            "Iteration 5666, loss = 0.15212556\n",
            "Iteration 5667, loss = 0.15209140\n",
            "Iteration 5668, loss = 0.15204169\n",
            "Iteration 5669, loss = 0.15205098\n",
            "Iteration 5670, loss = 0.15197008\n",
            "Iteration 5671, loss = 0.15196698\n",
            "Iteration 5672, loss = 0.15192423\n",
            "Iteration 5673, loss = 0.15195038\n",
            "Iteration 5674, loss = 0.15189615\n",
            "Iteration 5675, loss = 0.15184719\n",
            "Iteration 5676, loss = 0.15187209\n",
            "Iteration 5677, loss = 0.15184222\n",
            "Iteration 5678, loss = 0.15182175\n",
            "Iteration 5679, loss = 0.15177446\n",
            "Iteration 5680, loss = 0.15175579\n",
            "Iteration 5681, loss = 0.15180696\n",
            "Iteration 5682, loss = 0.15175407\n",
            "Iteration 5683, loss = 0.15181540\n",
            "Iteration 5684, loss = 0.15173294\n",
            "Iteration 5685, loss = 0.15170812\n",
            "Iteration 5686, loss = 0.15169064\n",
            "Iteration 5687, loss = 0.15168260\n",
            "Iteration 5688, loss = 0.15165157\n",
            "Iteration 5689, loss = 0.15164559\n",
            "Iteration 5690, loss = 0.15159087\n",
            "Iteration 5691, loss = 0.15153940\n",
            "Iteration 5692, loss = 0.15149958\n",
            "Iteration 5693, loss = 0.15147093\n",
            "Iteration 5694, loss = 0.15148720\n",
            "Iteration 5695, loss = 0.15139797\n",
            "Iteration 5696, loss = 0.15139467\n",
            "Iteration 5697, loss = 0.15136369\n",
            "Iteration 5698, loss = 0.15135430\n",
            "Iteration 5699, loss = 0.15133271\n",
            "Iteration 5700, loss = 0.15132467\n",
            "Iteration 5701, loss = 0.15125999\n",
            "Iteration 5702, loss = 0.15127615\n",
            "Iteration 5703, loss = 0.15121536\n",
            "Iteration 5704, loss = 0.15119416\n",
            "Iteration 5705, loss = 0.15117028\n",
            "Iteration 5706, loss = 0.15119459\n",
            "Iteration 5707, loss = 0.15114528\n",
            "Iteration 5708, loss = 0.15110590\n",
            "Iteration 5709, loss = 0.15107696\n",
            "Iteration 5710, loss = 0.15104340\n",
            "Iteration 5711, loss = 0.15109976\n",
            "Iteration 5712, loss = 0.15103661\n",
            "Iteration 5713, loss = 0.15099599\n",
            "Iteration 5714, loss = 0.15097522\n",
            "Iteration 5715, loss = 0.15092091\n",
            "Iteration 5716, loss = 0.15094773\n",
            "Iteration 5717, loss = 0.15091046\n",
            "Iteration 5718, loss = 0.15092640\n",
            "Iteration 5719, loss = 0.15089205\n",
            "Iteration 5720, loss = 0.15082749\n",
            "Iteration 5721, loss = 0.15082782\n",
            "Iteration 5722, loss = 0.15078589\n",
            "Iteration 5723, loss = 0.15079068\n",
            "Iteration 5724, loss = 0.15074223\n",
            "Iteration 5725, loss = 0.15073378\n",
            "Iteration 5726, loss = 0.15069130\n",
            "Iteration 5727, loss = 0.15067934\n",
            "Iteration 5728, loss = 0.15066189\n",
            "Iteration 5729, loss = 0.15062208\n",
            "Iteration 5730, loss = 0.15061419\n",
            "Iteration 5731, loss = 0.15055451\n",
            "Iteration 5732, loss = 0.15060423\n",
            "Iteration 5733, loss = 0.15058337\n",
            "Iteration 5734, loss = 0.15055087\n",
            "Iteration 5735, loss = 0.15052923\n",
            "Iteration 5736, loss = 0.15051406\n",
            "Iteration 5737, loss = 0.15050697\n",
            "Iteration 5738, loss = 0.15044190\n",
            "Iteration 5739, loss = 0.15047018\n",
            "Iteration 5740, loss = 0.15038139\n",
            "Iteration 5741, loss = 0.15037538\n",
            "Iteration 5742, loss = 0.15035893\n",
            "Iteration 5743, loss = 0.15031464\n",
            "Iteration 5744, loss = 0.15034747\n",
            "Iteration 5745, loss = 0.15029329\n",
            "Iteration 5746, loss = 0.15026824\n",
            "Iteration 5747, loss = 0.15025482\n",
            "Iteration 5748, loss = 0.15021631\n",
            "Iteration 5749, loss = 0.15021374\n",
            "Iteration 5750, loss = 0.15019132\n",
            "Iteration 5751, loss = 0.15018461\n",
            "Iteration 5752, loss = 0.15012815\n",
            "Iteration 5753, loss = 0.15009725\n",
            "Iteration 5754, loss = 0.15007886\n",
            "Iteration 5755, loss = 0.15004930\n",
            "Iteration 5756, loss = 0.14999983\n",
            "Iteration 5757, loss = 0.14998759\n",
            "Iteration 5758, loss = 0.14995220\n",
            "Iteration 5759, loss = 0.14992611\n",
            "Iteration 5760, loss = 0.14993369\n",
            "Iteration 5761, loss = 0.15001922\n",
            "Iteration 5762, loss = 0.14995673\n",
            "Iteration 5763, loss = 0.14993159\n",
            "Iteration 5764, loss = 0.14999125\n",
            "Iteration 5765, loss = 0.14990279\n",
            "Iteration 5766, loss = 0.14987750\n",
            "Iteration 5767, loss = 0.14986322\n",
            "Iteration 5768, loss = 0.14982461\n",
            "Iteration 5769, loss = 0.14977230\n",
            "Iteration 5770, loss = 0.14975586\n",
            "Iteration 5771, loss = 0.14974110\n",
            "Iteration 5772, loss = 0.14971727\n",
            "Iteration 5773, loss = 0.14971362\n",
            "Iteration 5774, loss = 0.14966931\n",
            "Iteration 5775, loss = 0.14963325\n",
            "Iteration 5776, loss = 0.14960141\n",
            "Iteration 5777, loss = 0.14956697\n",
            "Iteration 5778, loss = 0.14955445\n",
            "Iteration 5779, loss = 0.14954919\n",
            "Iteration 5780, loss = 0.14956579\n",
            "Iteration 5781, loss = 0.14951964\n",
            "Iteration 5782, loss = 0.14950071\n",
            "Iteration 5783, loss = 0.14950564\n",
            "Iteration 5784, loss = 0.14946157\n",
            "Iteration 5785, loss = 0.14942260\n",
            "Iteration 5786, loss = 0.14938946\n",
            "Iteration 5787, loss = 0.14936148\n",
            "Iteration 5788, loss = 0.14933678\n",
            "Iteration 5789, loss = 0.14932974\n",
            "Iteration 5790, loss = 0.14929462\n",
            "Iteration 5791, loss = 0.14926082\n",
            "Iteration 5792, loss = 0.14924723\n",
            "Iteration 5793, loss = 0.14920922\n",
            "Iteration 5794, loss = 0.14919656\n",
            "Iteration 5795, loss = 0.14918773\n",
            "Iteration 5796, loss = 0.14916665\n",
            "Iteration 5797, loss = 0.14914659\n",
            "Iteration 5798, loss = 0.14912023\n",
            "Iteration 5799, loss = 0.14914170\n",
            "Iteration 5800, loss = 0.14908385\n",
            "Iteration 5801, loss = 0.14910896\n",
            "Iteration 5802, loss = 0.14903559\n",
            "Iteration 5803, loss = 0.14902916\n",
            "Iteration 5804, loss = 0.14902174\n",
            "Iteration 5805, loss = 0.14897706\n",
            "Iteration 5806, loss = 0.14896442\n",
            "Iteration 5807, loss = 0.14894100\n",
            "Iteration 5808, loss = 0.14893949\n",
            "Iteration 5809, loss = 0.14890521\n",
            "Iteration 5810, loss = 0.14890143\n",
            "Iteration 5811, loss = 0.14886370\n",
            "Iteration 5812, loss = 0.14883922\n",
            "Iteration 5813, loss = 0.14885193\n",
            "Iteration 5814, loss = 0.14882499\n",
            "Iteration 5815, loss = 0.14877471\n",
            "Iteration 5816, loss = 0.14876497\n",
            "Iteration 5817, loss = 0.14873921\n",
            "Iteration 5818, loss = 0.14872751\n",
            "Iteration 5819, loss = 0.14869625\n",
            "Iteration 5820, loss = 0.14867673\n",
            "Iteration 5821, loss = 0.14870142\n",
            "Iteration 5822, loss = 0.14862629\n",
            "Iteration 5823, loss = 0.14860441\n",
            "Iteration 5824, loss = 0.14856870\n",
            "Iteration 5825, loss = 0.14865652\n",
            "Iteration 5826, loss = 0.14852142\n",
            "Iteration 5827, loss = 0.14852911\n",
            "Iteration 5828, loss = 0.14848242\n",
            "Iteration 5829, loss = 0.14851671\n",
            "Iteration 5830, loss = 0.14845747\n",
            "Iteration 5831, loss = 0.14844199\n",
            "Iteration 5832, loss = 0.14841706\n",
            "Iteration 5833, loss = 0.14838560\n",
            "Iteration 5834, loss = 0.14836991\n",
            "Iteration 5835, loss = 0.14835100\n",
            "Iteration 5836, loss = 0.14834251\n",
            "Iteration 5837, loss = 0.14834384\n",
            "Iteration 5838, loss = 0.14827830\n",
            "Iteration 5839, loss = 0.14828640\n",
            "Iteration 5840, loss = 0.14826219\n",
            "Iteration 5841, loss = 0.14825139\n",
            "Iteration 5842, loss = 0.14827226\n",
            "Iteration 5843, loss = 0.14820460\n",
            "Iteration 5844, loss = 0.14818862\n",
            "Iteration 5845, loss = 0.14816436\n",
            "Iteration 5846, loss = 0.14811476\n",
            "Iteration 5847, loss = 0.14807520\n",
            "Iteration 5848, loss = 0.14808417\n",
            "Iteration 5849, loss = 0.14809313\n",
            "Iteration 5850, loss = 0.14808460\n",
            "Iteration 5851, loss = 0.14807787\n",
            "Iteration 5852, loss = 0.14806129\n",
            "Iteration 5853, loss = 0.14800386\n",
            "Iteration 5854, loss = 0.14799612\n",
            "Iteration 5855, loss = 0.14796221\n",
            "Iteration 5856, loss = 0.14794678\n",
            "Iteration 5857, loss = 0.14795507\n",
            "Iteration 5858, loss = 0.14790116\n",
            "Iteration 5859, loss = 0.14787271\n",
            "Iteration 5860, loss = 0.14789329\n",
            "Iteration 5861, loss = 0.14785220\n",
            "Iteration 5862, loss = 0.14782505\n",
            "Iteration 5863, loss = 0.14780630\n",
            "Iteration 5864, loss = 0.14780901\n",
            "Iteration 5865, loss = 0.14776373\n",
            "Iteration 5866, loss = 0.14775030\n",
            "Iteration 5867, loss = 0.14772380\n",
            "Iteration 5868, loss = 0.14768371\n",
            "Iteration 5869, loss = 0.14766378\n",
            "Iteration 5870, loss = 0.14765822\n",
            "Iteration 5871, loss = 0.14762387\n",
            "Iteration 5872, loss = 0.14758524\n",
            "Iteration 5873, loss = 0.14757160\n",
            "Iteration 5874, loss = 0.14754552\n",
            "Iteration 5875, loss = 0.14751922\n",
            "Iteration 5876, loss = 0.14749412\n",
            "Iteration 5877, loss = 0.14756786\n",
            "Iteration 5878, loss = 0.14748612\n",
            "Iteration 5879, loss = 0.14746058\n",
            "Iteration 5880, loss = 0.14745733\n",
            "Iteration 5881, loss = 0.14744469\n",
            "Iteration 5882, loss = 0.14740498\n",
            "Iteration 5883, loss = 0.14741369\n",
            "Iteration 5884, loss = 0.14737679\n",
            "Iteration 5885, loss = 0.14739797\n",
            "Iteration 5886, loss = 0.14733868\n",
            "Iteration 5887, loss = 0.14728360\n",
            "Iteration 5888, loss = 0.14725779\n",
            "Iteration 5889, loss = 0.14723774\n",
            "Iteration 5890, loss = 0.14729563\n",
            "Iteration 5891, loss = 0.14722399\n",
            "Iteration 5892, loss = 0.14718822\n",
            "Iteration 5893, loss = 0.14715347\n",
            "Iteration 5894, loss = 0.14713437\n",
            "Iteration 5895, loss = 0.14711998\n",
            "Iteration 5896, loss = 0.14714479\n",
            "Iteration 5897, loss = 0.14709468\n",
            "Iteration 5898, loss = 0.14708158\n",
            "Iteration 5899, loss = 0.14709356\n",
            "Iteration 5900, loss = 0.14702983\n",
            "Iteration 5901, loss = 0.14705406\n",
            "Iteration 5902, loss = 0.14700163\n",
            "Iteration 5903, loss = 0.14698341\n",
            "Iteration 5904, loss = 0.14694571\n",
            "Iteration 5905, loss = 0.14691377\n",
            "Iteration 5906, loss = 0.14689960\n",
            "Iteration 5907, loss = 0.14686918\n",
            "Iteration 5908, loss = 0.14686598\n",
            "Iteration 5909, loss = 0.14682761\n",
            "Iteration 5910, loss = 0.14684364\n",
            "Iteration 5911, loss = 0.14681402\n",
            "Iteration 5912, loss = 0.14689025\n",
            "Iteration 5913, loss = 0.14674151\n",
            "Iteration 5914, loss = 0.14669476\n",
            "Iteration 5915, loss = 0.14671434\n",
            "Iteration 5916, loss = 0.14671237\n",
            "Iteration 5917, loss = 0.14673665\n",
            "Iteration 5918, loss = 0.14670102\n",
            "Iteration 5919, loss = 0.14667674\n",
            "Iteration 5920, loss = 0.14664034\n",
            "Iteration 5921, loss = 0.14654643\n",
            "Iteration 5922, loss = 0.14658738\n",
            "Iteration 5923, loss = 0.14656350\n",
            "Iteration 5924, loss = 0.14653807\n",
            "Iteration 5925, loss = 0.14651964\n",
            "Iteration 5926, loss = 0.14660978\n",
            "Iteration 5927, loss = 0.14655267\n",
            "Iteration 5928, loss = 0.14654753\n",
            "Iteration 5929, loss = 0.14649810\n",
            "Iteration 5930, loss = 0.14655793\n",
            "Iteration 5931, loss = 0.14650881\n",
            "Iteration 5932, loss = 0.14646805\n",
            "Iteration 5933, loss = 0.14645221\n",
            "Iteration 5934, loss = 0.14639824\n",
            "Iteration 5935, loss = 0.14641987\n",
            "Iteration 5936, loss = 0.14635703\n",
            "Iteration 5937, loss = 0.14632444\n",
            "Iteration 5938, loss = 0.14627998\n",
            "Iteration 5939, loss = 0.14630641\n",
            "Iteration 5940, loss = 0.14622128\n",
            "Iteration 5941, loss = 0.14621984\n",
            "Iteration 5942, loss = 0.14618877\n",
            "Iteration 5943, loss = 0.14621940\n",
            "Iteration 5944, loss = 0.14616175\n",
            "Iteration 5945, loss = 0.14612763\n",
            "Iteration 5946, loss = 0.14611854\n",
            "Iteration 5947, loss = 0.14607675\n",
            "Iteration 5948, loss = 0.14604966\n",
            "Iteration 5949, loss = 0.14603211\n",
            "Iteration 5950, loss = 0.14604369\n",
            "Iteration 5951, loss = 0.14601488\n",
            "Iteration 5952, loss = 0.14605177\n",
            "Iteration 5953, loss = 0.14598810\n",
            "Iteration 5954, loss = 0.14598095\n",
            "Iteration 5955, loss = 0.14597475\n",
            "Iteration 5956, loss = 0.14593142\n",
            "Iteration 5957, loss = 0.14593332\n",
            "Iteration 5958, loss = 0.14590427\n",
            "Iteration 5959, loss = 0.14585385\n",
            "Iteration 5960, loss = 0.14587408\n",
            "Iteration 5961, loss = 0.14582164\n",
            "Iteration 5962, loss = 0.14579561\n",
            "Iteration 5963, loss = 0.14583084\n",
            "Iteration 5964, loss = 0.14575812\n",
            "Iteration 5965, loss = 0.14578950\n",
            "Iteration 5966, loss = 0.14572293\n",
            "Iteration 5967, loss = 0.14569036\n",
            "Iteration 5968, loss = 0.14568626\n",
            "Iteration 5969, loss = 0.14570591\n",
            "Iteration 5970, loss = 0.14564496\n",
            "Iteration 5971, loss = 0.14565087\n",
            "Iteration 5972, loss = 0.14562170\n",
            "Iteration 5973, loss = 0.14555196\n",
            "Iteration 5974, loss = 0.14555903\n",
            "Iteration 5975, loss = 0.14555363\n",
            "Iteration 5976, loss = 0.14557412\n",
            "Iteration 5977, loss = 0.14556392\n",
            "Iteration 5978, loss = 0.14550033\n",
            "Iteration 5979, loss = 0.14546535\n",
            "Iteration 5980, loss = 0.14548322\n",
            "Iteration 5981, loss = 0.14544523\n",
            "Iteration 5982, loss = 0.14542570\n",
            "Iteration 5983, loss = 0.14539769\n",
            "Iteration 5984, loss = 0.14539284\n",
            "Iteration 5985, loss = 0.14543373\n",
            "Iteration 5986, loss = 0.14536256\n",
            "Iteration 5987, loss = 0.14534213\n",
            "Iteration 5988, loss = 0.14533375\n",
            "Iteration 5989, loss = 0.14529461\n",
            "Iteration 5990, loss = 0.14529879\n",
            "Iteration 5991, loss = 0.14529418\n",
            "Iteration 5992, loss = 0.14527366\n",
            "Iteration 5993, loss = 0.14529283\n",
            "Iteration 5994, loss = 0.14525536\n",
            "Iteration 5995, loss = 0.14519775\n",
            "Iteration 5996, loss = 0.14516136\n",
            "Iteration 5997, loss = 0.14518738\n",
            "Iteration 5998, loss = 0.14512085\n",
            "Iteration 5999, loss = 0.14514187\n",
            "Iteration 6000, loss = 0.14509781\n",
            "Iteration 6001, loss = 0.14513021\n",
            "Iteration 6002, loss = 0.14508157\n",
            "Iteration 6003, loss = 0.14512686\n",
            "Iteration 6004, loss = 0.14512624\n",
            "Iteration 6005, loss = 0.14509284\n",
            "Iteration 6006, loss = 0.14509917\n",
            "Iteration 6007, loss = 0.14505433\n",
            "Iteration 6008, loss = 0.14504567\n",
            "Iteration 6009, loss = 0.14502994\n",
            "Iteration 6010, loss = 0.14497087\n",
            "Iteration 6011, loss = 0.14491495\n",
            "Iteration 6012, loss = 0.14491676\n",
            "Iteration 6013, loss = 0.14487261\n",
            "Iteration 6014, loss = 0.14486299\n",
            "Iteration 6015, loss = 0.14491677\n",
            "Iteration 6016, loss = 0.14484340\n",
            "Iteration 6017, loss = 0.14481249\n",
            "Iteration 6018, loss = 0.14480667\n",
            "Iteration 6019, loss = 0.14477384\n",
            "Iteration 6020, loss = 0.14476147\n",
            "Iteration 6021, loss = 0.14474886\n",
            "Iteration 6022, loss = 0.14472837\n",
            "Iteration 6023, loss = 0.14468766\n",
            "Iteration 6024, loss = 0.14472449\n",
            "Iteration 6025, loss = 0.14463737\n",
            "Iteration 6026, loss = 0.14462485\n",
            "Iteration 6027, loss = 0.14462032\n",
            "Iteration 6028, loss = 0.14459533\n",
            "Iteration 6029, loss = 0.14456841\n",
            "Iteration 6030, loss = 0.14454197\n",
            "Iteration 6031, loss = 0.14453887\n",
            "Iteration 6032, loss = 0.14451883\n",
            "Iteration 6033, loss = 0.14451161\n",
            "Iteration 6034, loss = 0.14447569\n",
            "Iteration 6035, loss = 0.14446629\n",
            "Iteration 6036, loss = 0.14451293\n",
            "Iteration 6037, loss = 0.14446587\n",
            "Iteration 6038, loss = 0.14449219\n",
            "Iteration 6039, loss = 0.14445277\n",
            "Iteration 6040, loss = 0.14443166\n",
            "Iteration 6041, loss = 0.14440800\n",
            "Iteration 6042, loss = 0.14436978\n",
            "Iteration 6043, loss = 0.14430966\n",
            "Iteration 6044, loss = 0.14430032\n",
            "Iteration 6045, loss = 0.14427221\n",
            "Iteration 6046, loss = 0.14428748\n",
            "Iteration 6047, loss = 0.14425598\n",
            "Iteration 6048, loss = 0.14424183\n",
            "Iteration 6049, loss = 0.14422945\n",
            "Iteration 6050, loss = 0.14424715\n",
            "Iteration 6051, loss = 0.14422503\n",
            "Iteration 6052, loss = 0.14420865\n",
            "Iteration 6053, loss = 0.14416603\n",
            "Iteration 6054, loss = 0.14414696\n",
            "Iteration 6055, loss = 0.14410749\n",
            "Iteration 6056, loss = 0.14410409\n",
            "Iteration 6057, loss = 0.14409644\n",
            "Iteration 6058, loss = 0.14408425\n",
            "Iteration 6059, loss = 0.14406768\n",
            "Iteration 6060, loss = 0.14403899\n",
            "Iteration 6061, loss = 0.14403342\n",
            "Iteration 6062, loss = 0.14399823\n",
            "Iteration 6063, loss = 0.14395481\n",
            "Iteration 6064, loss = 0.14395404\n",
            "Iteration 6065, loss = 0.14391747\n",
            "Iteration 6066, loss = 0.14394087\n",
            "Iteration 6067, loss = 0.14389141\n",
            "Iteration 6068, loss = 0.14388759\n",
            "Iteration 6069, loss = 0.14386475\n",
            "Iteration 6070, loss = 0.14385543\n",
            "Iteration 6071, loss = 0.14383919\n",
            "Iteration 6072, loss = 0.14380189\n",
            "Iteration 6073, loss = 0.14377220\n",
            "Iteration 6074, loss = 0.14387140\n",
            "Iteration 6075, loss = 0.14381838\n",
            "Iteration 6076, loss = 0.14392500\n",
            "Iteration 6077, loss = 0.14380506\n",
            "Iteration 6078, loss = 0.14378878\n",
            "Iteration 6079, loss = 0.14380136\n",
            "Iteration 6080, loss = 0.14370470\n",
            "Iteration 6081, loss = 0.14371171\n",
            "Iteration 6082, loss = 0.14366775\n",
            "Iteration 6083, loss = 0.14364212\n",
            "Iteration 6084, loss = 0.14360005\n",
            "Iteration 6085, loss = 0.14359783\n",
            "Iteration 6086, loss = 0.14356994\n",
            "Iteration 6087, loss = 0.14354417\n",
            "Iteration 6088, loss = 0.14351905\n",
            "Iteration 6089, loss = 0.14349455\n",
            "Iteration 6090, loss = 0.14347127\n",
            "Iteration 6091, loss = 0.14341785\n",
            "Iteration 6092, loss = 0.14353937\n",
            "Iteration 6093, loss = 0.14343555\n",
            "Iteration 6094, loss = 0.14339656\n",
            "Iteration 6095, loss = 0.14342997\n",
            "Iteration 6096, loss = 0.14336506\n",
            "Iteration 6097, loss = 0.14334380\n",
            "Iteration 6098, loss = 0.14332215\n",
            "Iteration 6099, loss = 0.14330534\n",
            "Iteration 6100, loss = 0.14328260\n",
            "Iteration 6101, loss = 0.14325623\n",
            "Iteration 6102, loss = 0.14323824\n",
            "Iteration 6103, loss = 0.14323970\n",
            "Iteration 6104, loss = 0.14320360\n",
            "Iteration 6105, loss = 0.14324432\n",
            "Iteration 6106, loss = 0.14323298\n",
            "Iteration 6107, loss = 0.14323263\n",
            "Iteration 6108, loss = 0.14317625\n",
            "Iteration 6109, loss = 0.14321722\n",
            "Iteration 6110, loss = 0.14317398\n",
            "Iteration 6111, loss = 0.14310226\n",
            "Iteration 6112, loss = 0.14306435\n",
            "Iteration 6113, loss = 0.14304492\n",
            "Iteration 6114, loss = 0.14303698\n",
            "Iteration 6115, loss = 0.14300320\n",
            "Iteration 6116, loss = 0.14297822\n",
            "Iteration 6117, loss = 0.14297828\n",
            "Iteration 6118, loss = 0.14294962\n",
            "Iteration 6119, loss = 0.14297847\n",
            "Iteration 6120, loss = 0.14294598\n",
            "Iteration 6121, loss = 0.14292898\n",
            "Iteration 6122, loss = 0.14291250\n",
            "Iteration 6123, loss = 0.14290696\n",
            "Iteration 6124, loss = 0.14290591\n",
            "Iteration 6125, loss = 0.14285039\n",
            "Iteration 6126, loss = 0.14283497\n",
            "Iteration 6127, loss = 0.14281599\n",
            "Iteration 6128, loss = 0.14283365\n",
            "Iteration 6129, loss = 0.14280724\n",
            "Iteration 6130, loss = 0.14275870\n",
            "Iteration 6131, loss = 0.14274984\n",
            "Iteration 6132, loss = 0.14270429\n",
            "Iteration 6133, loss = 0.14269497\n",
            "Iteration 6134, loss = 0.14268415\n",
            "Iteration 6135, loss = 0.14264843\n",
            "Iteration 6136, loss = 0.14264529\n",
            "Iteration 6137, loss = 0.14262703\n",
            "Iteration 6138, loss = 0.14261837\n",
            "Iteration 6139, loss = 0.14260628\n",
            "Iteration 6140, loss = 0.14263680\n",
            "Iteration 6141, loss = 0.14261336\n",
            "Iteration 6142, loss = 0.14258167\n",
            "Iteration 6143, loss = 0.14258590\n",
            "Iteration 6144, loss = 0.14257172\n",
            "Iteration 6145, loss = 0.14253441\n",
            "Iteration 6146, loss = 0.14247408\n",
            "Iteration 6147, loss = 0.14246964\n",
            "Iteration 6148, loss = 0.14243530\n",
            "Iteration 6149, loss = 0.14240990\n",
            "Iteration 6150, loss = 0.14241572\n",
            "Iteration 6151, loss = 0.14240646\n",
            "Iteration 6152, loss = 0.14244757\n",
            "Iteration 6153, loss = 0.14241581\n",
            "Iteration 6154, loss = 0.14237173\n",
            "Iteration 6155, loss = 0.14233513\n",
            "Iteration 6156, loss = 0.14231763\n",
            "Iteration 6157, loss = 0.14230242\n",
            "Iteration 6158, loss = 0.14226891\n",
            "Iteration 6159, loss = 0.14224507\n",
            "Iteration 6160, loss = 0.14223413\n",
            "Iteration 6161, loss = 0.14223352\n",
            "Iteration 6162, loss = 0.14219473\n",
            "Iteration 6163, loss = 0.14216249\n",
            "Iteration 6164, loss = 0.14216389\n",
            "Iteration 6165, loss = 0.14213205\n",
            "Iteration 6166, loss = 0.14217063\n",
            "Iteration 6167, loss = 0.14212319\n",
            "Iteration 6168, loss = 0.14210714\n",
            "Iteration 6169, loss = 0.14207980\n",
            "Iteration 6170, loss = 0.14207100\n",
            "Iteration 6171, loss = 0.14207532\n",
            "Iteration 6172, loss = 0.14205048\n",
            "Iteration 6173, loss = 0.14207738\n",
            "Iteration 6174, loss = 0.14198397\n",
            "Iteration 6175, loss = 0.14199228\n",
            "Iteration 6176, loss = 0.14197423\n",
            "Iteration 6177, loss = 0.14195810\n",
            "Iteration 6178, loss = 0.14193624\n",
            "Iteration 6179, loss = 0.14192741\n",
            "Iteration 6180, loss = 0.14193074\n",
            "Iteration 6181, loss = 0.14191806\n",
            "Iteration 6182, loss = 0.14193512\n",
            "Iteration 6183, loss = 0.14188915\n",
            "Iteration 6184, loss = 0.14193906\n",
            "Iteration 6185, loss = 0.14191206\n",
            "Iteration 6186, loss = 0.14185716\n",
            "Iteration 6187, loss = 0.14182378\n",
            "Iteration 6188, loss = 0.14193359\n",
            "Iteration 6189, loss = 0.14177967\n",
            "Iteration 6190, loss = 0.14176706\n",
            "Iteration 6191, loss = 0.14176322\n",
            "Iteration 6192, loss = 0.14173746\n",
            "Iteration 6193, loss = 0.14172139\n",
            "Iteration 6194, loss = 0.14174658\n",
            "Iteration 6195, loss = 0.14178631\n",
            "Iteration 6196, loss = 0.14173751\n",
            "Iteration 6197, loss = 0.14171106\n",
            "Iteration 6198, loss = 0.14170155\n",
            "Iteration 6199, loss = 0.14163356\n",
            "Iteration 6200, loss = 0.14161390\n",
            "Iteration 6201, loss = 0.14162342\n",
            "Iteration 6202, loss = 0.14159380\n",
            "Iteration 6203, loss = 0.14157393\n",
            "Iteration 6204, loss = 0.14152791\n",
            "Iteration 6205, loss = 0.14155103\n",
            "Iteration 6206, loss = 0.14149267\n",
            "Iteration 6207, loss = 0.14155415\n",
            "Iteration 6208, loss = 0.14143836\n",
            "Iteration 6209, loss = 0.14141549\n",
            "Iteration 6210, loss = 0.14142389\n",
            "Iteration 6211, loss = 0.14140994\n",
            "Iteration 6212, loss = 0.14138313\n",
            "Iteration 6213, loss = 0.14135016\n",
            "Iteration 6214, loss = 0.14135114\n",
            "Iteration 6215, loss = 0.14138164\n",
            "Iteration 6216, loss = 0.14138039\n",
            "Iteration 6217, loss = 0.14134449\n",
            "Iteration 6218, loss = 0.14132663\n",
            "Iteration 6219, loss = 0.14133140\n",
            "Iteration 6220, loss = 0.14127438\n",
            "Iteration 6221, loss = 0.14127998\n",
            "Iteration 6222, loss = 0.14124647\n",
            "Iteration 6223, loss = 0.14127108\n",
            "Iteration 6224, loss = 0.14123083\n",
            "Iteration 6225, loss = 0.14119237\n",
            "Iteration 6226, loss = 0.14119346\n",
            "Iteration 6227, loss = 0.14121927\n",
            "Iteration 6228, loss = 0.14119445\n",
            "Iteration 6229, loss = 0.14114509\n",
            "Iteration 6230, loss = 0.14111224\n",
            "Iteration 6231, loss = 0.14110324\n",
            "Iteration 6232, loss = 0.14111161\n",
            "Iteration 6233, loss = 0.14105796\n",
            "Iteration 6234, loss = 0.14104459\n",
            "Iteration 6235, loss = 0.14099816\n",
            "Iteration 6236, loss = 0.14105634\n",
            "Iteration 6237, loss = 0.14098106\n",
            "Iteration 6238, loss = 0.14093444\n",
            "Iteration 6239, loss = 0.14091518\n",
            "Iteration 6240, loss = 0.14091072\n",
            "Iteration 6241, loss = 0.14103721\n",
            "Iteration 6242, loss = 0.14090911\n",
            "Iteration 6243, loss = 0.14090467\n",
            "Iteration 6244, loss = 0.14086248\n",
            "Iteration 6245, loss = 0.14083071\n",
            "Iteration 6246, loss = 0.14082265\n",
            "Iteration 6247, loss = 0.14079427\n",
            "Iteration 6248, loss = 0.14078925\n",
            "Iteration 6249, loss = 0.14078141\n",
            "Iteration 6250, loss = 0.14072647\n",
            "Iteration 6251, loss = 0.14071105\n",
            "Iteration 6252, loss = 0.14069478\n",
            "Iteration 6253, loss = 0.14068824\n",
            "Iteration 6254, loss = 0.14068198\n",
            "Iteration 6255, loss = 0.14066613\n",
            "Iteration 6256, loss = 0.14063640\n",
            "Iteration 6257, loss = 0.14059848\n",
            "Iteration 6258, loss = 0.14060160\n",
            "Iteration 6259, loss = 0.14060229\n",
            "Iteration 6260, loss = 0.14063822\n",
            "Iteration 6261, loss = 0.14059213\n",
            "Iteration 6262, loss = 0.14058491\n",
            "Iteration 6263, loss = 0.14057336\n",
            "Iteration 6264, loss = 0.14057618\n",
            "Iteration 6265, loss = 0.14054578\n",
            "Iteration 6266, loss = 0.14052793\n",
            "Iteration 6267, loss = 0.14052471\n",
            "Iteration 6268, loss = 0.14054759\n",
            "Iteration 6269, loss = 0.14049393\n",
            "Iteration 6270, loss = 0.14045859\n",
            "Iteration 6271, loss = 0.14048066\n",
            "Iteration 6272, loss = 0.14046207\n",
            "Iteration 6273, loss = 0.14040750\n",
            "Iteration 6274, loss = 0.14047636\n",
            "Iteration 6275, loss = 0.14039378\n",
            "Iteration 6276, loss = 0.14036671\n",
            "Iteration 6277, loss = 0.14033478\n",
            "Iteration 6278, loss = 0.14031272\n",
            "Iteration 6279, loss = 0.14029103\n",
            "Iteration 6280, loss = 0.14034017\n",
            "Iteration 6281, loss = 0.14030610\n",
            "Iteration 6282, loss = 0.14027888\n",
            "Iteration 6283, loss = 0.14024468\n",
            "Iteration 6284, loss = 0.14025322\n",
            "Iteration 6285, loss = 0.14027508\n",
            "Iteration 6286, loss = 0.14017109\n",
            "Iteration 6287, loss = 0.14022612\n",
            "Iteration 6288, loss = 0.14020764\n",
            "Iteration 6289, loss = 0.14013555\n",
            "Iteration 6290, loss = 0.14012640\n",
            "Iteration 6291, loss = 0.14009132\n",
            "Iteration 6292, loss = 0.14009772\n",
            "Iteration 6293, loss = 0.14005607\n",
            "Iteration 6294, loss = 0.14005546\n",
            "Iteration 6295, loss = 0.14002304\n",
            "Iteration 6296, loss = 0.14000633\n",
            "Iteration 6297, loss = 0.13996573\n",
            "Iteration 6298, loss = 0.13999179\n",
            "Iteration 6299, loss = 0.13997816\n",
            "Iteration 6300, loss = 0.13991841\n",
            "Iteration 6301, loss = 0.13999942\n",
            "Iteration 6302, loss = 0.13991768\n",
            "Iteration 6303, loss = 0.13991824\n",
            "Iteration 6304, loss = 0.13990925\n",
            "Iteration 6305, loss = 0.13986693\n",
            "Iteration 6306, loss = 0.13983848\n",
            "Iteration 6307, loss = 0.13985535\n",
            "Iteration 6308, loss = 0.13982895\n",
            "Iteration 6309, loss = 0.13982752\n",
            "Iteration 6310, loss = 0.13978614\n",
            "Iteration 6311, loss = 0.13978068\n",
            "Iteration 6312, loss = 0.13976802\n",
            "Iteration 6313, loss = 0.13974968\n",
            "Iteration 6314, loss = 0.13972241\n",
            "Iteration 6315, loss = 0.13972361\n",
            "Iteration 6316, loss = 0.13972659\n",
            "Iteration 6317, loss = 0.13973614\n",
            "Iteration 6318, loss = 0.13973237\n",
            "Iteration 6319, loss = 0.13969776\n",
            "Iteration 6320, loss = 0.13968791\n",
            "Iteration 6321, loss = 0.13963627\n",
            "Iteration 6322, loss = 0.13962448\n",
            "Iteration 6323, loss = 0.13961746\n",
            "Iteration 6324, loss = 0.13959658\n",
            "Iteration 6325, loss = 0.13959648\n",
            "Iteration 6326, loss = 0.13956420\n",
            "Iteration 6327, loss = 0.13959397\n",
            "Iteration 6328, loss = 0.13955345\n",
            "Iteration 6329, loss = 0.13953464\n",
            "Iteration 6330, loss = 0.13950524\n",
            "Iteration 6331, loss = 0.13953168\n",
            "Iteration 6332, loss = 0.13945351\n",
            "Iteration 6333, loss = 0.13944916\n",
            "Iteration 6334, loss = 0.13941436\n",
            "Iteration 6335, loss = 0.13939711\n",
            "Iteration 6336, loss = 0.13940358\n",
            "Iteration 6337, loss = 0.13936457\n",
            "Iteration 6338, loss = 0.13937283\n",
            "Iteration 6339, loss = 0.13935436\n",
            "Iteration 6340, loss = 0.13933479\n",
            "Iteration 6341, loss = 0.13933956\n",
            "Iteration 6342, loss = 0.13929137\n",
            "Iteration 6343, loss = 0.13928031\n",
            "Iteration 6344, loss = 0.13932605\n",
            "Iteration 6345, loss = 0.13927435\n",
            "Iteration 6346, loss = 0.13927625\n",
            "Iteration 6347, loss = 0.13926128\n",
            "Iteration 6348, loss = 0.13924000\n",
            "Iteration 6349, loss = 0.13922565\n",
            "Iteration 6350, loss = 0.13920461\n",
            "Iteration 6351, loss = 0.13925240\n",
            "Iteration 6352, loss = 0.13917168\n",
            "Iteration 6353, loss = 0.13916142\n",
            "Iteration 6354, loss = 0.13914554\n",
            "Iteration 6355, loss = 0.13909752\n",
            "Iteration 6356, loss = 0.13910488\n",
            "Iteration 6357, loss = 0.13909184\n",
            "Iteration 6358, loss = 0.13904243\n",
            "Iteration 6359, loss = 0.13904495\n",
            "Iteration 6360, loss = 0.13904636\n",
            "Iteration 6361, loss = 0.13903286\n",
            "Iteration 6362, loss = 0.13897471\n",
            "Iteration 6363, loss = 0.13896011\n",
            "Iteration 6364, loss = 0.13898400\n",
            "Iteration 6365, loss = 0.13893945\n",
            "Iteration 6366, loss = 0.13893656\n",
            "Iteration 6367, loss = 0.13892129\n",
            "Iteration 6368, loss = 0.13889757\n",
            "Iteration 6369, loss = 0.13890018\n",
            "Iteration 6370, loss = 0.13891388\n",
            "Iteration 6371, loss = 0.13890350\n",
            "Iteration 6372, loss = 0.13882087\n",
            "Iteration 6373, loss = 0.13887182\n",
            "Iteration 6374, loss = 0.13882197\n",
            "Iteration 6375, loss = 0.13879820\n",
            "Iteration 6376, loss = 0.13876591\n",
            "Iteration 6377, loss = 0.13872970\n",
            "Iteration 6378, loss = 0.13874052\n",
            "Iteration 6379, loss = 0.13875024\n",
            "Iteration 6380, loss = 0.13873266\n",
            "Iteration 6381, loss = 0.13870987\n",
            "Iteration 6382, loss = 0.13868481\n",
            "Iteration 6383, loss = 0.13870765\n",
            "Iteration 6384, loss = 0.13867579\n",
            "Iteration 6385, loss = 0.13867644\n",
            "Iteration 6386, loss = 0.13864566\n",
            "Iteration 6387, loss = 0.13867380\n",
            "Iteration 6388, loss = 0.13862627\n",
            "Iteration 6389, loss = 0.13860107\n",
            "Iteration 6390, loss = 0.13859159\n",
            "Iteration 6391, loss = 0.13854758\n",
            "Iteration 6392, loss = 0.13856626\n",
            "Iteration 6393, loss = 0.13853788\n",
            "Iteration 6394, loss = 0.13853089\n",
            "Iteration 6395, loss = 0.13852307\n",
            "Iteration 6396, loss = 0.13850265\n",
            "Iteration 6397, loss = 0.13853738\n",
            "Iteration 6398, loss = 0.13845285\n",
            "Iteration 6399, loss = 0.13845022\n",
            "Iteration 6400, loss = 0.13843198\n",
            "Iteration 6401, loss = 0.13841710\n",
            "Iteration 6402, loss = 0.13840481\n",
            "Iteration 6403, loss = 0.13837756\n",
            "Iteration 6404, loss = 0.13836073\n",
            "Iteration 6405, loss = 0.13836656\n",
            "Iteration 6406, loss = 0.13833080\n",
            "Iteration 6407, loss = 0.13834666\n",
            "Iteration 6408, loss = 0.13834514\n",
            "Iteration 6409, loss = 0.13832670\n",
            "Iteration 6410, loss = 0.13830170\n",
            "Iteration 6411, loss = 0.13828330\n",
            "Iteration 6412, loss = 0.13826918\n",
            "Iteration 6413, loss = 0.13822079\n",
            "Iteration 6414, loss = 0.13822547\n",
            "Iteration 6415, loss = 0.13823983\n",
            "Iteration 6416, loss = 0.13822918\n",
            "Iteration 6417, loss = 0.13823367\n",
            "Iteration 6418, loss = 0.13820286\n",
            "Iteration 6419, loss = 0.13822491\n",
            "Iteration 6420, loss = 0.13818774\n",
            "Iteration 6421, loss = 0.13814697\n",
            "Iteration 6422, loss = 0.13812339\n",
            "Iteration 6423, loss = 0.13811736\n",
            "Iteration 6424, loss = 0.13808571\n",
            "Iteration 6425, loss = 0.13805806\n",
            "Iteration 6426, loss = 0.13806824\n",
            "Iteration 6427, loss = 0.13806812\n",
            "Iteration 6428, loss = 0.13802785\n",
            "Iteration 6429, loss = 0.13802999\n",
            "Iteration 6430, loss = 0.13802987\n",
            "Iteration 6431, loss = 0.13801787\n",
            "Iteration 6432, loss = 0.13802142\n",
            "Iteration 6433, loss = 0.13797437\n",
            "Iteration 6434, loss = 0.13797421\n",
            "Iteration 6435, loss = 0.13793473\n",
            "Iteration 6436, loss = 0.13787006\n",
            "Iteration 6437, loss = 0.13797950\n",
            "Iteration 6438, loss = 0.13787770\n",
            "Iteration 6439, loss = 0.13784947\n",
            "Iteration 6440, loss = 0.13784782\n",
            "Iteration 6441, loss = 0.13788104\n",
            "Iteration 6442, loss = 0.13781176\n",
            "Iteration 6443, loss = 0.13781467\n",
            "Iteration 6444, loss = 0.13778256\n",
            "Iteration 6445, loss = 0.13775746\n",
            "Iteration 6446, loss = 0.13778477\n",
            "Iteration 6447, loss = 0.13773192\n",
            "Iteration 6448, loss = 0.13772514\n",
            "Iteration 6449, loss = 0.13770944\n",
            "Iteration 6450, loss = 0.13772972\n",
            "Iteration 6451, loss = 0.13767318\n",
            "Iteration 6452, loss = 0.13769797\n",
            "Iteration 6453, loss = 0.13766053\n",
            "Iteration 6454, loss = 0.13762523\n",
            "Iteration 6455, loss = 0.13761284\n",
            "Iteration 6456, loss = 0.13762863\n",
            "Iteration 6457, loss = 0.13758591\n",
            "Iteration 6458, loss = 0.13762027\n",
            "Iteration 6459, loss = 0.13759149\n",
            "Iteration 6460, loss = 0.13760055\n",
            "Iteration 6461, loss = 0.13755353\n",
            "Iteration 6462, loss = 0.13755722\n",
            "Iteration 6463, loss = 0.13753027\n",
            "Iteration 6464, loss = 0.13753321\n",
            "Iteration 6465, loss = 0.13750926\n",
            "Iteration 6466, loss = 0.13747266\n",
            "Iteration 6467, loss = 0.13746253\n",
            "Iteration 6468, loss = 0.13754720\n",
            "Iteration 6469, loss = 0.13744215\n",
            "Iteration 6470, loss = 0.13740131\n",
            "Iteration 6471, loss = 0.13739942\n",
            "Iteration 6472, loss = 0.13745525\n",
            "Iteration 6473, loss = 0.13740069\n",
            "Iteration 6474, loss = 0.13740769\n",
            "Iteration 6475, loss = 0.13743421\n",
            "Iteration 6476, loss = 0.13737007\n",
            "Iteration 6477, loss = 0.13735211\n",
            "Iteration 6478, loss = 0.13735653\n",
            "Iteration 6479, loss = 0.13734830\n",
            "Iteration 6480, loss = 0.13734882\n",
            "Iteration 6481, loss = 0.13734786\n",
            "Iteration 6482, loss = 0.13730504\n",
            "Iteration 6483, loss = 0.13728309\n",
            "Iteration 6484, loss = 0.13724735\n",
            "Iteration 6485, loss = 0.13726834\n",
            "Iteration 6486, loss = 0.13721907\n",
            "Iteration 6487, loss = 0.13725532\n",
            "Iteration 6488, loss = 0.13715050\n",
            "Iteration 6489, loss = 0.13714507\n",
            "Iteration 6490, loss = 0.13718705\n",
            "Iteration 6491, loss = 0.13719779\n",
            "Iteration 6492, loss = 0.13714810\n",
            "Iteration 6493, loss = 0.13714978\n",
            "Iteration 6494, loss = 0.13715026\n",
            "Iteration 6495, loss = 0.13713045\n",
            "Iteration 6496, loss = 0.13711968\n",
            "Iteration 6497, loss = 0.13709862\n",
            "Iteration 6498, loss = 0.13706704\n",
            "Iteration 6499, loss = 0.13704241\n",
            "Iteration 6500, loss = 0.13700897\n",
            "Iteration 6501, loss = 0.13701135\n",
            "Iteration 6502, loss = 0.13703602\n",
            "Iteration 6503, loss = 0.13695420\n",
            "Iteration 6504, loss = 0.13693345\n",
            "Iteration 6505, loss = 0.13694072\n",
            "Iteration 6506, loss = 0.13691866\n",
            "Iteration 6507, loss = 0.13690619\n",
            "Iteration 6508, loss = 0.13686528\n",
            "Iteration 6509, loss = 0.13683599\n",
            "Iteration 6510, loss = 0.13684506\n",
            "Iteration 6511, loss = 0.13680199\n",
            "Iteration 6512, loss = 0.13678207\n",
            "Iteration 6513, loss = 0.13685876\n",
            "Iteration 6514, loss = 0.13684580\n",
            "Iteration 6515, loss = 0.13678162\n",
            "Iteration 6516, loss = 0.13675269\n",
            "Iteration 6517, loss = 0.13674828\n",
            "Iteration 6518, loss = 0.13673163\n",
            "Iteration 6519, loss = 0.13676984\n",
            "Iteration 6520, loss = 0.13669333\n",
            "Iteration 6521, loss = 0.13667898\n",
            "Iteration 6522, loss = 0.13668146\n",
            "Iteration 6523, loss = 0.13670051\n",
            "Iteration 6524, loss = 0.13662723\n",
            "Iteration 6525, loss = 0.13667632\n",
            "Iteration 6526, loss = 0.13661317\n",
            "Iteration 6527, loss = 0.13660905\n",
            "Iteration 6528, loss = 0.13657753\n",
            "Iteration 6529, loss = 0.13655744\n",
            "Iteration 6530, loss = 0.13656756\n",
            "Iteration 6531, loss = 0.13652438\n",
            "Iteration 6532, loss = 0.13653389\n",
            "Iteration 6533, loss = 0.13649504\n",
            "Iteration 6534, loss = 0.13647655\n",
            "Iteration 6535, loss = 0.13645003\n",
            "Iteration 6536, loss = 0.13647266\n",
            "Iteration 6537, loss = 0.13642579\n",
            "Iteration 6538, loss = 0.13644335\n",
            "Iteration 6539, loss = 0.13644233\n",
            "Iteration 6540, loss = 0.13639533\n",
            "Iteration 6541, loss = 0.13642968\n",
            "Iteration 6542, loss = 0.13637646\n",
            "Iteration 6543, loss = 0.13634727\n",
            "Iteration 6544, loss = 0.13635608\n",
            "Iteration 6545, loss = 0.13630626\n",
            "Iteration 6546, loss = 0.13633111\n",
            "Iteration 6547, loss = 0.13629807\n",
            "Iteration 6548, loss = 0.13629139\n",
            "Iteration 6549, loss = 0.13627383\n",
            "Iteration 6550, loss = 0.13626750\n",
            "Iteration 6551, loss = 0.13623556\n",
            "Iteration 6552, loss = 0.13624769\n",
            "Iteration 6553, loss = 0.13622821\n",
            "Iteration 6554, loss = 0.13621093\n",
            "Iteration 6555, loss = 0.13622426\n",
            "Iteration 6556, loss = 0.13619755\n",
            "Iteration 6557, loss = 0.13621991\n",
            "Iteration 6558, loss = 0.13616877\n",
            "Iteration 6559, loss = 0.13615667\n",
            "Iteration 6560, loss = 0.13615514\n",
            "Iteration 6561, loss = 0.13612916\n",
            "Iteration 6562, loss = 0.13613788\n",
            "Iteration 6563, loss = 0.13610737\n",
            "Iteration 6564, loss = 0.13608458\n",
            "Iteration 6565, loss = 0.13607477\n",
            "Iteration 6566, loss = 0.13605413\n",
            "Iteration 6567, loss = 0.13605403\n",
            "Iteration 6568, loss = 0.13605323\n",
            "Iteration 6569, loss = 0.13604911\n",
            "Iteration 6570, loss = 0.13602872\n",
            "Iteration 6571, loss = 0.13600236\n",
            "Iteration 6572, loss = 0.13601413\n",
            "Iteration 6573, loss = 0.13599703\n",
            "Iteration 6574, loss = 0.13598822\n",
            "Iteration 6575, loss = 0.13594225\n",
            "Iteration 6576, loss = 0.13596606\n",
            "Iteration 6577, loss = 0.13589255\n",
            "Iteration 6578, loss = 0.13600240\n",
            "Iteration 6579, loss = 0.13595072\n",
            "Iteration 6580, loss = 0.13592080\n",
            "Iteration 6581, loss = 0.13591632\n",
            "Iteration 6582, loss = 0.13593687\n",
            "Iteration 6583, loss = 0.13595120\n",
            "Iteration 6584, loss = 0.13594620\n",
            "Iteration 6585, loss = 0.13590249\n",
            "Iteration 6586, loss = 0.13583846\n",
            "Iteration 6587, loss = 0.13583739\n",
            "Iteration 6588, loss = 0.13579247\n",
            "Iteration 6589, loss = 0.13577595\n",
            "Iteration 6590, loss = 0.13581520\n",
            "Iteration 6591, loss = 0.13577596\n",
            "Iteration 6592, loss = 0.13573384\n",
            "Iteration 6593, loss = 0.13570286\n",
            "Iteration 6594, loss = 0.13574784\n",
            "Iteration 6595, loss = 0.13569231\n",
            "Iteration 6596, loss = 0.13569645\n",
            "Iteration 6597, loss = 0.13566072\n",
            "Iteration 6598, loss = 0.13565794\n",
            "Iteration 6599, loss = 0.13562898\n",
            "Iteration 6600, loss = 0.13563125\n",
            "Iteration 6601, loss = 0.13560950\n",
            "Iteration 6602, loss = 0.13564287\n",
            "Iteration 6603, loss = 0.13572057\n",
            "Iteration 6604, loss = 0.13567403\n",
            "Iteration 6605, loss = 0.13565269\n",
            "Iteration 6606, loss = 0.13565005\n",
            "Iteration 6607, loss = 0.13556804\n",
            "Iteration 6608, loss = 0.13559163\n",
            "Iteration 6609, loss = 0.13559478\n",
            "Iteration 6610, loss = 0.13554110\n",
            "Iteration 6611, loss = 0.13554642\n",
            "Iteration 6612, loss = 0.13553086\n",
            "Iteration 6613, loss = 0.13550995\n",
            "Iteration 6614, loss = 0.13551755\n",
            "Iteration 6615, loss = 0.13547540\n",
            "Iteration 6616, loss = 0.13549760\n",
            "Iteration 6617, loss = 0.13548436\n",
            "Iteration 6618, loss = 0.13547370\n",
            "Iteration 6619, loss = 0.13547016\n",
            "Iteration 6620, loss = 0.13546340\n",
            "Iteration 6621, loss = 0.13543195\n",
            "Iteration 6622, loss = 0.13540509\n",
            "Iteration 6623, loss = 0.13539874\n",
            "Iteration 6624, loss = 0.13535101\n",
            "Iteration 6625, loss = 0.13536075\n",
            "Iteration 6626, loss = 0.13531090\n",
            "Iteration 6627, loss = 0.13530579\n",
            "Iteration 6628, loss = 0.13528848\n",
            "Iteration 6629, loss = 0.13526768\n",
            "Iteration 6630, loss = 0.13526209\n",
            "Iteration 6631, loss = 0.13522825\n",
            "Iteration 6632, loss = 0.13525263\n",
            "Iteration 6633, loss = 0.13520435\n",
            "Iteration 6634, loss = 0.13518529\n",
            "Iteration 6635, loss = 0.13517006\n",
            "Iteration 6636, loss = 0.13517062\n",
            "Iteration 6637, loss = 0.13515076\n",
            "Iteration 6638, loss = 0.13511632\n",
            "Iteration 6639, loss = 0.13515407\n",
            "Iteration 6640, loss = 0.13509979\n",
            "Iteration 6641, loss = 0.13510332\n",
            "Iteration 6642, loss = 0.13507567\n",
            "Iteration 6643, loss = 0.13506318\n",
            "Iteration 6644, loss = 0.13504043\n",
            "Iteration 6645, loss = 0.13504876\n",
            "Iteration 6646, loss = 0.13502445\n",
            "Iteration 6647, loss = 0.13503263\n",
            "Iteration 6648, loss = 0.13496472\n",
            "Iteration 6649, loss = 0.13497281\n",
            "Iteration 6650, loss = 0.13500852\n",
            "Iteration 6651, loss = 0.13494393\n",
            "Iteration 6652, loss = 0.13493408\n",
            "Iteration 6653, loss = 0.13492623\n",
            "Iteration 6654, loss = 0.13491518\n",
            "Iteration 6655, loss = 0.13490655\n",
            "Iteration 6656, loss = 0.13486013\n",
            "Iteration 6657, loss = 0.13487849\n",
            "Iteration 6658, loss = 0.13487477\n",
            "Iteration 6659, loss = 0.13485659\n",
            "Iteration 6660, loss = 0.13485672\n",
            "Iteration 6661, loss = 0.13485149\n",
            "Iteration 6662, loss = 0.13481018\n",
            "Iteration 6663, loss = 0.13478538\n",
            "Iteration 6664, loss = 0.13480024\n",
            "Iteration 6665, loss = 0.13476509\n",
            "Iteration 6666, loss = 0.13475386\n",
            "Iteration 6667, loss = 0.13472875\n",
            "Iteration 6668, loss = 0.13481289\n",
            "Iteration 6669, loss = 0.13466552\n",
            "Iteration 6670, loss = 0.13471453\n",
            "Iteration 6671, loss = 0.13467680\n",
            "Iteration 6672, loss = 0.13463680\n",
            "Iteration 6673, loss = 0.13465405\n",
            "Iteration 6674, loss = 0.13467370\n",
            "Iteration 6675, loss = 0.13462509\n",
            "Iteration 6676, loss = 0.13464219\n",
            "Iteration 6677, loss = 0.13460325\n",
            "Iteration 6678, loss = 0.13459783\n",
            "Iteration 6679, loss = 0.13466957\n",
            "Iteration 6680, loss = 0.13456120\n",
            "Iteration 6681, loss = 0.13454826\n",
            "Iteration 6682, loss = 0.13454545\n",
            "Iteration 6683, loss = 0.13453444\n",
            "Iteration 6684, loss = 0.13452622\n",
            "Iteration 6685, loss = 0.13450291\n",
            "Iteration 6686, loss = 0.13452622\n",
            "Iteration 6687, loss = 0.13448092\n",
            "Iteration 6688, loss = 0.13447096\n",
            "Iteration 6689, loss = 0.13442701\n",
            "Iteration 6690, loss = 0.13440792\n",
            "Iteration 6691, loss = 0.13443160\n",
            "Iteration 6692, loss = 0.13440617\n",
            "Iteration 6693, loss = 0.13438231\n",
            "Iteration 6694, loss = 0.13438897\n",
            "Iteration 6695, loss = 0.13435510\n",
            "Iteration 6696, loss = 0.13433059\n",
            "Iteration 6697, loss = 0.13437040\n",
            "Iteration 6698, loss = 0.13434084\n",
            "Iteration 6699, loss = 0.13432466\n",
            "Iteration 6700, loss = 0.13432529\n",
            "Iteration 6701, loss = 0.13432369\n",
            "Iteration 6702, loss = 0.13430217\n",
            "Iteration 6703, loss = 0.13428774\n",
            "Iteration 6704, loss = 0.13429573\n",
            "Iteration 6705, loss = 0.13429228\n",
            "Iteration 6706, loss = 0.13428279\n",
            "Iteration 6707, loss = 0.13429229\n",
            "Iteration 6708, loss = 0.13426921\n",
            "Iteration 6709, loss = 0.13425221\n",
            "Iteration 6710, loss = 0.13426918\n",
            "Iteration 6711, loss = 0.13427197\n",
            "Iteration 6712, loss = 0.13430352\n",
            "Iteration 6713, loss = 0.13421227\n",
            "Iteration 6714, loss = 0.13416894\n",
            "Iteration 6715, loss = 0.13416747\n",
            "Iteration 6716, loss = 0.13412134\n",
            "Iteration 6717, loss = 0.13415682\n",
            "Iteration 6718, loss = 0.13408866\n",
            "Iteration 6719, loss = 0.13407727\n",
            "Iteration 6720, loss = 0.13407753\n",
            "Iteration 6721, loss = 0.13411169\n",
            "Iteration 6722, loss = 0.13411036\n",
            "Iteration 6723, loss = 0.13411893\n",
            "Iteration 6724, loss = 0.13407492\n",
            "Iteration 6725, loss = 0.13401587\n",
            "Iteration 6726, loss = 0.13405624\n",
            "Iteration 6727, loss = 0.13399610\n",
            "Iteration 6728, loss = 0.13396701\n",
            "Iteration 6729, loss = 0.13398403\n",
            "Iteration 6730, loss = 0.13392079\n",
            "Iteration 6731, loss = 0.13393134\n",
            "Iteration 6732, loss = 0.13390321\n",
            "Iteration 6733, loss = 0.13389905\n",
            "Iteration 6734, loss = 0.13388254\n",
            "Iteration 6735, loss = 0.13385833\n",
            "Iteration 6736, loss = 0.13383891\n",
            "Iteration 6737, loss = 0.13391751\n",
            "Iteration 6738, loss = 0.13385381\n",
            "Iteration 6739, loss = 0.13380779\n",
            "Iteration 6740, loss = 0.13379946\n",
            "Iteration 6741, loss = 0.13376993\n",
            "Iteration 6742, loss = 0.13375696\n",
            "Iteration 6743, loss = 0.13377509\n",
            "Iteration 6744, loss = 0.13376416\n",
            "Iteration 6745, loss = 0.13374873\n",
            "Iteration 6746, loss = 0.13373752\n",
            "Iteration 6747, loss = 0.13372057\n",
            "Iteration 6748, loss = 0.13367198\n",
            "Iteration 6749, loss = 0.13367330\n",
            "Iteration 6750, loss = 0.13371253\n",
            "Iteration 6751, loss = 0.13367421\n",
            "Iteration 6752, loss = 0.13369305\n",
            "Iteration 6753, loss = 0.13378580\n",
            "Iteration 6754, loss = 0.13370910\n",
            "Iteration 6755, loss = 0.13367476\n",
            "Iteration 6756, loss = 0.13371151\n",
            "Iteration 6757, loss = 0.13366296\n",
            "Iteration 6758, loss = 0.13364934\n",
            "Iteration 6759, loss = 0.13358972\n",
            "Iteration 6760, loss = 0.13359461\n",
            "Iteration 6761, loss = 0.13361851\n",
            "Iteration 6762, loss = 0.13357763\n",
            "Iteration 6763, loss = 0.13355242\n",
            "Iteration 6764, loss = 0.13354511\n",
            "Iteration 6765, loss = 0.13353491\n",
            "Iteration 6766, loss = 0.13350795\n",
            "Iteration 6767, loss = 0.13350466\n",
            "Iteration 6768, loss = 0.13347860\n",
            "Iteration 6769, loss = 0.13349285\n",
            "Iteration 6770, loss = 0.13347488\n",
            "Iteration 6771, loss = 0.13346472\n",
            "Iteration 6772, loss = 0.13341552\n",
            "Iteration 6773, loss = 0.13342303\n",
            "Iteration 6774, loss = 0.13339936\n",
            "Iteration 6775, loss = 0.13340521\n",
            "Iteration 6776, loss = 0.13337752\n",
            "Iteration 6777, loss = 0.13335803\n",
            "Iteration 6778, loss = 0.13339610\n",
            "Iteration 6779, loss = 0.13336389\n",
            "Iteration 6780, loss = 0.13333767\n",
            "Iteration 6781, loss = 0.13331951\n",
            "Iteration 6782, loss = 0.13332374\n",
            "Iteration 6783, loss = 0.13328349\n",
            "Iteration 6784, loss = 0.13328566\n",
            "Iteration 6785, loss = 0.13326724\n",
            "Iteration 6786, loss = 0.13326336\n",
            "Iteration 6787, loss = 0.13325340\n",
            "Iteration 6788, loss = 0.13324638\n",
            "Iteration 6789, loss = 0.13324811\n",
            "Iteration 6790, loss = 0.13323345\n",
            "Iteration 6791, loss = 0.13318487\n",
            "Iteration 6792, loss = 0.13319515\n",
            "Iteration 6793, loss = 0.13316524\n",
            "Iteration 6794, loss = 0.13317122\n",
            "Iteration 6795, loss = 0.13318539\n",
            "Iteration 6796, loss = 0.13308532\n",
            "Iteration 6797, loss = 0.13311287\n",
            "Iteration 6798, loss = 0.13315759\n",
            "Iteration 6799, loss = 0.13312979\n",
            "Iteration 6800, loss = 0.13309843\n",
            "Iteration 6801, loss = 0.13308571\n",
            "Iteration 6802, loss = 0.13305541\n",
            "Iteration 6803, loss = 0.13308795\n",
            "Iteration 6804, loss = 0.13300755\n",
            "Iteration 6805, loss = 0.13315536\n",
            "Iteration 6806, loss = 0.13303707\n",
            "Iteration 6807, loss = 0.13301666\n",
            "Iteration 6808, loss = 0.13300807\n",
            "Iteration 6809, loss = 0.13300900\n",
            "Iteration 6810, loss = 0.13298242\n",
            "Iteration 6811, loss = 0.13296032\n",
            "Iteration 6812, loss = 0.13293547\n",
            "Iteration 6813, loss = 0.13293017\n",
            "Iteration 6814, loss = 0.13291243\n",
            "Iteration 6815, loss = 0.13289933\n",
            "Iteration 6816, loss = 0.13286020\n",
            "Iteration 6817, loss = 0.13291848\n",
            "Iteration 6818, loss = 0.13289774\n",
            "Iteration 6819, loss = 0.13288101\n",
            "Iteration 6820, loss = 0.13289066\n",
            "Iteration 6821, loss = 0.13286282\n",
            "Iteration 6822, loss = 0.13287043\n",
            "Iteration 6823, loss = 0.13290072\n",
            "Iteration 6824, loss = 0.13290781\n",
            "Iteration 6825, loss = 0.13287366\n",
            "Iteration 6826, loss = 0.13285280\n",
            "Iteration 6827, loss = 0.13279780\n",
            "Iteration 6828, loss = 0.13281328\n",
            "Iteration 6829, loss = 0.13279485\n",
            "Iteration 6830, loss = 0.13277950\n",
            "Iteration 6831, loss = 0.13274271\n",
            "Iteration 6832, loss = 0.13273802\n",
            "Iteration 6833, loss = 0.13270818\n",
            "Iteration 6834, loss = 0.13272589\n",
            "Iteration 6835, loss = 0.13269692\n",
            "Iteration 6836, loss = 0.13270254\n",
            "Iteration 6837, loss = 0.13266352\n",
            "Iteration 6838, loss = 0.13266880\n",
            "Iteration 6839, loss = 0.13264384\n",
            "Iteration 6840, loss = 0.13263629\n",
            "Iteration 6841, loss = 0.13264690\n",
            "Iteration 6842, loss = 0.13261184\n",
            "Iteration 6843, loss = 0.13257848\n",
            "Iteration 6844, loss = 0.13260924\n",
            "Iteration 6845, loss = 0.13263753\n",
            "Iteration 6846, loss = 0.13258857\n",
            "Iteration 6847, loss = 0.13256736\n",
            "Iteration 6848, loss = 0.13256704\n",
            "Iteration 6849, loss = 0.13251961\n",
            "Iteration 6850, loss = 0.13260185\n",
            "Iteration 6851, loss = 0.13251306\n",
            "Iteration 6852, loss = 0.13253151\n",
            "Iteration 6853, loss = 0.13247259\n",
            "Iteration 6854, loss = 0.13246830\n",
            "Iteration 6855, loss = 0.13247911\n",
            "Iteration 6856, loss = 0.13246968\n",
            "Iteration 6857, loss = 0.13245545\n",
            "Iteration 6858, loss = 0.13244808\n",
            "Iteration 6859, loss = 0.13245613\n",
            "Iteration 6860, loss = 0.13247700\n",
            "Iteration 6861, loss = 0.13240268\n",
            "Iteration 6862, loss = 0.13251060\n",
            "Iteration 6863, loss = 0.13239373\n",
            "Iteration 6864, loss = 0.13240568\n",
            "Iteration 6865, loss = 0.13240089\n",
            "Iteration 6866, loss = 0.13237940\n",
            "Iteration 6867, loss = 0.13239167\n",
            "Iteration 6868, loss = 0.13235775\n",
            "Iteration 6869, loss = 0.13238515\n",
            "Iteration 6870, loss = 0.13232113\n",
            "Iteration 6871, loss = 0.13230508\n",
            "Iteration 6872, loss = 0.13232648\n",
            "Iteration 6873, loss = 0.13227260\n",
            "Iteration 6874, loss = 0.13225884\n",
            "Iteration 6875, loss = 0.13231916\n",
            "Iteration 6876, loss = 0.13224051\n",
            "Iteration 6877, loss = 0.13223413\n",
            "Iteration 6878, loss = 0.13226530\n",
            "Iteration 6879, loss = 0.13218291\n",
            "Iteration 6880, loss = 0.13216868\n",
            "Iteration 6881, loss = 0.13217720\n",
            "Iteration 6882, loss = 0.13214929\n",
            "Iteration 6883, loss = 0.13214098\n",
            "Iteration 6884, loss = 0.13214876\n",
            "Iteration 6885, loss = 0.13211191\n",
            "Iteration 6886, loss = 0.13208889\n",
            "Iteration 6887, loss = 0.13208037\n",
            "Iteration 6888, loss = 0.13206051\n",
            "Iteration 6889, loss = 0.13203827\n",
            "Iteration 6890, loss = 0.13205100\n",
            "Iteration 6891, loss = 0.13201875\n",
            "Iteration 6892, loss = 0.13201840\n",
            "Iteration 6893, loss = 0.13206162\n",
            "Iteration 6894, loss = 0.13209734\n",
            "Iteration 6895, loss = 0.13207446\n",
            "Iteration 6896, loss = 0.13207098\n",
            "Iteration 6897, loss = 0.13210019\n",
            "Iteration 6898, loss = 0.13201253\n",
            "Iteration 6899, loss = 0.13198278\n",
            "Iteration 6900, loss = 0.13199482\n",
            "Iteration 6901, loss = 0.13197709\n",
            "Iteration 6902, loss = 0.13197214\n",
            "Iteration 6903, loss = 0.13200507\n",
            "Iteration 6904, loss = 0.13193603\n",
            "Iteration 6905, loss = 0.13191943\n",
            "Iteration 6906, loss = 0.13190035\n",
            "Iteration 6907, loss = 0.13190932\n",
            "Iteration 6908, loss = 0.13185471\n",
            "Iteration 6909, loss = 0.13187173\n",
            "Iteration 6910, loss = 0.13182100\n",
            "Iteration 6911, loss = 0.13181021\n",
            "Iteration 6912, loss = 0.13177783\n",
            "Iteration 6913, loss = 0.13182176\n",
            "Iteration 6914, loss = 0.13177242\n",
            "Iteration 6915, loss = 0.13177060\n",
            "Iteration 6916, loss = 0.13174495\n",
            "Iteration 6917, loss = 0.13174332\n",
            "Iteration 6918, loss = 0.13172765\n",
            "Iteration 6919, loss = 0.13173854\n",
            "Iteration 6920, loss = 0.13170237\n",
            "Iteration 6921, loss = 0.13172826\n",
            "Iteration 6922, loss = 0.13175966\n",
            "Iteration 6923, loss = 0.13172408\n",
            "Iteration 6924, loss = 0.13169615\n",
            "Iteration 6925, loss = 0.13171598\n",
            "Iteration 6926, loss = 0.13168523\n",
            "Iteration 6927, loss = 0.13165002\n",
            "Iteration 6928, loss = 0.13164159\n",
            "Iteration 6929, loss = 0.13161873\n",
            "Iteration 6930, loss = 0.13164394\n",
            "Iteration 6931, loss = 0.13160704\n",
            "Iteration 6932, loss = 0.13157591\n",
            "Iteration 6933, loss = 0.13156092\n",
            "Iteration 6934, loss = 0.13159520\n",
            "Iteration 6935, loss = 0.13154670\n",
            "Iteration 6936, loss = 0.13152909\n",
            "Iteration 6937, loss = 0.13152194\n",
            "Iteration 6938, loss = 0.13149820\n",
            "Iteration 6939, loss = 0.13160687\n",
            "Iteration 6940, loss = 0.13155272\n",
            "Iteration 6941, loss = 0.13155932\n",
            "Iteration 6942, loss = 0.13156010\n",
            "Iteration 6943, loss = 0.13151165\n",
            "Iteration 6944, loss = 0.13149224\n",
            "Iteration 6945, loss = 0.13147730\n",
            "Iteration 6946, loss = 0.13143753\n",
            "Iteration 6947, loss = 0.13149460\n",
            "Iteration 6948, loss = 0.13144180\n",
            "Iteration 6949, loss = 0.13141918\n",
            "Iteration 6950, loss = 0.13140864\n",
            "Iteration 6951, loss = 0.13145495\n",
            "Iteration 6952, loss = 0.13139332\n",
            "Iteration 6953, loss = 0.13141477\n",
            "Iteration 6954, loss = 0.13141010\n",
            "Iteration 6955, loss = 0.13137907\n",
            "Iteration 6956, loss = 0.13143495\n",
            "Iteration 6957, loss = 0.13139993\n",
            "Iteration 6958, loss = 0.13137242\n",
            "Iteration 6959, loss = 0.13137198\n",
            "Iteration 6960, loss = 0.13130568\n",
            "Iteration 6961, loss = 0.13127111\n",
            "Iteration 6962, loss = 0.13122681\n",
            "Iteration 6963, loss = 0.13140009\n",
            "Iteration 6964, loss = 0.13127318\n",
            "Iteration 6965, loss = 0.13124903\n",
            "Iteration 6966, loss = 0.13128979\n",
            "Iteration 6967, loss = 0.13124833\n",
            "Iteration 6968, loss = 0.13130188\n",
            "Iteration 6969, loss = 0.13124378\n",
            "Iteration 6970, loss = 0.13124586\n",
            "Iteration 6971, loss = 0.13126154\n",
            "Iteration 6972, loss = 0.13125100\n",
            "Iteration 6973, loss = 0.13128370\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(learning_rate_init=0.0001, max_iter=10000, tol=1e-06,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(learning_rate_init=0.0001, max_iter=10000, tol=1e-06,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(learning_rate_init=0.0001, max_iter=10000, tol=1e-06,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = redeneural.predict(x_treino)"
      ],
      "metadata": {
        "id": "iLjnm0Hi4-98"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precisao = accuracy_score(y_treino, y_pred)\n",
        "print(\"Precisão: {:.2f}%\".format(precisao * 100))"
      ],
      "metadata": {
        "id": "N__Tdilw5IOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b10c8b-b58e-4d61-bf03-18d06b46097d"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisão: 96.24%\n"
          ]
        }
      ]
    }
  ]
}